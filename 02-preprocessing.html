<!DOCTYPE html>
<!-- START: inst/pkgdown/templates/layout.html --><!-- Generated by pkgdown: do not edit by hand --><html lang="en" data-bs-theme="auto"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><title>Fundamentals of Natural Language Processing (NLP) in Python: Episode 1: From text to vectors</title><meta name="viewport" content="width=device-width, initial-scale=1"><script src="assets/themetoggle.js"></script><link rel="stylesheet" type="text/css" href="assets/styles.css"><script src="assets/scripts.js" type="text/javascript"></script><!-- mathjax --><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      config: ["MMLorHTML.js"],
      jax: ["input/TeX","input/MathML","output/HTML-CSS","output/NativeMML", "output/PreviewHTML"],
      extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
      TeX: {
        extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
      },
      tex2jax: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [ ['$$','$$'], ['\\[', '\\]'] ],
        processEscapes: true
      }
    });
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><!-- Responsive Favicon for The Carpentries --><link rel="apple-touch-icon" sizes="180x180" href="favicons/incubator/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="favicons/incubator/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="favicons/incubator/favicon-16x16.png"><link rel="manifest" href="favicons/incubator/site.webmanifest"><link rel="mask-icon" href="favicons/incubator/safari-pinned-tab.svg" color="#5bbad5"><meta name="msapplication-TileColor" content="#da532c"><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"><meta name="theme-color" media="(prefers-color-scheme: dark)" content="black"></head><body>
    <header id="top" class="navbar navbar-expand-md top-nav incubator"><svg xmlns="http://www.w3.org/2000/svg" class="d-none"><symbol id="check2" viewbox="0 0 16 16"><path d="M13.854 3.646a.5.5 0 0 1 0 .708l-7 7a.5.5 0 0 1-.708 0l-3.5-3.5a.5.5 0 1 1 .708-.708L6.5 10.293l6.646-6.647a.5.5 0 0 1 .708 0z"></path></symbol><symbol id="circle-half" viewbox="0 0 16 16"><path d="M8 15A7 7 0 1 0 8 1v14zm0 1A8 8 0 1 1 8 0a8 8 0 0 1 0 16z"></path></symbol><symbol id="moon-stars-fill" viewbox="0 0 16 16"><path d="M6 .278a.768.768 0 0 1 .08.858 7.208 7.208 0 0 0-.878 3.46c0 4.021 3.278 7.277 7.318 7.277.527 0 1.04-.055 1.533-.16a.787.787 0 0 1 .81.316.733.733 0 0 1-.031.893A8.349 8.349 0 0 1 8.344 16C3.734 16 0 12.286 0 7.71 0 4.266 2.114 1.312 5.124.06A.752.752 0 0 1 6 .278z"></path><path d="M10.794 3.148a.217.217 0 0 1 .412 0l.387 1.162c.173.518.579.924 1.097 1.097l1.162.387a.217.217 0 0 1 0 .412l-1.162.387a1.734 1.734 0 0 0-1.097 1.097l-.387 1.162a.217.217 0 0 1-.412 0l-.387-1.162A1.734 1.734 0 0 0 9.31 6.593l-1.162-.387a.217.217 0 0 1 0-.412l1.162-.387a1.734 1.734 0 0 0 1.097-1.097l.387-1.162zM13.863.099a.145.145 0 0 1 .274 0l.258.774c.115.346.386.617.732.732l.774.258a.145.145 0 0 1 0 .274l-.774.258a1.156 1.156 0 0 0-.732.732l-.258.774a.145.145 0 0 1-.274 0l-.258-.774a1.156 1.156 0 0 0-.732-.732l-.774-.258a.145.145 0 0 1 0-.274l.774-.258c.346-.115.617-.386.732-.732L13.863.1z"></path></symbol><symbol id="sun-fill" viewbox="0 0 16 16"><path d="M8 12a4 4 0 1 0 0-8 4 4 0 0 0 0 8zM8 0a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 0zm0 13a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 13zm8-5a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2a.5.5 0 0 1 .5.5zM3 8a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2A.5.5 0 0 1 3 8zm10.657-5.657a.5.5 0 0 1 0 .707l-1.414 1.415a.5.5 0 1 1-.707-.708l1.414-1.414a.5.5 0 0 1 .707 0zm-9.193 9.193a.5.5 0 0 1 0 .707L3.05 13.657a.5.5 0 0 1-.707-.707l1.414-1.414a.5.5 0 0 1 .707 0zm9.193 2.121a.5.5 0 0 1-.707 0l-1.414-1.414a.5.5 0 0 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .707zM4.464 4.465a.5.5 0 0 1-.707 0L2.343 3.05a.5.5 0 1 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .708z"></path></symbol></svg><a class="visually-hidden-focusable skip-link" href="#main-content">Skip to main content</a>
  <div class="container-fluid top-nav-container">
    <div class="col-md-8">
      <div class="large-logo">
        <img id="incubator-logo" alt="Carpentries Incubator" src="assets/images/incubator-logo.svg"><span class="badge text-bg-danger">
          <abbr title="This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught.">
            <a href="https://docs.carpentries.org/resources/curriculum/lesson-life-cycle.html" class="external-link alert-link">
              <i aria-hidden="true" class="icon" data-feather="alert-octagon" style="border-radius: 5px"></i>
              Pre-Alpha
            </a>
            <span class="visually-hidden">This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught.</span>
          </abbr>
        </span>

      </div>
    </div>
    <div class="selector-container">
      <div id="theme-selector">
        <li class="nav-item dropdown" id="theme-button-list">
          <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
            <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="bd-theme-text"><li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                Light
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                Dark
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                Auto
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
          </ul></li>
      </div>

      <div class="dropdown" id="instructor-dropdown">
        <button class="btn btn-secondary dropdown-toggle bordered-button" type="button" id="dropdownMenu1" data-bs-toggle="dropdown" aria-expanded="false">
          <i aria-hidden="true" class="icon" data-feather="eye"></i> Learner View <i data-feather="chevron-down"></i>
        </button>
        <ul class="dropdown-menu" aria-labelledby="dropdownMenu1"><li><button class="dropdown-item" type="button" onclick="window.location.href='instructor/02-preprocessing.html';">Instructor View</button></li>
        </ul></div>
    </div>
  </div>
  <hr></header><nav class="navbar navbar-expand-xl bottom-nav incubator" aria-label="Main Navigation"><div class="container-fluid nav-container">
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle Navigation">
      <span class="navbar-toggler-icon"></span>
      <span class="menu-title">Menu</span>
    </button>
    <div class="nav-logo">
      <img class="small-logo" alt="Carpentries Incubator" src="assets/images/incubator-logo-sm.svg"></div>
    <div class="lesson-title-md">
      Fundamentals of Natural Language Processing (NLP) in Python
    </div>
    <div class="search-icon-sm">
      <!-- TODO: do not show until we have search
        <i role="img" aria-label="Search the All In One page" data-feather="search"></i>
      -->
    </div>
    <div class="desktop-nav">
      <ul class="navbar-nav me-auto mb-2 mb-lg-0"><li class="nav-item">
          <span class="lesson-title">
            Fundamentals of Natural Language Processing (NLP) in Python
          </span>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="key-points.html">Key Points</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="reference.html#glossary">Glossary</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="profiles.html">Learner Profiles</a>
        </li>
        <li class="nav-item dropdown">
          <button class="nav-link dropdown-toggle" id="navbarDropdown" data-bs-toggle="dropdown" aria-expanded="false">
            More <i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu" aria-labelledby="navbarDropdown"></ul></li>
      </ul></div>
    <!--
    <form class="d-flex col-md-2 search-form">
      <fieldset disabled>
      <input class="form-control me-2 searchbox" type="search" placeholder="" aria-label="">
        <button class="btn btn-outline-success tablet-search-button"  type="submit">
          <i class="search-icon" data-feather="search" role="img" aria-label="Search the All In One page"></i>
        </button>
      </fieldset>
    </form>
    -->
    <a id="search-button" class="btn btn-primary" href="aio.html" role="button" aria-label="Search the All In One page">Search the All In One page</a>
  </div><!--/div.container-fluid -->
</nav><div class="col-md-12 mobile-title">
  Fundamentals of Natural Language Processing (NLP) in Python
</div>

<aside class="col-md-12 lesson-progress"><div style="width: 29%" class="percentage">
    29%
  </div>
  <div class="progress incubator">
    <div class="progress-bar incubator" role="progressbar" style="width: 29%" aria-valuenow="29" aria-label="Lesson Progress" aria-valuemin="0" aria-valuemax="100">
    </div>
  </div>
</aside><div class="container">
      <div class="row">
        <!-- START: inst/pkgdown/templates/navbar.html -->
<div id="sidebar-col" class="col-lg-4">
  <div id="sidebar" class="sidebar">
      <nav aria-labelledby="flush-headingEleven"><button role="button" aria-label="close menu" alt="close menu" aria-expanded="true" aria-controls="sidebar" class="collapse-toggle" data-collapse="Collapse " data-episodes="Episodes ">
          <i class="search-icon" data-feather="x" role="img"></i>
        </button>
        <div class="sidebar-inner">
          <div class="row mobile-row" id="theme-row-mobile">
            <div class="col" id="theme-selector">
              <li class="nav-item dropdown" id="theme-button-list">
                <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
                  <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><span class="d-lg-none ms-1" id="bd-theme-text">Toggle Theme</span>
                </button>
                <ul class="dropdown-menu dropdown-menu-right" aria-labelledby="bd-theme-text"><li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                      Light
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                      Dark
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                      <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                      Auto
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                </ul></li>
            </div>
          </div>
          <div class="row mobile-row">
            <div class="col">
              <div class="sidenav-view-selector">
                <div class="accordion accordion-flush" id="accordionFlush9">
                  <div class="accordion-item">
                    <h2 class="accordion-header" id="flush-headingNine">
                      <button class="accordion-button collapsed" id="instructor" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseNine" aria-expanded="false" aria-controls="flush-collapseNine">
                        <i id="eye" aria-hidden="true" class="icon" data-feather="eye"></i> Learner View
                      </button>
                    </h2>
                    <div id="flush-collapseNine" class="accordion-collapse collapse" aria-labelledby="flush-headingNine" data-bs-parent="#accordionFlush2">
                      <div class="accordion-body">
                        <a href="instructor/02-preprocessing.html">Instructor View</a>
                      </div>
                    </div>
                  </div><!--/div.accordion-item-->
                </div><!--/div.accordion-flush-->
              </div><!--div.sidenav-view-selector -->
            </div><!--/div.col -->

            <hr></div><!--/div.mobile-row -->

          <div class="accordion accordion-flush" id="accordionFlush11">
            <div class="accordion-item">

              <button id="chapters" class="accordion-button show" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseEleven" aria-expanded="false" aria-controls="flush-collapseEleven">
                <h2 class="accordion-header chapters" id="flush-headingEleven">
                  EPISODES
                </h2>
              </button>
              <div id="flush-collapseEleven" class="accordion-collapse show collapse" aria-labelledby="flush-headingEleven" data-bs-parent="#accordionFlush11">

                <div class="accordion-body">
                  <div class="accordion accordion-flush" id="accordionFlush1">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading1">
        <a href="index.html">Summary and Setup</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush2">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading2">
        <a href="01-introduction.html">1. Introduction</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlushcurrent">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-headingcurrent">
      <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapsecurrent" aria-expanded="true" aria-controls="flush-collapsecurrent">
        <span class="visually-hidden">Current Chapter</span>
        <span class="current-chapter">
        2. Episode 1: From text to vectors
        </span>
      </button>
    </div><!--/div.accordion-header-->

    <div id="flush-collapsecurrent" class="accordion-collapse collapse show" aria-labelledby="flush-headingcurrent" data-bs-parent="#accordionFlushcurrent">
      <div class="accordion-body">
        <ul><li><a href="#introduction">Introduction</a></li>
<li><a href="#preprocessing-text">Preprocessing Text</a></li>
<li><a href="#term-document-matrix">Term-Document Matrix</a></li>
<li><a href="#what-are-word-embeddings">What are word embeddings?</a></li>
<li><a href="#train-the-word2vec-model">Train the Word2Vec model</a></li>
        </ul></div><!--/div.accordion-body-->
    </div><!--/div.accordion-collapse-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush4">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading4">
        <a href="03-transformers.html">3. Episode 2: BERT and Transformers</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush5">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading5">
        <a href="04-LargeLanguageModels.html">4. Episode 3: Using large language models</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

                </div>
              </div>
            </div>

            <hr class="half-width"><div class="accordion accordion-flush lesson-resources" id="accordionFlush12">
              <div class="accordion-item">
                <h2 class="accordion-header" id="flush-headingTwelve">
                  <button class="accordion-button collapsed" id="lesson-resources" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseTwelve" aria-expanded="false" aria-controls="flush-collapseTwelve">
                    RESOURCES
                  </button>
                </h2>
                <div id="flush-collapseTwelve" class="accordion-collapse collapse" aria-labelledby="flush-headingTwelve" data-bs-parent="#accordionFlush12">
                  <div class="accordion-body">
                    <ul><li>
                        <a href="key-points.html">Key Points</a>
                      </li>
                      <li>
                        <a href="reference.html#glossary">Glossary</a>
                      </li>
                      <li>
                        <a href="profiles.html">Learner Profiles</a>
                      </li>

                    </ul></div>
                </div>
              </div>
            </div>
            <hr class="half-width lesson-resources"><a href="aio.html">See all in one page</a>


            <hr class="d-none d-sm-block d-md-none"><div class="d-grid gap-1">

            </div>
          </div><!-- /div.accordion -->
        </div><!-- /div.sidebar-inner -->
      </nav></div><!-- /div.sidebar -->
  </div><!-- /div.sidebar-col -->
<!-- END:   inst/pkgdown/templates/navbar.html-->

        <!-- START: inst/pkgdown/templates/content-instructor.html -->
  <div class="col-xl-8 col-lg-12 primary-content">
    <nav class="lesson-content mx-md-4" aria-label="Previous and Next Chapter"><!-- content for small screens --><div class="d-block d-sm-block d-md-none">
        <a class="chapter-link" href="01-introduction.html"><i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>Previous</a>
        <a class="chapter-link float-end" href="03-transformers.html">Next<i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i></a>
      </div>
      <!-- content for large screens -->
      <div class="d-none d-sm-none d-md-block">
        <a class="chapter-link" href="01-introduction.html" rel="prev">
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>
          Previous: Introduction
        </a>
        <a class="chapter-link float-end" href="03-transformers.html" rel="next">
          Next: Episode 2: BERT and...
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i>
        </a>
      </div>
      <hr></nav><main id="main-content" class="main-content"><div class="container lesson-content">
        <h1>Episode 1: From text to vectors</h1>
        <p>Last updated on 2025-09-18 |

        <a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/edit/main/episodes/02-preprocessing.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>



        <div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>

        

<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul><li>How do I load text and do basic linguistic analysis?</li>
<li>Why do we need to prepare a text for training?</li>
<li>How do I use words as features in a machine learning model?</li>
<li>What is a word2vec model?</li>
<li>What properties do word embeddings have?</li>
<li>What insights can I get from word embeddings?</li>
<li>How do we train a word2vec model?</li>
</ul></div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<p>After following this lesson, learners will be able to:</p>
<ul><li>Implement a full preprocessing pipeline on a text</li>
<li>Use Word2Vec to train a model</li>
<li>Inspect word embeddings</li>
</ul></div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a></h2>
<hr class="half-width"><p>In the previous episode we emphasized how text is different from
structured datasets. Given the linguistic properties embedded in
unstructured text, we also learned how to use existing libraries such as
SpaCy for segmenting text and accessing basic linguistic properties.</p>
<p>We learned the different levels of language and that it is ambiugous,
compositional and discrete. Because of this, it is hard to know how
words relate with each other, therefore obtaining meaning from text
alone is possible only through proxies that can be quantified. We made
our first attempt to approach word meaning by using co-occurrences of
words in a fixed text window around specific target words of
interest.</p>
<p>In this episode, we will expand on this idea and continue working
with words as individual features of text. We will introduce the concept
of Term-Document matrix, one of the most basic techniques to use words
as features that represent the texts where they appear, which can be fed
directly it into Machine Learning classifiers.</p>
<p>We will then visit the distributional hypothesis, which the linguist
J.R. Firth, in the 1950s, summarized with the phrase: “You shall know a
word by the company it keeps”. Based on this hypothesis, Mikolov et.
al. decided to train neural networks on large amounts of text in order
to predict a word based on it’s surrounding context or viceversa, in the
famous Word2Vec model. We will learn how to use these models, and
understand how do they map discrete words into numerical vectors that
capture the semantic similarity of words in a continuous space. By
representing words with vectors, we can mathematically manipulate them
through vector arithmetic and exploit the similarity patterns that
emerge from a collection of texts. Finally, we will show how to train
your own Word2Vec models.</p>
</section><section><h2 class="section-heading" id="preprocessing-text">Preprocessing Text<a class="anchor" aria-label="anchor" href="#preprocessing-text"></a></h2>
<hr class="half-width"><p>NLP models work by learning the statistical regularities within the
constituent parts of the language (i.e, letters, digits, words and
sentences) in a text. However, text contains also other type of
information that humans find useful to convey meaning. To signal pauses,
give emphasis and convey tone, for instance, we use punctuation.
Articles, conjunctions and prepositions also alter the meaning of a
sentence. The machine does not know the difference among all of these
linguistic units, as it treats them all as equal.</p>
<p>We have already done some basic data pre-processing in the
introduction. Here we will formalize this initial step and present some
of the most common pre-processing steps when dealing with structured
data. This is analogue to the data cleaning and sanitation step in any
Machine Learning task. In the case of linguistic data, we are interested
in getting rid of unwanted components (such as rare punctuation or
formatting characters) that can confuse a tokenizer and, depending on
the task at hand, we might also be interested in normalizing our tokens
to avoid possible noise in our final results. As we already know, an NLP
module such as SpaCy comes in handy to deal with the preprocessing of
text, here is the list of the recommended (always optional!) steps:</p>
<ul><li>
<strong>Tokenization:</strong> splitting strings into
meaningful/useful units. This step also includes a method for “mapping
back” the segments to their character position in the original
string.</li>
<li>
<strong>Lowercasing:</strong> removing uppercases to e.g. avoid
treating “Dog” and “dog” as two different words)</li>
<li>
<strong>Punctuation and Special Character Removal:</strong> if we
are interested in <em>content only,</em> we can filter out anything that
is not alphanumerical. We can also explicitly exlude symbols that are
just noise in our dataset. Note that getting rid of punctuation can
significantly change meaning! A special mention is that new lines are a
character in text, sometimes we can use them in our benefit (for example
to separate paragraphs) but many times they are just noise.</li>
<li>
<strong>Stop Word Removal:</strong> as we’ve seen, the most frequent
words in texts are those which contribute little semantic value on their
own: articles (‘the’, ‘a’ , ‘an’), conjunctions (‘and’, ‘or’, ‘but’),
prepositions (‘on’, ‘by’), auxiliary verbs (‘is’, ‘am’), pronouns (‘he’,
‘which’), or any highly frequent word that might not be of interest in
several <em>content only</em> related tasks. A special case is the word
‘not’ which carries the significant semantic value of negation.</li>
<li>
<strong>Lemmatization:</strong> although it has become less
frequent, normalizing words into their <em>dictionary form</em> can help
to focus on relevant aspects of text. Think how “eating”, “ate”, “eaten”
are all a variation of the verb “eat”.</li>
</ul><div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a> <span class="co">"Here my python code using spacy... Here we load the 'Dirty Book of Frankenstein' and the task is to arrive to the clean tokens to train Word2Vec"</span></span></code></pre>
</div>
<div id="callout1" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div class="callout-inner">
<div class="callout-content">
<ul><li><p>Preprocessing approaches affect significantly the quality of the
training when working with word embeddings. For example, [Rahimi &amp;
Homayounpour (2022)] (<a href="https://link.springer.com/article/10.1007/s10579-022-09620-5" class="external-link uri">https://link.springer.com/article/10.1007/s10579-022-09620-5</a>)
demonstrated that for text classification and sentiment analysis, the
removal of punctuation and stopwords leads to higher
performance.</p></li>
<li><p>You do not always need to do all the preprocessing steps, and
which ones you should do depends on what you want to do. For example, if
you want to segment text into sentences then characters such as ‘.’, ‘,’
or ‘?’ are the most important; if you want to extract Named Entities
from text, you explicitly do not want to lowercase the text, as capitals
are a component in the identification process, and if you are interested
in gender bias you definitely want to keep the pronouns, etc…</p></li>
<li><p>Preprocessing can be very diffent for different languages. This
is both in terms of which steps to apply, but also which methods to use
for a specific step.</p></li>
</ul></div>
</div>
</div>
<p>We will prepare the data for the two experiments in this episode 1.
Build a Term-Document Matrix 2. Train a Word2Vec model</p>
<p>For both taska we need to prepare our texts by applying the same
preprocessing steps. We are focusing on content words for now, so even
though our preprocessin will unfortunately loose a lot of the original
information, in exchange we will be able manipulate words as individual
numeric representations. Therefore the preprocessing includes: cleaning
the text, tokenizing, lowercasing words, removing punctuation,
lemmatizing words and removing stop words. Let’s apply this step by
step.</p>
<div class="section level3">
<h3 id="cleaning-the-text">1. Cleaning the text<a class="anchor" aria-label="anchor" href="#cleaning-the-text"></a></h3>
<p>We start by importing the <code>spaCy</code> library that will help
us go through the preprocessing steps. SpaCy is a popular open-source
library for NLP in Python and it works with pre-trained languages models
that we can load and use to process and analyse the text efficiently. We
can then load the SpaCy model into the pipeline function.</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="im">import</span> spacy</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>doc <span class="op">=</span> nlp(corpus)</span></code></pre>
</div>
<p>Next, we’ll eliminate the triple dashes that separate different news
articles, as well as the vertical bars used to divide some columns.</p>
</div>
<div class="section level3">
<h3 id="tokenizing">2. Tokenizing<a class="anchor" aria-label="anchor" href="#tokenizing"></a></h3>
<p>Tokenization is essential in NLP, as it helps to create structure
from raw text. It involves the segmentation of the text into smaller
units referred as <code>tokens</code>. Tokens can be sentences
(e.g. <code>'the happy cat'</code>), words
(<code>'the', 'happy', 'cat'</code>), subwords
(<code>'un', 'happiness'</code>) or characters
(<code>'c','a', 't'</code>). The choice of tokens depends by the
requirement of the model used for training, and the text. This step is
carried out by a pre-trained model (called tokeniser) that has been
fine-tuned for the target language. In our case, this is
<code>en_core_web_sm</code> loaded before.</p>
<div id="callout2" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div class="callout-inner">
<div class="callout-content">
<p>A good word tokeniser for example, does not simply break up a text
based on spaces and punctuation, but it should be able to
distinguish:</p>
<ul><li>abbreviations that include points (e.g.: <em>e.g.</em>)</li>
<li>times (<em>11:15</em>) and dates written in various formats
(<em>01/01/2024</em> or <em>01-01-2024</em>)</li>
<li>word contractions such as <em>don’t</em>, these should be split into
<em>do</em> and <em>n’t</em>
</li>
<li>URLs</li>
</ul><p>Many older tokenisers are rule-based, meaning that they iterate over
a number of predefined rules to split the text into tokens, which is
useful for splitting text into word tokens for example. Modern large
language models use subword tokenisation, which are more flexible.</p>
</div>
</div>
</div>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a>spacy_corpus <span class="op">=</span> nlp(corpus_clean)</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a><span class="co"># Get the tokens from the pipeline</span></span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>tokens <span class="op">=</span> [token.text <span class="cf">for</span> token <span class="kw">in</span> spacy_corpus]</span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a>tokens[:<span class="dv">10</span>]</span></code></pre>
</div>
<p><code>['mens', 'op', 'maan', '\n ', '„', 'de', 'eagle', 'is', 'geland', '”']</code></p>
<p>As one can see the tokeniser has split each word in a token, however
it has considered also blank spaces <code>\n</code> and also
punctuation.</p>
</div>
<div class="section level3">
<h3 id="lowercasing">3. Lowercasing<a class="anchor" aria-label="anchor" href="#lowercasing"></a></h3>
<p>Our next step is to lowercase the text. Our goal here is to generate
a list of unique words from the text, so in order to not have words
twice in the list - once normal and once capitalised when it is at the
start of a sentence for example - we can lowercase the full text.</p>
<p>corpus_lower = corpus_clean.lower()</p>
<p>print(corpus_lower)</p>
<p>mens op maan „ de eagle is geland ” reisduur : 102 uur , uitstappen
binnen 20 iuli , 21.17 uur 45 […]</p>
</div>
<div class="section level3">
<h3 id="remove-punctuation">4. Remove punctuation<a class="anchor" aria-label="anchor" href="#remove-punctuation"></a></h3>
<p>The next step we will apply is to remove punctuation. We are
interested in training our model to learn the meaning of the words. This
task is highly influenced by the state of our text and punctuation would
decrease the quality of the learning as it would add spurious
information. We’ll see how the learning process works later in the
episode.</p>
<p>The punctuation symbols are defined in:</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="im">import</span> string</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>string.punctuation</span></code></pre>
</div>
<p>We can loop over these symbols to remove them from the text:</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="co"># remove punctuation from set</span></span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>tokens_no_punct <span class="op">=</span> [token <span class="cf">for</span> token <span class="kw">in</span> tokens <span class="cf">if</span> token <span class="kw">not</span> <span class="kw">in</span> string.punctuation]</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a><span class="co"># remove also blank spaces</span></span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a>tokens_no_punct <span class="op">=</span> [token <span class="cf">for</span> token <span class="kw">in</span> tokens_no_punct <span class="cf">if</span> token.strip() <span class="op">!=</span> <span class="st">''</span>]</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="bu">print</span>(tokens_no_punct[:<span class="dv">10</span>])</span></code></pre>
</div>
<p><code>['mens', 'op', 'maan', 'de', 'eagle', 'is', 'geland', 'reisduur', '102', 'uur']</code></p>
<div class="section level4">
<h4 id="visualise-the-tokens">Visualise the tokens<a class="anchor" aria-label="anchor" href="#visualise-the-tokens"></a></h4>
<p>This was the end of our preprocessing step. Let’s look at what tokens
we have extracted and how frequently they occur in the text.</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> Counter</span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a><span class="co"># count the frequency of occurrence of each token</span></span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a>token_counts <span class="op">=</span> Counter(tokens_no_punct)</span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a><span class="co"># get the top n most common tokens (otherwise the plot would be too crowded) and their relative frequencies</span></span>
<span id="cb7-8"><a href="#cb7-8" tabindex="-1"></a>most_common <span class="op">=</span> token_counts.most_common(<span class="dv">100</span>)</span>
<span id="cb7-9"><a href="#cb7-9" tabindex="-1"></a>tokens <span class="op">=</span> [item[<span class="dv">0</span>] <span class="cf">for</span> item <span class="kw">in</span> most_common]</span>
<span id="cb7-10"><a href="#cb7-10" tabindex="-1"></a>frequencies <span class="op">=</span> [item[<span class="dv">1</span>] <span class="cf">for</span> item <span class="kw">in</span> most_common]</span>
<span id="cb7-11"><a href="#cb7-11" tabindex="-1"></a></span>
<span id="cb7-12"><a href="#cb7-12" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">6</span>))</span>
<span id="cb7-13"><a href="#cb7-13" tabindex="-1"></a>plt.bar(tokens, frequencies)</span>
<span id="cb7-14"><a href="#cb7-14" tabindex="-1"></a>plt.xlabel(<span class="st">'Tokens'</span>)</span>
<span id="cb7-15"><a href="#cb7-15" tabindex="-1"></a>plt.ylabel(<span class="st">'Frequency'</span>)</span>
<span id="cb7-16"><a href="#cb7-16" tabindex="-1"></a>plt.title(<span class="st">'Token Frequencies'</span>)</span>
<span id="cb7-17"><a href="#cb7-17" tabindex="-1"></a>plt.xticks(rotation<span class="op">=</span><span class="dv">90</span>)</span>
<span id="cb7-18"><a href="#cb7-18" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb7-19"><a href="#cb7-19" tabindex="-1"></a>plt.show()</span></code></pre>
</div>
<p>As one can see, words in the text have a very specific <a href="https://link.springer.com/article/10.3758/s13423-014-0585-6" class="external-link">skewed
distribution</a>, such that there are few very high-frequency words that
account for most of the tokens in text (e.g., articles, conjunctions)
and many low frequency words.</p>
</div>
</div>
<div class="section level3">
<h3 id="stop-word-removal">5. Stop word removal<a class="anchor" aria-label="anchor" href="#stop-word-removal"></a></h3>
<p>For some NLP tasks only the important words in the text are needed. A
text however often contains many <code>stop words</code>: common words
such as <code>de</code>, <code>het</code>, <code>een</code> that add
little meaningful content compared to nouns and words. In those cases,
it is best to remove stop words from your corpus to reduce the number of
words to process.</p>
</div>
</section><section><h2 class="section-heading" id="term-document-matrix">Term-Document Matrix<a class="anchor" aria-label="anchor" href="#term-document-matrix"></a></h2>
<hr class="half-width"><p>A Term-Document Matrix (TDM) is a matrix where:</p>
<ul><li>Each <strong>row</strong> is a unique word (term) in the corpus</li>
<li>Each <strong>column</strong> is a document in the corpus</li>
<li>Each <strong>cell</strong> <span class="math inline">\((i,j)\)</span> has a value of 1 if the <span class="math inline">\(term_i\)</span> appears in <span class="math inline">\(column_j\)</span> or 0 otherwise</li>
</ul><p>This is also sometimes known as a bag-of-words as it ignores grammar
and word sequences in exchange of emphazising content, where each
document is characterized by the words that appear in it. Similar
documents will contain similar bags of words and documents that talk
about different topics will be associated with numerical columns that
are different from each other. Let’s look at a quick example:</p>
<ul><li>Doc 1: “Natural language processing is exciting”</li>
<li>Doc 2: “Processing natural language helps computers understand”</li>
<li>Doc 3: “Language processing with computers is NLP”</li>
<li>Doc 4: “Today it rained a lot”</li>
</ul><table class="table"><thead><tr class="header"><th>Term</th>
<th>Doc1</th>
<th>Doc2</th>
<th>Doc3</th>
<th>Doc3</th>
</tr></thead><tbody><tr class="odd"><td>natural</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr><tr class="even"><td>language</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>0</td>
</tr><tr class="odd"><td>processing</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>0</td>
</tr><tr class="even"><td>is</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr><tr class="odd"><td>exciting</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr><tr class="even"><td>helps</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr><tr class="odd"><td>computers</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0</td>
</tr><tr class="even"><td>understand</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr><tr class="odd"><td>with</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr><tr class="even"><td>NLP</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr><tr class="odd"><td>today</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr><tr class="even"><td>it</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr><tr class="odd"><td>rained</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr><tr class="even"><td>a</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr><tr class="odd"><td>lot</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr></tbody></table><p>We can represent each document by taking its column and treating it
as a vector of 0’s and 1’s. The vector is of fixed size (in this case
the vocabulary size is 15), therefore suitable for traditional ML
classifiers. With TDM there is a problem of scalability, as the matrix
size grows with the amount of documents times the vocabulary found in
the documents we are processing. This means that if we have 100
documents in which 5,000 unique words appear, we would have to store a
matrix of 500,000 numbers! We also have the problem of sparsity present:
document “vectors” will have mostly 0’s. TDM is also a good solution to
characterize documents based on their vocabulary, however the converse
is even more desirable: to characterize words based on the context where
they appear, so we can study words independently of their documents of
origin, and more importantly, how do they relate to each other. To solve
these and other limitations we enter the world of word embeddings!</p>
</section><section><h2 class="section-heading" id="what-are-word-embeddings">What are word embeddings?<a class="anchor" aria-label="anchor" href="#what-are-word-embeddings"></a></h2>
<hr class="half-width"><p>A Word Embedding is a word representation type that maps words in a
numerical manner (i.e., into vectors) in a multidimensional space,
capturing their meaning based on characteristics or context. Since
similar words occur in similar contexts, or have same characteristics,
the system automatically learns to assign similar vectors to similar
words.</p>
<p>Let’s illustrate this concept using animals. This example will show
us an intuitive way of representing things into vectors.</p>
<p>Suppose we want to represent a <code>cat</code> using measurable
characteristics:</p>
<ul><li>Furriness: Let’s assign a score of 70 to a cat</li>
<li>Number of legs: A cat has 4 legs</li>
</ul><div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a>cat <span class="op">=</span> np.array([[<span class="dv">70</span>, <span class="dv">4</span>]])</span></code></pre>
</div>
<p>So the vector representation of a cat becomes:
<code>[70 (furriness), 4 (legs)]</code></p>
<figure><img src="fig/emb3.png" class="figure mx-auto d-block"></figure><p>This vector doesn’t fully describe a cat but provides a basis for
comparison with other animals.</p>
<p>Let’s add vectors for a dog and a caterpillar:</p>
<ul><li>Dog: [56, 4]</li>
<li>Caterpillar: [70, 100]</li>
</ul><div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a></span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a>dog <span class="op">=</span> np.array([[<span class="dv">56</span>, <span class="dv">4</span>]])</span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a>caterpillar <span class="op">=</span> np.array([[<span class="dv">70</span>, <span class="dv">100</span>]])</span></code></pre>
</div>
<figure><img src="fig/emb5.png" class="figure mx-auto d-block"></figure><p>To determine which animal is more similar to a cat, we use
<code>cosine similarity</code>, which measures the cosine of the angle
between two vectors.</p>
<div id="callout3" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div class="callout-inner">
<div class="callout-content">
<p><a href="https://en.wikipedia.org/wiki/Cosine_similarity" class="external-link">cosine
similarity</a> ranges between [<code>-1</code> and <code>1</code>]. It
is the cosine of the angle between two vectors, divided by the product
of their length. It is a useful metric to measure how similar two
vectors are likely to be.</p>
<figure><img src="fig/emb12.png" alt="" class="figure mx-auto d-block"></figure></div>
</div>
</div>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a><span class="im">from</span> sklearn.metrics.pairwise <span class="im">import</span> cosine_similarity</span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a>similarity_cat_dog <span class="op">=</span> cosine_similarity(cat, dog)[<span class="dv">0</span>][<span class="dv">0</span>]</span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a>similarity_cat_caterpillar <span class="op">=</span> cosine_similarity(cat, caterpillar)[<span class="dv">0</span>][<span class="dv">0</span>]</span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a></span>
<span id="cb10-6"><a href="#cb10-6" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Cosine similarity between cat and dog: </span><span class="sc">{</span>similarity_cat_dog<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-7"><a href="#cb10-7" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Cosine similarity between cat and caterpillar: </span><span class="sc">{</span>similarity_cat_caterpillar<span class="sc">}</span><span class="ss">"</span>)</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a></span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a>Cosine similarity between cat <span class="kw">and</span> dog: <span class="fl">0.9998987965747193</span></span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a>Cosine similarity between cat <span class="kw">and</span> caterpillar: <span class="fl">0.6192653797321375</span></span></code></pre>
</div>
<p>The higher similarity score between the cat and the dog indicates
they are more similar based on these characteristics. Adding more
characteristics can enrich our vectors, detecting more semantic
nuances.</p>
<figure><img src="fig/emb6.png" class="figure mx-auto d-block"></figure><div id="challenge1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div class="callout-inner">
<h3 class="callout-title">Challenge</h3>
<div class="callout-content">
<ul><li>Add one of two other dimensions. What characteristics could they
map?</li>
<li>Add another animal and map their dimensions</li>
<li>Compute again the cosine similarity among those animals and find the
couple that is the least similar and the most similar</li>
</ul></div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<ol style="list-style-type: decimal"><li>Add one of two other dimensions</li>
</ol><p>We could add the dimension of “velocity” or “speed” that goes from 0
to 100 meters/second.</p>
<ul><li>Caterpillar: 0.001 m/s</li>
<li>Cat: 1.5 m/s</li>
<li>Dog: 2.5 m/s</li>
</ul><p>(just as an example, actual speeds may vary)</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a>cat <span class="op">=</span> np.asarray([[<span class="dv">70</span>, <span class="dv">4</span>, <span class="fl">1.5</span>]])</span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a>dog <span class="op">=</span> np.asarray([[<span class="dv">56</span>, <span class="dv">4</span>, <span class="fl">2.5</span>]])</span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a>caterpillar <span class="op">=</span> np.asarray([[<span class="dv">70</span>, <span class="dv">100</span>, <span class="fl">.001</span>]])</span></code></pre>
</div>
<p>Another dimension could be weight in Kg:</p>
<ul><li>Caterpillar: .05 Kg</li>
<li>Cat: 4 Kg</li>
<li>Dog: 15 Kg</li>
</ul><p>(just as an example, actual weight may vary)</p>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a>cat <span class="op">=</span> np.asarray([[<span class="dv">70</span>, <span class="dv">4</span>, <span class="fl">1.5</span>, <span class="dv">4</span>]])</span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a>dog <span class="op">=</span> np.asarray([[<span class="dv">56</span>, <span class="dv">4</span>, <span class="fl">2.5</span>, <span class="dv">15</span>]])</span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a>caterpillar <span class="op">=</span> np.asarray([[<span class="dv">70</span>, <span class="dv">100</span>, <span class="fl">.001</span>, <span class="fl">.05</span>]])</span></code></pre>
</div>
<p>Then the cosine similarity would be:</p>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a>cosine_similarity(cat, caterpillar)</span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" tabindex="-1"></a>cosine_similarity(cat, dog)</span></code></pre>
</div>
<p>Output:</p>
<div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a>array([[<span class="fl">0.61814254</span>]])</span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a>array([[<span class="fl">0.97893809</span>]])</span></code></pre>
</div>
<ol start="2" style="list-style-type: decimal"><li>Add another animal and map their dimensions</li>
</ol><p>Another animal that we could add is the Tarantula!</p>
<div class="codewrapper sourceCode" id="cb16">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a>cat <span class="op">=</span> np.asarray([[<span class="dv">70</span>, <span class="dv">4</span>, <span class="fl">1.5</span>, <span class="dv">4</span>]])</span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a>dog <span class="op">=</span> np.asarray([[<span class="dv">56</span>, <span class="dv">4</span>, <span class="fl">2.5</span>, <span class="dv">15</span>]])</span>
<span id="cb16-3"><a href="#cb16-3" tabindex="-1"></a>caterpillar <span class="op">=</span> np.asarray([[<span class="dv">70</span>, <span class="dv">100</span>, <span class="fl">.001</span>, <span class="fl">.05</span>]])</span>
<span id="cb16-4"><a href="#cb16-4" tabindex="-1"></a>tarantula <span class="op">=</span> np.asarray([[<span class="dv">80</span>, <span class="dv">6</span>, <span class="fl">.1</span>, <span class="fl">.3</span>]])</span></code></pre>
</div>
<ol start="3" style="list-style-type: decimal"><li>Compute again the cosine similarity among those animals - find out
the most and least similar couple</li>
</ol><p>Given the values above, the least similar couple is the dog and the
caterpillar, whose cosine similarity is
<code>array([[0.60855407]])</code>.</p>
<p>The most similar couple is the cat and the tarantula:
<code>array([[0.99822302]])</code></p>
</div>
</div>
</div>
</div>
<p>By representing words as vectors with multiple dimensions, we capture
more nuances of their meanings or characteristics.</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<span class="callout-header">Key Points</span>
<div class="callout-inner">
<div class="callout-content">
<ul><li>We can represent text as vectors of numbers (which makes it
interpretable for machines)</li>
<li>The most efficient and useful way is to use word embeddings</li>
<li>We can easily compute how words are similar to each other with the
cosine similarity</li>
</ul></div>
</div>
</div>
<p>When semantic change occurs, words in their context <em>also</em>
change. We can trace how a word evolves semantically over time through
comparison of that word with other similar words. The idea is that the
most similar words are not always fixed in each different year, if a
word acquires a new meaning.</p>
</section><section><h2 class="section-heading" id="train-the-word2vec-model">Train the Word2Vec model<a class="anchor" aria-label="anchor" href="#train-the-word2vec-model"></a></h2>
<hr class="half-width"><div class="section level3">
<h3 id="load-the-embeddings-and-inspect-them">Load the embeddings and inspect them<a class="anchor" aria-label="anchor" href="#load-the-embeddings-and-inspect-them"></a></h3>
<p>We proceed to load our models. We will load all pre-trained model
files from the original Word2Vec paper, which was trained on a big
corpus from Google News. The library <code>gensim</code> contains a
method called <code>KeyedVectors</code> which allows us to load
them.</p>
</div>
<div class="section level3">
<h3 id="prepare-the-data-to-be-ingested-by-the-model-preprocessing">Prepare the data to be ingested by the model (preprocessing)<a class="anchor" aria-label="anchor" href="#prepare-the-data-to-be-ingested-by-the-model-preprocessing"></a></h3>
<p>Also, the decision to remove or retain these parts of text is quite
crucial for training our model, as it affects the quality of generated
word vectors.</p>
<div id="dataset-size-in-training" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="dataset-size-in-training" class="callout-inner">
<h3 class="callout-title">Dataset size in training</h3>
<div class="callout-content">
<p>To obtain high-quality embeddings, the size/length of your training
dataset plays a crucial role. Generally <a href="https://cs.stanford.edu/~quocle/paragraph_vector.pdf" class="external-link">tens of
thousands of documents</a> are considered a reasonable amount of data
for decent results.</p>
<p>Is there however a strict minimum? Not really. Things to keep in mind
is that <code>vocabulary size</code>, <code>document length</code> and
<code>desired vector size</code> interacts with each other. The higher
the dimensional vectors (e.g. 200-300 dimensions) the more data is
required, and of high quality, i.e. that allows the learning of words in
a variety of contexts.</p>
<p>While word2vec models typically perform better with large datasets
containing millions of words, using a single page is sufficient for
demonstration and learning purposes. This smaller dataset allows us to
train the model quickly and understand how word2vec works without the
need for extensive computational resources.</p>
</div>
</div>
</div>
<p>For the purpose of this episode and to make training easy on our
laptop, we’ll train our word2vec model using <strong>just one
book</strong>. Subsequently, we’ll load pre-trained models for tackling
our task.</p>
<p>Now we will train a two-layer neural network to transform our tokens
into word embeddings. We will be using the library <code>gensim</code>
and the model we will be using is called <code>Word2Vec</code>,
developed by Tomas Mikolov et al. in 2013.</p>
<p>Import the necessary libraries:</p>
<div class="codewrapper sourceCode" id="cb17">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a><span class="im">import</span> gensim</span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a><span class="im">from</span> gensim.models <span class="im">import</span> Word2Vec</span>
<span id="cb17-3"><a href="#cb17-3" tabindex="-1"></a></span>
<span id="cb17-4"><a href="#cb17-4" tabindex="-1"></a><span class="co"># import logging to monitor training</span></span>
<span id="cb17-5"><a href="#cb17-5" tabindex="-1"></a><span class="im">import</span> logging</span>
<span id="cb17-6"><a href="#cb17-6" tabindex="-1"></a></span>
<span id="cb17-7"><a href="#cb17-7" tabindex="-1"></a><span class="co"># set up logging</span></span>
<span id="cb17-8"><a href="#cb17-8" tabindex="-1"></a>logging.basicConfig(<span class="bu">format</span><span class="op">=</span><span class="st">'</span><span class="sc">%(asctime)s</span><span class="st"> : </span><span class="sc">%(levelname)s</span><span class="st"> : </span><span class="sc">%(message)s</span><span class="st">'</span>, level<span class="op">=</span>logging.INFO)</span></code></pre>
</div>
<p>There are two main architectures for training Word2Vec:</p>
<ul><li>Continuous Bag-of-Words (CBOW): Predicts a target word based on its
surrounding context words.</li>
<li>Continuous Skip-Gram: Predicts surrounding context words given a
target word.</li>
</ul><figure><img src="fig/emb13.png" class="figure mx-auto d-block"></figure><div id="callout5" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div class="callout-inner">
<div class="callout-content">
<p>CBOW is faster to train, while Skip-Gram is more effective for
infrequent words. Increasing context size improves embeddings but
increases training time.</p>
</div>
</div>
</div>
<p>We will be using CBOW. We are interested in having vectors with 300
dimensions and a context size of 5 surrounding words. We include all
words present in the corpora, regardless of their frequency of
occurrence and use 4 CPU cores for training. All these specifics are
translated in only one line of code.</p>
<p>Let’s train our model then:</p>
<div class="codewrapper sourceCode" id="cb18">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" tabindex="-1"></a>model <span class="op">=</span> Word2Vec([tokens_no_stopwords], vector_size<span class="op">=</span><span class="dv">300</span>, window<span class="op">=</span><span class="dv">5</span>, min_count<span class="op">=</span><span class="dv">1</span>, workers<span class="op">=</span><span class="dv">4</span>, sg<span class="op">=</span><span class="dv">0</span>)</span></code></pre>
</div>
<p>We can inspect already what’s the output of this training, by
checking the top 5 most similar words to “maan” (moon):</p>
<div class="codewrapper sourceCode" id="cb19">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" tabindex="-1"></a>word_vectors.most_similar(<span class="st">'maan'</span>, topn<span class="op">=</span><span class="dv">5</span>)</span></code></pre>
</div>
<p><code>[('plek', 0.48467501997947693), ('ouders', 0.46935707330703735), ('supe|', 0.3929591178894043), ('rotterdam', 0.37788015604019165), ('verkeerden', 0.33672046661376953)]</code></p>
<p>We have trained our model on one page only of the newspaper and the
training was very quick. However, to approach our problem it’s best to
train our model on the entire dataset. We dont’ have the resources for
doing that on our local laptop, but luckily for us, <a href="https://zenodo.org/records/3237380" class="external-link">Wevers, M (2019)</a> did that
already for us and released it publicly. Let’s download this dataset on
our laptop and let’s save them in a folder called <code>w2v</code>.</p>
<div class="codewrapper sourceCode" id="cb20">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" tabindex="-1"></a>folder_path <span class="op">=</span> <span class="st">'data/w2v/'</span></span></code></pre>
</div>
</div>
</section></div> <!-- / div.lesson-content -->
    </main><!-- / main#main-content.main-content --><nav class="bottom-pagination mx-md-4" aria-label="Previous and Next Chapter"><div class="d-block d-sm-block d-md-none">
        <a class="chapter-link" href="01-introduction.html"><i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>Previous</a>
        <a class="chapter-link float-end" href="03-transformers.html">Next<i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i></a>
      </div>
      <!-- content for large screens -->
      <div class="d-none d-sm-none d-md-block">
        <a class="chapter-link" href="01-introduction.html" rel="prev">
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>
          Previous: Introduction
        </a>
        <a class="chapter-link float-end" href="03-transformers.html" rel="next">
          Next: Episode 2: BERT and...
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i>
        </a>
      </div>
    </nav></div> <!-- / div.primary-content.col-xs-12 -->
<!-- END:   inst/pkgdown/templates/content-instructor.html-->

      </div><!--/div.row-->
      		<footer class="row footer mx-md-3"><hr><div class="col-md-6">
        <p>This lesson is subject to the <a href="CODE_OF_CONDUCT.html">Code of Conduct</a></p>
        <p>

        <a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/edit/main/episodes/02-preprocessing.md" class="external-link">Edit on GitHub</a>

	
        | <a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/blob/main/CONTRIBUTING.md" class="external-link">Contributing</a>
        | <a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/" class="external-link">Source</a></p>
				<p><a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/blob/main/CITATION.cff" class="external-link">Cite</a> | <a href="mailto:team@carpentries.org">Contact</a> | <a href="https://carpentries.org/about/" class="external-link">About</a></p>
			</div>
			<div class="col-md-6">

        <p>Materials licensed under <a href="LICENSE.html">CC-BY 4.0</a> by the authors</p>

        <p>Template licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/" class="external-link">CC-BY 4.0</a> by <a href="https://carpentries.org/" class="external-link">The Carpentries</a></p>
        <p>Built with <a href="https://github.com/carpentries/sandpaper/tree/0.17.1" class="external-link">sandpaper (0.17.1)</a>, <a href="https://github.com/carpentries/pegboard/tree/0.7.9" class="external-link">pegboard (0.7.9)</a>, and <a href="https://github.com/carpentries/varnish/tree/1.0.7" class="external-link">varnish (1.0.7)</a></p>
			</div>
		</footer></div> <!-- / div.container -->
	<div id="to-top">
		<a href="#top">
      <i class="search-icon" data-feather="arrow-up" role="img" aria-label="Back To Top"></i><br><!-- <span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top --><span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top
		</a>
	</div>
  <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "LearningResource",
  "@id": "https://esciencecenter-digital-skills.github.io/Natural-language-processing/02-preprocessing.html",
  "inLanguage": "en",
  "dct:conformsTo": "https://bioschemas.org/profiles/LearningResource/1.0-RELEASE",
  "description": "A Carpentries Lesson teaching foundational data and coding skills to researchers worldwide",
  "keywords": "software, data, lesson, The Carpentries, NLP, English, social sciences, pre-alpha",
  "name": "Episode 1: From text to vectors",
  "creativeWorkStatus": "active",
  "url": "https://esciencecenter-digital-skills.github.io/Natural-language-processing/02-preprocessing.html",
  "identifier": "https://esciencecenter-digital-skills.github.io/Natural-language-processing/02-preprocessing.html",
  "dateCreated": "2024-04-24",
  "dateModified": "2025-09-18",
  "datePublished": "2025-09-23"
}

  </script><script>
		feather.replace();
	</script></body></html><!-- END:   inst/pkgdown/templates/layout.html-->

