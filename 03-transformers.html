<!DOCTYPE html>
<!-- START: inst/pkgdown/templates/layout.html --><!-- Generated by pkgdown: do not edit by hand --><html lang="en" data-bs-theme="auto"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><title>Fundamentals of Natural Language Processing (NLP) in Python: Transformers: BERT and Beyond</title><meta name="viewport" content="width=device-width, initial-scale=1"><script src="assets/themetoggle.js"></script><link rel="stylesheet" type="text/css" href="assets/styles.css"><script src="assets/scripts.js" type="text/javascript"></script><!-- mathjax --><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      config: ["MMLorHTML.js"],
      jax: ["input/TeX","input/MathML","output/HTML-CSS","output/NativeMML", "output/PreviewHTML"],
      extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
      TeX: {
        extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
      },
      tex2jax: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [ ['$$','$$'], ['\\[', '\\]'] ],
        processEscapes: true
      }
    });
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><!-- Responsive Favicon for The Carpentries --><link rel="apple-touch-icon" sizes="180x180" href="favicons/incubator/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="favicons/incubator/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="favicons/incubator/favicon-16x16.png"><link rel="manifest" href="favicons/incubator/site.webmanifest"><link rel="mask-icon" href="favicons/incubator/safari-pinned-tab.svg" color="#5bbad5"><meta name="msapplication-TileColor" content="#da532c"><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"><meta name="theme-color" media="(prefers-color-scheme: dark)" content="black"></head><body>
    <header id="top" class="navbar navbar-expand-md top-nav incubator"><svg xmlns="http://www.w3.org/2000/svg" class="d-none"><symbol id="check2" viewbox="0 0 16 16"><path d="M13.854 3.646a.5.5 0 0 1 0 .708l-7 7a.5.5 0 0 1-.708 0l-3.5-3.5a.5.5 0 1 1 .708-.708L6.5 10.293l6.646-6.647a.5.5 0 0 1 .708 0z"></path></symbol><symbol id="circle-half" viewbox="0 0 16 16"><path d="M8 15A7 7 0 1 0 8 1v14zm0 1A8 8 0 1 1 8 0a8 8 0 0 1 0 16z"></path></symbol><symbol id="moon-stars-fill" viewbox="0 0 16 16"><path d="M6 .278a.768.768 0 0 1 .08.858 7.208 7.208 0 0 0-.878 3.46c0 4.021 3.278 7.277 7.318 7.277.527 0 1.04-.055 1.533-.16a.787.787 0 0 1 .81.316.733.733 0 0 1-.031.893A8.349 8.349 0 0 1 8.344 16C3.734 16 0 12.286 0 7.71 0 4.266 2.114 1.312 5.124.06A.752.752 0 0 1 6 .278z"></path><path d="M10.794 3.148a.217.217 0 0 1 .412 0l.387 1.162c.173.518.579.924 1.097 1.097l1.162.387a.217.217 0 0 1 0 .412l-1.162.387a1.734 1.734 0 0 0-1.097 1.097l-.387 1.162a.217.217 0 0 1-.412 0l-.387-1.162A1.734 1.734 0 0 0 9.31 6.593l-1.162-.387a.217.217 0 0 1 0-.412l1.162-.387a1.734 1.734 0 0 0 1.097-1.097l.387-1.162zM13.863.099a.145.145 0 0 1 .274 0l.258.774c.115.346.386.617.732.732l.774.258a.145.145 0 0 1 0 .274l-.774.258a1.156 1.156 0 0 0-.732.732l-.258.774a.145.145 0 0 1-.274 0l-.258-.774a1.156 1.156 0 0 0-.732-.732l-.774-.258a.145.145 0 0 1 0-.274l.774-.258c.346-.115.617-.386.732-.732L13.863.1z"></path></symbol><symbol id="sun-fill" viewbox="0 0 16 16"><path d="M8 12a4 4 0 1 0 0-8 4 4 0 0 0 0 8zM8 0a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 0zm0 13a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 13zm8-5a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2a.5.5 0 0 1 .5.5zM3 8a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2A.5.5 0 0 1 3 8zm10.657-5.657a.5.5 0 0 1 0 .707l-1.414 1.415a.5.5 0 1 1-.707-.708l1.414-1.414a.5.5 0 0 1 .707 0zm-9.193 9.193a.5.5 0 0 1 0 .707L3.05 13.657a.5.5 0 0 1-.707-.707l1.414-1.414a.5.5 0 0 1 .707 0zm9.193 2.121a.5.5 0 0 1-.707 0l-1.414-1.414a.5.5 0 0 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .707zM4.464 4.465a.5.5 0 0 1-.707 0L2.343 3.05a.5.5 0 1 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .708z"></path></symbol></svg><a class="visually-hidden-focusable skip-link" href="#main-content">Skip to main content</a>
  <div class="container-fluid top-nav-container">
    <div class="col-md-8">
      <div class="large-logo">
        <img id="incubator-logo" alt="Carpentries Incubator" src="assets/images/incubator-logo.svg"><span class="badge text-bg-danger">
          <abbr title="This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught.">
            <a href="https://docs.carpentries.org/resources/curriculum/lesson-life-cycle.html" class="external-link alert-link">
              <i aria-hidden="true" class="icon" data-feather="alert-octagon" style="border-radius: 5px"></i>
              Pre-Alpha
            </a>
            <span class="visually-hidden">This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught.</span>
          </abbr>
        </span>

      </div>
    </div>
    <div class="selector-container">
      <div id="theme-selector">
        <li class="nav-item dropdown" id="theme-button-list">
          <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
            <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="bd-theme-text"><li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                Light
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                Dark
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                Auto
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
          </ul></li>
      </div>

      <div class="dropdown" id="instructor-dropdown">
        <button class="btn btn-secondary dropdown-toggle bordered-button" type="button" id="dropdownMenu1" data-bs-toggle="dropdown" aria-expanded="false">
          <i aria-hidden="true" class="icon" data-feather="eye"></i> Learner View <i data-feather="chevron-down"></i>
        </button>
        <ul class="dropdown-menu" aria-labelledby="dropdownMenu1"><li><button class="dropdown-item" type="button" onclick="window.location.href='instructor/03-transformers.html';">Instructor View</button></li>
        </ul></div>
    </div>
  </div>
  <hr></header><nav class="navbar navbar-expand-xl bottom-nav incubator" aria-label="Main Navigation"><div class="container-fluid nav-container">
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle Navigation">
      <span class="navbar-toggler-icon"></span>
      <span class="menu-title">Menu</span>
    </button>
    <div class="nav-logo">
      <img class="small-logo" alt="Carpentries Incubator" src="assets/images/incubator-logo-sm.svg"></div>
    <div class="lesson-title-md">
      Fundamentals of Natural Language Processing (NLP) in Python
    </div>
    <div class="search-icon-sm">
      <!-- TODO: do not show until we have search
        <i role="img" aria-label="Search the All In One page" data-feather="search"></i>
      -->
    </div>
    <div class="desktop-nav">
      <ul class="navbar-nav me-auto mb-2 mb-lg-0"><li class="nav-item">
          <span class="lesson-title">
            Fundamentals of Natural Language Processing (NLP) in Python
          </span>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="key-points.html">Key Points</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="reference.html#glossary">Glossary</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="profiles.html">Learner Profiles</a>
        </li>
        <li class="nav-item dropdown">
          <button class="nav-link dropdown-toggle" id="navbarDropdown" data-bs-toggle="dropdown" aria-expanded="false">
            More <i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu" aria-labelledby="navbarDropdown"></ul></li>
      </ul></div>
    <!--
    <form class="d-flex col-md-2 search-form">
      <fieldset disabled>
      <input class="form-control me-2 searchbox" type="search" placeholder="" aria-label="">
        <button class="btn btn-outline-success tablet-search-button"  type="submit">
          <i class="search-icon" data-feather="search" role="img" aria-label="Search the All In One page"></i>
        </button>
      </fieldset>
    </form>
    -->
    <a id="search-button" class="btn btn-primary" href="aio.html" role="button" aria-label="Search the All In One page">Search the All In One page</a>
  </div><!--/div.container-fluid -->
</nav><div class="col-md-12 mobile-title">
  Fundamentals of Natural Language Processing (NLP) in Python
</div>

<aside class="col-md-12 lesson-progress"><div style="width: 57%" class="percentage">
    57%
  </div>
  <div class="progress incubator">
    <div class="progress-bar incubator" role="progressbar" style="width: 57%" aria-valuenow="57" aria-label="Lesson Progress" aria-valuemin="0" aria-valuemax="100">
    </div>
  </div>
</aside><div class="container">
      <div class="row">
        <!-- START: inst/pkgdown/templates/navbar.html -->
<div id="sidebar-col" class="col-lg-4">
  <div id="sidebar" class="sidebar">
      <nav aria-labelledby="flush-headingEleven"><button role="button" aria-label="close menu" alt="close menu" aria-expanded="true" aria-controls="sidebar" class="collapse-toggle" data-collapse="Collapse " data-episodes="Episodes ">
          <i class="search-icon" data-feather="x" role="img"></i>
        </button>
        <div class="sidebar-inner">
          <div class="row mobile-row" id="theme-row-mobile">
            <div class="col" id="theme-selector">
              <li class="nav-item dropdown" id="theme-button-list">
                <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
                  <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><span class="d-lg-none ms-1" id="bd-theme-text">Toggle Theme</span>
                </button>
                <ul class="dropdown-menu dropdown-menu-right" aria-labelledby="bd-theme-text"><li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                      Light
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                      Dark
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                      <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                      Auto
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                </ul></li>
            </div>
          </div>
          <div class="row mobile-row">
            <div class="col">
              <div class="sidenav-view-selector">
                <div class="accordion accordion-flush" id="accordionFlush9">
                  <div class="accordion-item">
                    <h2 class="accordion-header" id="flush-headingNine">
                      <button class="accordion-button collapsed" id="instructor" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseNine" aria-expanded="false" aria-controls="flush-collapseNine">
                        <i id="eye" aria-hidden="true" class="icon" data-feather="eye"></i> Learner View
                      </button>
                    </h2>
                    <div id="flush-collapseNine" class="accordion-collapse collapse" aria-labelledby="flush-headingNine" data-bs-parent="#accordionFlush2">
                      <div class="accordion-body">
                        <a href="instructor/03-transformers.html">Instructor View</a>
                      </div>
                    </div>
                  </div><!--/div.accordion-item-->
                </div><!--/div.accordion-flush-->
              </div><!--div.sidenav-view-selector -->
            </div><!--/div.col -->

            <hr></div><!--/div.mobile-row -->

          <div class="accordion accordion-flush" id="accordionFlush11">
            <div class="accordion-item">

              <button id="chapters" class="accordion-button show" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseEleven" aria-expanded="false" aria-controls="flush-collapseEleven">
                <h2 class="accordion-header chapters" id="flush-headingEleven">
                  EPISODES
                </h2>
              </button>
              <div id="flush-collapseEleven" class="accordion-collapse show collapse" aria-labelledby="flush-headingEleven" data-bs-parent="#accordionFlush11">

                <div class="accordion-body">
                  <div class="accordion accordion-flush" id="accordionFlush1">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading1">
        <a href="index.html">Summary and Setup</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush2">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading2">
        <a href="01-introduction.html">1. Introduction</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush3">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading3">
        <a href="02-word_representations.html">2. From words to vectors</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlushcurrent">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-headingcurrent">
        <span class="visually-hidden">Current Chapter</span>
        <span class="current-chapter">
        3. Transformers: BERT and Beyond
        </span>
      
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush5">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading5">
        <a href="04-LargeLanguageModels.html">4. Episode 3: Using large language models</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

                </div>
              </div>
            </div>

            <hr class="half-width"><div class="accordion accordion-flush lesson-resources" id="accordionFlush12">
              <div class="accordion-item">
                <h2 class="accordion-header" id="flush-headingTwelve">
                  <button class="accordion-button collapsed" id="lesson-resources" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseTwelve" aria-expanded="false" aria-controls="flush-collapseTwelve">
                    RESOURCES
                  </button>
                </h2>
                <div id="flush-collapseTwelve" class="accordion-collapse collapse" aria-labelledby="flush-headingTwelve" data-bs-parent="#accordionFlush12">
                  <div class="accordion-body">
                    <ul><li>
                        <a href="key-points.html">Key Points</a>
                      </li>
                      <li>
                        <a href="reference.html#glossary">Glossary</a>
                      </li>
                      <li>
                        <a href="profiles.html">Learner Profiles</a>
                      </li>

                    </ul></div>
                </div>
              </div>
            </div>
            <hr class="half-width lesson-resources"><a href="aio.html">See all in one page</a>


            <hr class="d-none d-sm-block d-md-none"><div class="d-grid gap-1">

            </div>
          </div><!-- /div.accordion -->
        </div><!-- /div.sidebar-inner -->
      </nav></div><!-- /div.sidebar -->
  </div><!-- /div.sidebar-col -->
<!-- END:   inst/pkgdown/templates/navbar.html-->

        <!-- START: inst/pkgdown/templates/content-instructor.html -->
  <div class="col-xl-8 col-lg-12 primary-content">
    <nav class="lesson-content mx-md-4" aria-label="Previous and Next Chapter"><!-- content for small screens --><div class="d-block d-sm-block d-md-none">
        <a class="chapter-link" href="02-word_representations.html"><i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>Previous</a>
        <a class="chapter-link float-end" href="04-LargeLanguageModels.html">Next<i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i></a>
      </div>
      <!-- content for large screens -->
      <div class="d-none d-sm-none d-md-block">
        <a class="chapter-link" href="02-word_representations.html" rel="prev">
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>
          Previous: From words to
        </a>
        <a class="chapter-link float-end" href="04-LargeLanguageModels.html" rel="next">
          Next: Episode 3: Using...
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i>
        </a>
      </div>
      <hr></nav><main id="main-content" class="main-content"><div class="container lesson-content">
        <h1>Transformers: BERT and Beyond</h1>
        <p>Last updated on 2025-09-24 |

        <a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/edit/main/episodes/03-transformers.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>



        <div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>

        

<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul><li>What are some drawbacks of static word embeddings?</li>
<li>What are Transformers?</li>
<li>What is BERT and how does it work?</li>
<li>How can I use BERT to solve NLP tasks?</li>
<li>How should I evaluate my classifiers?</li>
<li>Which other Transformer variants are available?</li>
</ul></div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul><li>Understand how a Transformer works and recognize their different use
cases.</li>
<li>Understand how to use pre-trained tranfromers (Use Case: BERT)</li>
<li>Use BERT to classify texts.</li>
<li>Use BERT as a Named Entity Recognizer.</li>
<li>Understand assumptions and basic evaluation for NLP outputs.</li>
</ul></div>
</div>
</div>
</div>
</div>
<p>Static word embeddings such as Word2Vec can be used to represent each
word as a unique vectors. Vector representations also allow us to apply
numerical operations that can be mapped to some syntactic and semantic
properties of words, such as the cases of analogies or finding synonyms.
Once we transform words into vectors, these can also be used as
<strong>features</strong> for classifiers that can be trained predict
any supervised NLP task.</p>
<p>However, a big drawback of Word2Vec is that <strong>each word is
represented in isolation</strong>, and unfortunately that is not how
language works. Words get their meanings based on the specific context
in which they are used (take for example polysemy, the cases where the
same word can have very different meanings depending on the context);
therefore, we would like to have richer vector representations of words
that also integrate context into account in order to obtain more
powerful representations.</p>
<div id="polysemy-in-language" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="polysemy-in-language" class="callout-inner">
<h3 class="callout-title">Polysemy in Language</h3>
<div class="callout-content">
<p>Think of (at least 2) different words that can have more than one
meaning depending on the context. Come up with one simple sentence per
meaning and explain what they mean in each context. Discuss: How do you
know what of the possible meanings does the word have when you use
it?</p>
<p>OPTIONAL: Why do you think Word2Vec can’t caputure different meanings
of words?</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<p>Two possible examples can be the words ‘fine’ and ‘run’</p>
<p>Sentences for ‘fine’: - She has a fine watch (fine == high-quality) -
He had to pay a fine (fine == penalty) - I am feeling fine (fine == not
bad)</p>
<p>Sentences for ‘run’: - I had to run to catch the bus (run == moving
fast) - Stop talking, before you run out of ideas (run (out) ==
exhaust)</p>
<p>Note how in the “run out” example we even have to understand that the
meaning of run is not literal but goes accompained with a preposition
that changes its meaning.</p>
</div>
</div>
</div>
</div>
<p>In 2019, the BERT language model was introduced. Using a novel
architecture called Transformer (2017), BERT can integrate context into
word representations. To understand BERT, we will first look at what a
transformer is and we will then directly use some code to make use of
BERT.</p>
<div class="section level1">
<h1 id="transformers">Transformers<a class="anchor" aria-label="anchor" href="#transformers"></a></h1>
<p>The Transformer is a neural network architecture proposed by Google
researchers <a href="https://arxiv.org/pdf/1706.03762" class="external-link">in 2017</a> in a
paper called <em>Attention is all you Need</em>. They tackled
specifically the NLP task of Machine Translation (MT), which is stated
as: how to generate a sentence (sequence of words) in target language B
given a sentence in source language A? We all know that translation
cannot be done word by word in isolations, therefore integrating the
context from both the source language and the target language is
necessary. In order to translate, first one neural network needs to
<em>encode</em> the whole meaning of the senetence in language A into a
single vector representation, then a second neural network needs to
<em>decode</em> that representation into tokens that are both coherent
with the meaning of language A and understandable in language B.
Therefore we say that translation is modeling language B
<em>conditioned</em> on what language A originally said.</p>
<figure><img src="fig/trans1.png" alt="Transformer Architecture" class="figure mx-auto d-block"><div class="figcaption">Transformer Architecture</div>
</figure><p>As seen in the picture, the original Transformer is an
Encoder-Decoder network that tackles translation. We first need a token
embedder which converts the string of words into a sequence of vectors
that the Transformer network can process. The first component, the
<strong>Encoder</strong>, is optimized for creating <strong>rich
representations</strong> of the source sequence (in this case an English
sentence) while the second one, the <strong>Decoder</strong> is a
<strong>generative network</strong> that is conditioned on the encoded
representation. The third component we see is the infamous attention
mechanism, a third neural network what computes the correlation between
source and target tokens (<em>Which word in Dutch should I pay attention
to decide a better next English word?</em>) to generate the most likely
token in the target sequence (in this case Dutch words).</p>
<div id="emulate-the-attention-mechanism" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="emulate-the-attention-mechanism" class="callout-inner">
<h3 class="callout-title">Emulate the Attention Mechanism</h3>
<div class="callout-content">
<p>Pair with a person who speaks a language different from English (we
will cal it language B). This time you should think of 2 simple
sentences in English and come up with their translations in the second
language. In a piece of paper write down both sentences (one on top of
the other) and try to: 1. Draw a one to one mapping of words in English
to language B. Is it always possible to do this? 2. Think of each word
in language B and draw as many lines as necessary to the relevant
English words that can “help you” predict the word in language B. If you
managed, congratulations, this is how attention works!</p>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2"> Show me the solution </h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" aria-labelledby="headingSolution2" data-bs-parent="#accordionSolution2">
<div class="accordion-body">
<p>Here an image of a bilingual “manual attention” example</p>
</div>
</div>
</div>
</div>
<p>Next, we will see how BERT exploits the idea of a <strong>Transformer
Encoder</strong> to perform the NLP Task we are interested in:
generating powerful word representations.</p>
</div>
<div class="section level1">
<h1 id="bert">BERT<a class="anchor" aria-label="anchor" href="#bert"></a></h1>
<p><a href="https://aclanthology.org/N19-1423.pdf" class="external-link">BERT</a> is an
acronym that stands for <strong>B</strong>idirectional
<strong>E</strong>ncoder <strong>R</strong>epresentations from
<strong>T</strong>ransformers. The name describes it all: the idea is to
use the power of the Encoder component of the Transformer architecture
to create powerful token representations that preserve the contextual
meaning of the whole input segment, instead of each word in isolation.
The BERT vector representations of each token take into account both the
left context (what comes before the word) and the right context (what
comes after the word). Another advantage of the transformer Encoder is
that it is parallelizable, which made it posible for the first time to
train these networks on millions of datapoints, dramatically improving
model generalization.</p>
<div id="pretraining-bert" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="pretraining-bert" class="callout-inner">
<h3 class="callout-title">Pretraining BERT</h3>
<div class="callout-content">
<p>To obtain the BERT vector representations the Encoder is pre-trained
with two different tasks: - <strong>Masked Language Model:</strong> for
each sentence, mask one token at a time and predict which token is
missing based on the context from both sides. A training input example
would be “Maria [MASK] Groningen” and the model should predict the word
“loves”. - <strong>Next Sentence Prediction:</strong> the Encoder gets a
linear binary classifier on top, which is trained to decide for each
pair of sequences A and B, if sequence A precedes sequence B in a text.
For the sentence pair: “Maria loves Groningen.” and “This is a city in
the Netherlands.” the output of the classifier is “True” and for the
pair “Maria loves Groningen.” and “It was a tasty cake.” the output
should be “false” as there is no obvious continuation between the two
sentences.</p>
<p>Already the second pre-training task gives us an idea of the power of
BERT: after it has been pretrained on hundreds of thousands of texts,
one can plug-in a classifier on top and re-use the <em>linguistic</em>
knowledge previously acquired to fine-tune it for a specific task,
without needing to learn the weights of the whole network from scratch
all over again. In the next sections we will describe the components of
BERT and show how to use it. This model and hundreds of related
transformer-based pre-trained encoders can also be found on <a href="https://huggingface.co/google-bert/bert-base-cased" class="external-link">Hugging
Face</a>.</p>
</div>
</div>
</div>
</div>
<div class="section level1">
<h1 id="bert-architecture">BERT Architecture<a class="anchor" aria-label="anchor" href="#bert-architecture"></a></h1>
<p>The BERT Architecture can be seen as a basic NLP pipeline on its
own:</p>
<ol style="list-style-type: decimal"><li>
<strong>Tokenizer:</strong> splits text into tokens that the model
recognizes</li>
<li>
<strong>Embedder:</strong> converts each token into a fixed-sized
vector that represents it. These vectors are the actual input for the
Encoder.</li>
<li>
<strong>Encoder</strong> several neural layers that model the
token-level interactions of the input sequence to enhance meaning
representation. The output of the encoder is a set of
<strong>H</strong>idden layers, the vector representation of the
ingested sequence.</li>
<li>
<strong>Output Layer:</strong> the final encoder layer (which we
depict as a sequence <strong>H</strong>’s in the figure) contains
arguably the best token-level representations that encode syntactic and
semantic properties of each token, but this time each vector is already
contextualized with the specific sequence.</li>
<li>
<em>OPTIONAL</em> <strong>Classifier Layer:</strong> an additional
classifier can be connected on top of the BERT token vectors which are
used as features for performing a downstream task. This can be used to
classify at the text level, for example sentiment analysis of a
sentence, or at the token-level, for example Named Entity
Recognition.</li>
</ol><figure><img src="fig/bert3.png" alt="BERT Architecture" class="figure mx-auto d-block"><div class="figcaption">BERT Architecture</div>
</figure><p>BERT uses (self-) attention, which is very useful to capture
longer-range word dependencies such as correference, where, for example,
a pronoun can be linked to the noun it refers to previously in the same
sentence. See the following example:</p>
<figure><img src="fig/trans5.png" alt="The Encoder Self-Attention Mechanism" class="figure mx-auto d-block"><div class="figcaption">The Encoder Self-Attention Mechanism</div>
</figure><div class="section level2">
<h2 id="bert-for-word-based-analysis">BERT for Word-Based Analysis<a class="anchor" aria-label="anchor" href="#bert-for-word-based-analysis"></a></h2>
<p>Let’s see how these components can be manipulated with code. For this
we will be using the HugingFace’s <em>transformers</em> python library.
The first two main components we need to initialize are the model and
tokenizer. The HuggingFace hub contains thousands of models based on a
Transformer architecture for dozens of tasks, data domains and also
hundreds of languages. Here we will explore the vanilla English BERT
which was how everything started. We can initialize this model with the
next lines:</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> BertTokenizer, BertModel</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a>tokenizer <span class="op">=</span> BertTokenizer.from_pretrained(<span class="st">'bert-base-cased'</span>)</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a>model <span class="op">=</span> BertModel.from_pretrained(<span class="st">"bert-base-cased"</span>)</span></code></pre>
</div>
<div class="section level3">
<h3 id="bert-tokenizer">BERT Tokenizer<a class="anchor" aria-label="anchor" href="#bert-tokenizer"></a></h3>
<p>We start with a string of text as written in any blog, book,
newspaper etcetera. The <code>tokenizer</code> object is responsible of
splitting the string into recognizable tokens for the model and
embedding the tokens into their vector representations</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"Maria loves Groningen"</span></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>encoded_input <span class="op">=</span> tokenizer(text, return_tensors<span class="op">=</span><span class="st">'pt'</span>)</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a><span class="bu">print</span>(encoded_input)</span></code></pre>
</div>
<p>The print shows the <code>encoded_input</code> object returned by the
tokenizer, with its attributes and values. The <code>input_ids</code>
are the most important output for now, as these are the token IDs
recognized by BERT</p>
<pre><code>{
    'input_ids': tensor([[  101,  3406,  7871,   144,  3484, 15016,   102]]),
    'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]),
    'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])
}
</code></pre>
<p>NOTE: the printing function shows transformers objects as
dictionaries; however, to access the attributes, you must use the python
object syntax, such as in the following example:</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="bu">print</span>(encoded_input.input_ids.shape)</span></code></pre>
</div>
<p>Output:</p>
<p><code>torch.Size([1, 7])</code></p>
<p>The output is a 2-dimensional tensor where the first dimention
contains 1 element (this dimension represents the batch size), and the
second dimension contains 7 elements which are equivalent to the 7
tokens that BERT generated with our string input.</p>
<p>In order to see what these Token IDs represent, we can
<em>translate</em> them into human readable strings. This includes
converting the tensors into numpy arrays and converting each ID into its
string representation:</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a>token_ids <span class="op">=</span> <span class="bu">list</span>(encoded_input.input_ids[<span class="dv">0</span>].detach().numpy())</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>string_tokens <span class="op">=</span> tokenizer.convert_ids_to_tokens(token_ids)</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"IDs:"</span>, token_ids)</span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"TOKENS:"</span>, string_tokens)</span></code></pre>
</div>
<p><code>IDs: [101, 3406, 7871, 144, 3484, 15016, 102]</code></p>
<p><code>TOKENS: ['[CLS]', 'Maria', 'loves', 'G', '##ron', '##ingen', '[SEP]']</code></p>
<div id="callout2" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div class="callout-inner">
<div class="callout-content">
<p>In the case of wanting to obtain a single vector for
<em>enchanting</em>, you can average the three vectors that belong to
the token pieces that ultimately form that word. For example:</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>tok_en <span class="op">=</span> output.last_hidden_state[<span class="dv">0</span>][<span class="dv">15</span>].detach().numpy()</span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a>tok_chan <span class="op">=</span> output.last_hidden_state[<span class="dv">0</span>][<span class="dv">16</span>].detach().numpy()</span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a>tok_ting <span class="op">=</span> output.last_hidden_state[<span class="dv">0</span>][<span class="dv">17</span>].detach().numpy()</span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a>tok_enchanting <span class="op">=</span> np.mean([tok_en, tok_chan, tok_ting], axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a>tok_enchanting.shape</span></code></pre>
</div>
<p>We use the functions <code>detach().numpy()</code> to bring the
values from the Pytorch execution environment (for example a GPU) into
the main python thread and treat it as a numpy vector for convenvience.
Then, since we are dealing with three numpy vectors we can average the
three of them and end op with a single <code>enchanting</code> vector of
768-dimensions representing the average of
<code>'en', '##chan', '##ting'</code>.</p>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="polysemy-in-bert">Polysemy in BERT<a class="anchor" aria-label="anchor" href="#polysemy-in-bert"></a></h3>
<p>We can encode two sentences containing the word <em>note</em> to see
how BERT actually handles polysemy (<em>note</em> means something very
different in each sentence) thanks to the representation of each word
now being contextualized instead of isolated as was the case with
word2vec.</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="co"># Search for the index of 'note' and obtain its vector from the sequence</span></span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>note_index_1 <span class="op">=</span> string_tokens.index(<span class="st">"note"</span>)</span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a>note_vector_1 <span class="op">=</span> output.last_hidden_state[<span class="dv">0</span>][note_index_1].detach().numpy()</span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a>note_token_id_1 <span class="op">=</span> token_ids[note_index_1]</span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a><span class="bu">print</span>(note_index_1, note_token_id_1, string_tokens)</span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a><span class="bu">print</span>(note_vector_1[:<span class="dv">5</span>])</span></code></pre>
</div>
<p>We are basically printing the tokenized sentence from the previous
example and showing the index of the token <code>note</code> in the list
of tokens. We are also printing the tokenID assigned to this token and
the list of tokens. Finally, the last print shows the first five
dimensions of the vector representing the token <code>note</code>.</p>
<pre><code>12 3805 ['[CLS]', 'Maria', "'", 's', 'passion', 'for', 'music', 'is', 'clearly', 'heard', 'in', 'every', 'note', 'and', 'every', 'en', '##chan', '##ting', 'melody', '.', '[SEP]']
[0.15780845 0.38866335 0.41498923 0.03389652 0.40278202]</code></pre>
<p>Let’s encode now another sentence, also containing the word
<code>note</code>, and confirm that the same token string, with the same
assigned tokenID holds a vector with different weights:</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="co"># Encode and then take the 'note' token from the second sentence</span></span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a>note_text_2 <span class="op">=</span> <span class="st">"I could not buy milk in the supermarket because the bank note I wanted to use was fake."</span></span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a>encoded_note_2 <span class="op">=</span> tokenizer(note_text_2, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a>token_ids <span class="op">=</span> <span class="bu">list</span>(encoded_note_2.input_ids[<span class="dv">0</span>].detach().numpy())</span>
<span id="cb9-5"><a href="#cb9-5" tabindex="-1"></a>string_tokens_2 <span class="op">=</span> tokenizer.convert_ids_to_tokens(token_ids)</span>
<span id="cb9-6"><a href="#cb9-6" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" tabindex="-1"></a>note_index_2 <span class="op">=</span> string_tokens_2.index(<span class="st">"note"</span>)</span>
<span id="cb9-8"><a href="#cb9-8" tabindex="-1"></a>note_vector_2 <span class="op">=</span> model(<span class="op">**</span>encoded_note_2).last_hidden_state[<span class="dv">0</span>][note_index_2].detach().numpy()</span>
<span id="cb9-9"><a href="#cb9-9" tabindex="-1"></a>note_token_id_2 <span class="op">=</span> token_ids[note_index_2]</span>
<span id="cb9-10"><a href="#cb9-10" tabindex="-1"></a></span>
<span id="cb9-11"><a href="#cb9-11" tabindex="-1"></a><span class="bu">print</span>(note_index_2, note_token_id_2, string_tokens_2)</span>
<span id="cb9-12"><a href="#cb9-12" tabindex="-1"></a><span class="bu">print</span>(note_vector_2[:<span class="dv">5</span>])</span></code></pre>
</div>
<pre><code>12 3805 ['[CLS]', 'I', 'could', 'not', 'buy', 'milk', 'in', 'the', 'supermarket', 'because', 'the', 'bank', 'note', 'I', 'wanted', 'to', 'use', 'was', 'fake', '.', '[SEP]']
[ 0.5003222   0.653664    0.22919582 -0.32637975  0.52929205]</code></pre>
<p>To be sure, we can compute the cosine similarity of the word
<em>note</em> in the first sentence and the word <em>note</em> in the
second sentence confirming that they are indeed two different
representations, even when in both cases they have the same token-id and
they are the 12th token of the sentence:</p>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a><span class="im">from</span> sklearn.metrics.pairwise <span class="im">import</span> cosine_similarity</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a>vector1 <span class="op">=</span> np.array(note_vector_1).reshape(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a>vector2 <span class="op">=</span> np.array(note_vector_2).reshape(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb11-5"><a href="#cb11-5" tabindex="-1"></a></span>
<span id="cb11-6"><a href="#cb11-6" tabindex="-1"></a>similarity <span class="op">=</span> cosine_similarity(vector1, vector2)</span>
<span id="cb11-7"><a href="#cb11-7" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Cosine Similarity 'note' vs 'note': </span><span class="sc">{</span>similarity[<span class="dv">0</span>][<span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code></pre>
</div>
<p>With this small experiment, we have confirmed that the Encoder
produces context-dependent word representations, as opposed to Word2Vec,
where <em>note</em> would always have the same vector no matter where it
appeared.</p>
<div id="callout3" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div class="callout-inner">
<div class="callout-content">
<p>When running examples in a BERT pre-trained model, it is advisable to
wrap your code inside a <code>torch.no_grad():</code> context. This is
linked to the fact that BERT is a Neural Network that has been trained
(and can be further finetuned) with the Backpropagation algorithm.
Essentially, this wrapper tells the model that we are not in training
mode, and we are not interested in <em>updating</em> the weights (as it
would happen when training any neural network), because the weights are
already optimal enough. By using this wrapper, we make the model more
efficient as it does not need to calculate the gradients for an eventual
backpropagation step, since we are only interested in what <em>comes
out</em> of the Encoder. So the previous code can be made more efficient
like this:</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a><span class="im">import</span> torch </span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb12-4"><a href="#cb12-4" tabindex="-1"></a>    output <span class="op">=</span> model(<span class="op">**</span>encoded_input)</span>
<span id="cb12-5"><a href="#cb12-5" tabindex="-1"></a>    <span class="bu">print</span>(output)</span>
<span id="cb12-6"><a href="#cb12-6" tabindex="-1"></a>    <span class="bu">print</span>(output.last_hidden_state.shape)</span></code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="section level1">
<h1 id="bert-as-a-language-model">BERT as a Language Model<a class="anchor" aria-label="anchor" href="#bert-as-a-language-model"></a></h1>
<p>As mentioned before, the main pre-training task of BERT is Language
Modelling (LM): calculating the probability of a word based on the known
neighboring words (yes, Word2Vec was also a kind of LM!). Obtaining
training data for this task is very cheap, as all we need is millions of
sentences from existing texts, without any labels. In this setting, BERT
encodes a sequence of words, and predicts from a set of English tokens,
what is the most likely token that could be inserted in the
<code>[MASK]</code> position</p>
<figure><img src="fig/bert1b.png" alt="BERT Language Modeling" class="figure mx-auto d-block"><div class="figcaption">BERT Language Modeling</div>
</figure><p>We can therefore start using BERT as a predictor for word completion.
From now own, we will learn how to use the <code>pipeline</code> object,
this is very useful when we only want to use a pre-trained model for
predictions (no need to fine-tune or do word-specific analysis). The
<code>pipeline</code> will internally initialize both model and
tokenizer for us and also merge back word pieces into complete
words.</p>
<p>In this case again we use <code>bert-base-cased</code>, which refers
to the vanilla BERT English model. Once we declared a pipeline, we can
feed it with sentences that contain one masked token at a time (beware
that BERT can only predict one word at a time, since that was its
training scheme). For example:</p>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a><span class="kw">def</span> pretty_print_outputs(sentences, model_outputs):</span>
<span id="cb13-4"><a href="#cb13-4" tabindex="-1"></a>    <span class="cf">for</span> i, model_out <span class="kw">in</span> <span class="bu">enumerate</span>(model_outputs):</span>
<span id="cb13-5"><a href="#cb13-5" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">=====</span><span class="ch">\t</span><span class="st">"</span>,sentences[i])</span>
<span id="cb13-6"><a href="#cb13-6" tabindex="-1"></a>        <span class="cf">for</span> label_scores <span class="kw">in</span> model_out:</span>
<span id="cb13-7"><a href="#cb13-7" tabindex="-1"></a>            <span class="bu">print</span>(label_scores)</span>
<span id="cb13-8"><a href="#cb13-8" tabindex="-1"></a></span>
<span id="cb13-9"><a href="#cb13-9" tabindex="-1"></a></span>
<span id="cb13-10"><a href="#cb13-10" tabindex="-1"></a>nlp <span class="op">=</span> pipeline(task<span class="op">=</span><span class="st">"fill-mask"</span>, model<span class="op">=</span><span class="st">"bert-base-cased"</span>, tokenizer<span class="op">=</span><span class="st">"bert-base-cased"</span>)</span>
<span id="cb13-11"><a href="#cb13-11" tabindex="-1"></a>sentences <span class="op">=</span> [<span class="st">"Paris is the [MASK] of France"</span>, <span class="st">"I want to eat a cold [MASK] this afternoon"</span>, <span class="st">"Maria [MASK] Groningen"</span>]</span>
<span id="cb13-12"><a href="#cb13-12" tabindex="-1"></a>model_outputs <span class="op">=</span> nlp(sentences, top_k<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb13-13"><a href="#cb13-13" tabindex="-1"></a>pretty_print_outputs(sentences, model_outputs)</span></code></pre>
</div>
<pre><code>=====	 Paris is the [MASK] of France
{'score': 0.9807965755462646, 'token': 2364, 'token_str': 'capital', 'sequence': 'Paris is the capital of France'}
{'score': 0.004513159394264221, 'token': 6299, 'token_str': 'Capital', 'sequence': 'Paris is the Capital of France'}
{'score': 0.004281804896891117, 'token': 2057, 'token_str': 'center', 'sequence': 'Paris is the center of France'}
{'score': 0.002848200500011444, 'token': 2642, 'token_str': 'centre', 'sequence': 'Paris is the centre of France'}
{'score': 0.0022805952467024326, 'token': 1331, 'token_str': 'city', 'sequence': 'Paris is the city of France'}

=====	 I want to eat a cold [MASK] this afternoon
{'score': 0.19168031215667725, 'token': 13473, 'token_str': 'pizza', 'sequence': 'I want to eat a cold pizza this afternoon'}
{'score': 0.14800849556922913, 'token': 25138, 'token_str': 'turkey', 'sequence': 'I want to eat a cold turkey this afternoon'}
{'score': 0.14620967209339142, 'token': 14327, 'token_str': 'sandwich', 'sequence': 'I want to eat a cold sandwich this afternoon'}
{'score': 0.09997560828924179, 'token': 5953, 'token_str': 'lunch', 'sequence': 'I want to eat a cold lunch this afternoon'}
{'score': 0.06001955270767212, 'token': 4014, 'token_str': 'dinner', 'sequence': 'I want to eat a cold dinner this afternoon'}

=====	 Maria [MASK] Groningen
{'score': 0.24399833381175995, 'token': 117, 'token_str': ',', 'sequence': 'Maria, Groningen'}
{'score': 0.12300779670476913, 'token': 1104, 'token_str': 'of', 'sequence': 'Maria of Groningen'}
{'score': 0.11991506069898605, 'token': 1107, 'token_str': 'in', 'sequence': 'Maria in Groningen'}
{'score': 0.07722211629152298, 'token': 1306, 'token_str': '##m', 'sequence': 'Mariam Groningen'}
{'score': 0.0632941722869873, 'token': 118, 'token_str': '-', 'sequence': 'Maria - Groningen'}
</code></pre>
<p>When we call the <code>nlp</code> pipeline, requesting to return the
<code>top_k</code> most likely suggestions to complete the provided
sentences (in this case <code>k=5</code>). The pipeline returns a list
of outputs as python dictionaries. Depending on the task, the fields of
the dictionary will differ. In this case, the <code>fill-mask</code>
task returns a score (between 0 and 1, the higher the score the more
likely the token is), a tokenId, and its corresponding string, as well
as the full “unmasked” sequence.</p>
<p>In the list of outputs we can observe: the first example shows
correctly that the missing token in the first sentence is
<em>capital</em>, the second example is a bit more ambiguous, but the
model at least uses the context to correctly predict a series of items
that can be eaten (unfortunately, none of its suggestions sound very
tasty); finally, the third example gives almost no useful context so the
model plays it safe and only suggests prepositions or punctuation. This
already shows some of the weaknesses of the approach.</p>
<p>We will next see the case of combining BERT with a classifier on
top.</p>
</div>
<div class="section level1">
<h1 id="bert-for-text-classification">BERT for Text Classification<a class="anchor" aria-label="anchor" href="#bert-for-text-classification"></a></h1>
<p>The task of text classification is assigning a label to a whole
sequence of tokens, for example a sentence. With the parameter
<code>task="text_classification"</code> the <code>pipeline()</code>
function will load the base model and automatically add a linear layer
with a softmax on top. This layer can be fine-tuned with our own labeled
data or we can also directly load the fully pre-trained text
classification models that are already available in HuggingFace.</p>
<figure><img src="fig/bert4.png" alt="BERT as an Emotion Classifier" class="figure mx-auto d-block"><div class="figcaption">BERT as an Emotion Classifier</div>
</figure><p>Let’s see the example of a ready pre-trained emotion classifier based
on <code>RoBERTa</code> model. This model was fine-tuned in the Go
emotions <a href="https://huggingface.co/datasets/google-research-datasets/go_emotions" class="external-link">dataset</a>,
taken from English Reddit and labeled for 28 different emotions at the
sentence level. The fine-tuned model is called <a href="https://huggingface.co/SamLowe/roberta-base-go_emotions" class="external-link">roberta-base-go_emotions</a>.
This model takes a sentence as input and ouputs a probability
distribution over the 28 possible emotions that might be conveyed in the
text. For example:</p>
<div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a></span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a>classifier <span class="op">=</span> pipeline(task<span class="op">=</span><span class="st">"text-classification"</span>, model<span class="op">=</span><span class="st">"SamLowe/roberta-base-go_emotions"</span>, top_k<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb15-3"><a href="#cb15-3" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" tabindex="-1"></a>sentences <span class="op">=</span> [<span class="st">"I am not having a great day"</span>, <span class="st">"This is a lovely and innocent sentence"</span>, <span class="st">"Maria loves Groningen"</span>]</span>
<span id="cb15-5"><a href="#cb15-5" tabindex="-1"></a>model_outputs <span class="op">=</span> classifier(sentences)</span>
<span id="cb15-6"><a href="#cb15-6" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" tabindex="-1"></a>pretty_print_outputs(sentences, model_outputs)</span></code></pre>
</div>
<pre><code>=====	 I am not having a great day
{'label': 'disappointment', 'score': 0.46669483184814453}
{'label': 'sadness', 'score': 0.39849498867988586}
{'label': 'annoyance', 'score': 0.06806594133377075}

=====	 This is a lovely and innocent sentence
{'label': 'admiration', 'score': 0.6457845568656921}
{'label': 'approval', 'score': 0.5112180113792419}
{'label': 'love', 'score': 0.09214121848344803}

=====	 Maria loves Groningen
{'label': 'love', 'score': 0.8922032117843628}
{'label': 'neutral', 'score': 0.10132959485054016}
{'label': 'approval', 'score': 0.02525361441075802}</code></pre>
<p>This code outputs again a list of dictionaries with the
<code>top-k</code> (<code>k=3</code>) emotions that each of the two
sentences convey. In this case, the first sentence evokes (in order of
likelihood) <em>dissapointment</em>, <em>sadness</em> and
<em>annoyance</em>; whereas the second sentence evokes <em>love</em>,
<em>neutral</em> and <em>approval</em>. Note however that the likelihood
of each prediction decreases dramatically below the top choice, so
perhaps this specific classifier is only useful for the top emotion.</p>
<div id="callout4" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div class="callout-inner">
<div class="callout-content">
<p>Finetunning BERT is very cheap, because we only need to train the
<em>classifier</em> layer, a very small neural network, that can learn
to choose between the classes (labels) for your custom classification
problem, without needing a big amount of annotated data. This classifier
is just a one-layer neural layer with a softmax that assigns a score
that can be translated to the probability over a set of labels, given
the input features provided by BERT, which <em>encodes</em> the meaning
of the entire sequence in its hidden states.</p>
</div>
</div>
</div>
<figure><img src="fig/bert4b.png" alt="BERT as an Emotion Classifier" class="figure mx-auto d-block"><div class="figcaption">BERT as an Emotion Classifier</div>
</figure></div>
<div class="section level1">
<h1 id="bert-for-token-classification">BERT for Token Classification<a class="anchor" aria-label="anchor" href="#bert-for-token-classification"></a></h1>
<p>Just as we plugged in a trainable text classifier layer, we can add a
token-level classifier that assigns a class to each of the tokens
encoded by a transformer (as opposed to one label for the whole
sequence). A specific example of this task is Named Entity Recognition,
but you can basically define any task that requires to
<em>highlight</em> sub-strings of text and classify them using this
technique.</p>
<div class="section level2">
<h2 id="named-entity-recognition">Named Entity Recognition<a class="anchor" aria-label="anchor" href="#named-entity-recognition"></a></h2>
<p>Named Entity Recognition (NER) is the task of recognizing mentions of
real-world entities inside a text. The concept of
<strong>Entity</strong> includes proper names that unequivocally
identify a unique individual (PER), place (LOC), organization (ORG), or
other object/name (MISC). Depending on the domain, the concept can
expanded to recognize other unique (and more conceptual) entities such
as DATE, MONEY, WORK_OF_ART, DISEASE, PROTEIN_TYPE, etcetera…</p>
<p>In terms of NLP, this boils down to classifying each token into a
series of labels (<code>PER</code>, <code>LOC</code>, <code>ORG</code>,
<code>O</code>[no-entity] ). Since a single entity can be expressed with
multiple words (e.g. New York) the usual notation used for labeling the
text is IOB (<strong>I</strong>nner <strong>O</strong>ut
<strong>B</strong>eginnig of entity) notations which identifies the
limits of each entity tokens. For example:</p>
<figure><img src="fig/bert5.png" alt="BERT as an NER Classifier" class="figure mx-auto d-block"><div class="figcaption">BERT as an NER Classifier</div>
</figure><p>This is a typical sequence classification problem where an imput
sequence must be fully mapped into an output sequence of labels with
global constraints (for example, there can’t be an inner I-LOC label
before a beginning B-LOC label). Since the labels of the tokens are
context dependent, a language model with attention mechanism such as
BERT is very beneficial for a task like NER.</p>
<p>Because this is one of the core tasks in NLP, there are dozens of
pre-trained NER classifiers in HuggingFace that you can use right away.
We use once again the <code>pipeline()</code> to run the model for
predictions in your custom data, in this case with
<code>task="ner"</code>. For example:</p>
<div class="codewrapper sourceCode" id="cb17">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer, AutoModelForTokenClassification</span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb17-3"><a href="#cb17-3" tabindex="-1"></a></span>
<span id="cb17-4"><a href="#cb17-4" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(<span class="st">"dslim/bert-base-NER"</span>)</span>
<span id="cb17-5"><a href="#cb17-5" tabindex="-1"></a>model <span class="op">=</span> AutoModelForTokenClassification.from_pretrained(<span class="st">"dslim/bert-base-NER"</span>)</span>
<span id="cb17-6"><a href="#cb17-6" tabindex="-1"></a></span>
<span id="cb17-7"><a href="#cb17-7" tabindex="-1"></a>ner_classifier <span class="op">=</span> pipeline(<span class="st">"token-classification"</span>, model<span class="op">=</span>model, tokenizer<span class="op">=</span>tokenizer)</span>
<span id="cb17-8"><a href="#cb17-8" tabindex="-1"></a>example <span class="op">=</span> <span class="st">"My name is Wolfgang Schmid and I live in Berlin"</span></span>
<span id="cb17-9"><a href="#cb17-9" tabindex="-1"></a></span>
<span id="cb17-10"><a href="#cb17-10" tabindex="-1"></a>ner_results <span class="op">=</span> ner_classifier(example)</span>
<span id="cb17-11"><a href="#cb17-11" tabindex="-1"></a><span class="cf">for</span> nr <span class="kw">in</span> ner_results:</span>
<span id="cb17-12"><a href="#cb17-12" tabindex="-1"></a>    <span class="bu">print</span>(nr)</span></code></pre>
</div>
<p>The code prints the following:</p>
<pre><code>{'entity': 'B-PER', 'score': 0.9996068, 'index': 4, 'word': 'Wolfgang', 'start': 11, 'end': 19}
{'entity': 'I-PER', 'score': 0.999582, 'index': 5, 'word': 'Sc', 'start': 20, 'end': 22}
{'entity': 'I-PER', 'score': 0.9990482, 'index': 6, 'word': '##hm', 'start': 22, 'end': 24}
{'entity': 'I-PER', 'score': 0.9951691, 'index': 7, 'word': '##id', 'start': 24, 'end': 26}
{'entity': 'B-LOC', 'score': 0.99956733, 'index': 12, 'word': 'Berlin', 'start': 41, 'end': 47}</code></pre>
<p>In this case the output of the pipeline is a list of dictionaries,
each one representing only entity <code>IOB</code> labels at the BERT
token level. IMPORTANT: this list is per wordPiece and NOT per <em>human
word</em> even if the provided text is pre-tokenized. You can assume all
of the tokens that don’t appear in the output were labeled as no-entity,
that is <code>"O"</code>. To recover the full-word entities you can
initialize the pipeline with
<code>aggregation_strategy="first"</code>:</p>
<div class="codewrapper sourceCode" id="cb19">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" tabindex="-1"></a>ner_classifier <span class="op">=</span> pipeline(<span class="st">"token-classification"</span>, model<span class="op">=</span>model, tokenizer<span class="op">=</span>tokenizer, aggregation_strategy<span class="op">=</span><span class="st">"first"</span>)</span>
<span id="cb19-2"><a href="#cb19-2" tabindex="-1"></a>example <span class="op">=</span> <span class="st">"My name is Wolfgang Schmid and I live in Berlin"</span></span>
<span id="cb19-3"><a href="#cb19-3" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" tabindex="-1"></a>ner_results <span class="op">=</span> ner_classifier(example)</span>
<span id="cb19-5"><a href="#cb19-5" tabindex="-1"></a><span class="cf">for</span> nr <span class="kw">in</span> ner_results:</span>
<span id="cb19-6"><a href="#cb19-6" tabindex="-1"></a>    <span class="bu">print</span>(nr)</span></code></pre>
</div>
<p>The code now prints the following:</p>
<pre><code>{'entity_group': 'PER', 'score': 0.9995944, 'word': 'Wolfgang Schmid', 'start': 11, 'end': 26}
{'entity_group': 'LOC', 'score': 0.99956733, 'word': 'Berlin', 'start': 41, 'end': 47}</code></pre>
<p>As you can see, entities aggregated at the Span Leven (instead of the
Token Level). Word pieces are merged back into <em>human words</em> and
also multiword entities are assigned a single entity label unifying the
<code>IOB</code> labels into one. Depending on your use case you can
request the pipeline to give different
<code>aggregation_strateg[ies]</code>. More info about the pipeline can
be found <a href="https://huggingface.co/docs/transformers/main_classes/pipelines" class="external-link">here</a>.</p>
<p>The next step is crucial: evaluate how does the pre-trained model
actually performs in <strong>your dataset</strong>. This is important
since the fine-tuned model could be overfitted to other custom
benchmarks that do not share the characteristics of your dataset.</p>
<p>To observe this, we can first see the performance on the test portion
of the dataset in which this classifier was trained, and then evaluate
the same pre-trained classifier on a NER dataset form a different
domain.</p>
</div>
<div class="section level2">
<h2 id="model-evaluation">Model Evaluation<a class="anchor" aria-label="anchor" href="#model-evaluation"></a></h2>
<p>To perform evaluation in your data you can use again the
<code>seqeval</code> package:</p>
<div class="codewrapper sourceCode" id="cb21">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" tabindex="-1"></a></span>
<span id="cb21-2"><a href="#cb21-2" tabindex="-1"></a><span class="im">from</span> seqeval.metrics <span class="im">import</span> classification_report</span>
<span id="cb21-3"><a href="#cb21-3" tabindex="-1"></a><span class="bu">print</span>(classification_report(gold_labels, model_predictions))</span></code></pre>
</div>
<p>Since we took a classifier that was not trained for the book domain,
the performance is quite poor. But this example shows us that
classifiers performing very well on their own domain most of the times
transfer poorly to other apparently similar datasets.</p>
<p>The solution in this case is to use another of the great
characteristics of BERT: fine-tuning for domain adaptation. It is
possible to train your own classifier with relatively small data (given
that a lot of linguistic knowledge was already provided during the
language modeling pre-training). In the following section we will see
how to train your own NER model and use it for predictions.</p>
</div>
</div>



      </div> <!-- / div.lesson-content -->
    </main><!-- / main#main-content.main-content --><nav class="bottom-pagination mx-md-4" aria-label="Previous and Next Chapter"><div class="d-block d-sm-block d-md-none">
        <a class="chapter-link" href="02-word_representations.html"><i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>Previous</a>
        <a class="chapter-link float-end" href="04-LargeLanguageModels.html">Next<i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i></a>
      </div>
      <!-- content for large screens -->
      <div class="d-none d-sm-none d-md-block">
        <a class="chapter-link" href="02-word_representations.html" rel="prev">
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>
          Previous: From words to
        </a>
        <a class="chapter-link float-end" href="04-LargeLanguageModels.html" rel="next">
          Next: Episode 3: Using...
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i>
        </a>
      </div>
    </nav></div> <!-- / div.primary-content.col-xs-12 -->
<!-- END:   inst/pkgdown/templates/content-instructor.html-->

      </div><!--/div.row-->
      		<footer class="row footer mx-md-3"><hr><div class="col-md-6">
        <p>This lesson is subject to the <a href="CODE_OF_CONDUCT.html">Code of Conduct</a></p>
        <p>

        <a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/edit/main/episodes/03-transformers.md" class="external-link">Edit on GitHub</a>

	
        | <a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/blob/main/CONTRIBUTING.md" class="external-link">Contributing</a>
        | <a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/" class="external-link">Source</a></p>
				<p><a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/blob/main/CITATION.cff" class="external-link">Cite</a> | <a href="mailto:team@carpentries.org">Contact</a> | <a href="https://carpentries.org/about/" class="external-link">About</a></p>
			</div>
			<div class="col-md-6">

        <p>Materials licensed under <a href="LICENSE.html">CC-BY 4.0</a> by the authors</p>

        <p>Template licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/" class="external-link">CC-BY 4.0</a> by <a href="https://carpentries.org/" class="external-link">The Carpentries</a></p>
        <p>Built with <a href="https://github.com/carpentries/sandpaper/tree/0.17.1" class="external-link">sandpaper (0.17.1)</a>, <a href="https://github.com/carpentries/pegboard/tree/0.7.9" class="external-link">pegboard (0.7.9)</a>, and <a href="https://github.com/carpentries/varnish/tree/1.0.7" class="external-link">varnish (1.0.7)</a></p>
			</div>
		</footer></div> <!-- / div.container -->
	<div id="to-top">
		<a href="#top">
      <i class="search-icon" data-feather="arrow-up" role="img" aria-label="Back To Top"></i><br><!-- <span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top --><span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top
		</a>
	</div>
  <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "LearningResource",
  "@id": "https://esciencecenter-digital-skills.github.io/Natural-language-processing/03-transformers.html",
  "inLanguage": "en",
  "dct:conformsTo": "https://bioschemas.org/profiles/LearningResource/1.0-RELEASE",
  "description": "A Carpentries Lesson teaching foundational data and coding skills to researchers worldwide",
  "keywords": "software, data, lesson, The Carpentries, NLP, English, social sciences, pre-alpha",
  "name": "Transformers: BERT and Beyond",
  "creativeWorkStatus": "active",
  "url": "https://esciencecenter-digital-skills.github.io/Natural-language-processing/03-transformers.html",
  "identifier": "https://esciencecenter-digital-skills.github.io/Natural-language-processing/03-transformers.html",
  "dateCreated": "2024-04-24",
  "dateModified": "2025-09-24",
  "datePublished": "2025-11-18"
}

  </script><script>
		feather.replace();
	</script></body></html><!-- END:   inst/pkgdown/templates/layout.html-->

