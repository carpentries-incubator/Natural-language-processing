<!DOCTYPE html>
<!-- START: inst/pkgdown/templates/layout.html --><!-- Generated by pkgdown: do not edit by hand --><html lang="en" data-bs-theme="auto"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><title>Fundamentals of Natural Language Processing (NLP) in Python: From words to vectors</title><meta name="viewport" content="width=device-width, initial-scale=1"><script src="assets/themetoggle.js"></script><link rel="stylesheet" type="text/css" href="assets/styles.css"><script src="assets/scripts.js" type="text/javascript"></script><!-- mathjax --><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      config: ["MMLorHTML.js"],
      jax: ["input/TeX","input/MathML","output/HTML-CSS","output/NativeMML", "output/PreviewHTML"],
      extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
      TeX: {
        extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
      },
      tex2jax: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [ ['$$','$$'], ['\\[', '\\]'] ],
        processEscapes: true
      }
    });
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><!-- Responsive Favicon for The Carpentries --><link rel="apple-touch-icon" sizes="180x180" href="favicons/incubator/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="favicons/incubator/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="favicons/incubator/favicon-16x16.png"><link rel="manifest" href="favicons/incubator/site.webmanifest"><link rel="mask-icon" href="favicons/incubator/safari-pinned-tab.svg" color="#5bbad5"><meta name="msapplication-TileColor" content="#da532c"><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"><meta name="theme-color" media="(prefers-color-scheme: dark)" content="black"></head><body>
    <header id="top" class="navbar navbar-expand-md top-nav incubator"><svg xmlns="http://www.w3.org/2000/svg" class="d-none"><symbol id="check2" viewbox="0 0 16 16"><path d="M13.854 3.646a.5.5 0 0 1 0 .708l-7 7a.5.5 0 0 1-.708 0l-3.5-3.5a.5.5 0 1 1 .708-.708L6.5 10.293l6.646-6.647a.5.5 0 0 1 .708 0z"></path></symbol><symbol id="circle-half" viewbox="0 0 16 16"><path d="M8 15A7 7 0 1 0 8 1v14zm0 1A8 8 0 1 1 8 0a8 8 0 0 1 0 16z"></path></symbol><symbol id="moon-stars-fill" viewbox="0 0 16 16"><path d="M6 .278a.768.768 0 0 1 .08.858 7.208 7.208 0 0 0-.878 3.46c0 4.021 3.278 7.277 7.318 7.277.527 0 1.04-.055 1.533-.16a.787.787 0 0 1 .81.316.733.733 0 0 1-.031.893A8.349 8.349 0 0 1 8.344 16C3.734 16 0 12.286 0 7.71 0 4.266 2.114 1.312 5.124.06A.752.752 0 0 1 6 .278z"></path><path d="M10.794 3.148a.217.217 0 0 1 .412 0l.387 1.162c.173.518.579.924 1.097 1.097l1.162.387a.217.217 0 0 1 0 .412l-1.162.387a1.734 1.734 0 0 0-1.097 1.097l-.387 1.162a.217.217 0 0 1-.412 0l-.387-1.162A1.734 1.734 0 0 0 9.31 6.593l-1.162-.387a.217.217 0 0 1 0-.412l1.162-.387a1.734 1.734 0 0 0 1.097-1.097l.387-1.162zM13.863.099a.145.145 0 0 1 .274 0l.258.774c.115.346.386.617.732.732l.774.258a.145.145 0 0 1 0 .274l-.774.258a1.156 1.156 0 0 0-.732.732l-.258.774a.145.145 0 0 1-.274 0l-.258-.774a1.156 1.156 0 0 0-.732-.732l-.774-.258a.145.145 0 0 1 0-.274l.774-.258c.346-.115.617-.386.732-.732L13.863.1z"></path></symbol><symbol id="sun-fill" viewbox="0 0 16 16"><path d="M8 12a4 4 0 1 0 0-8 4 4 0 0 0 0 8zM8 0a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 0zm0 13a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 13zm8-5a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2a.5.5 0 0 1 .5.5zM3 8a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2A.5.5 0 0 1 3 8zm10.657-5.657a.5.5 0 0 1 0 .707l-1.414 1.415a.5.5 0 1 1-.707-.708l1.414-1.414a.5.5 0 0 1 .707 0zm-9.193 9.193a.5.5 0 0 1 0 .707L3.05 13.657a.5.5 0 0 1-.707-.707l1.414-1.414a.5.5 0 0 1 .707 0zm9.193 2.121a.5.5 0 0 1-.707 0l-1.414-1.414a.5.5 0 0 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .707zM4.464 4.465a.5.5 0 0 1-.707 0L2.343 3.05a.5.5 0 1 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .708z"></path></symbol></svg><a class="visually-hidden-focusable skip-link" href="#main-content">Skip to main content</a>
  <div class="container-fluid top-nav-container">
    <div class="col-md-8">
      <div class="large-logo">
        <img id="incubator-logo" alt="Carpentries Incubator" src="assets/images/incubator-logo.svg"><span class="badge text-bg-danger">
          <abbr title="This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught.">
            <a href="https://docs.carpentries.org/resources/curriculum/lesson-life-cycle.html" class="external-link alert-link">
              <i aria-hidden="true" class="icon" data-feather="alert-octagon" style="border-radius: 5px"></i>
              Pre-Alpha
            </a>
            <span class="visually-hidden">This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught.</span>
          </abbr>
        </span>

      </div>
    </div>
    <div class="selector-container">
      <div id="theme-selector">
        <li class="nav-item dropdown" id="theme-button-list">
          <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
            <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="bd-theme-text"><li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                Light
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                Dark
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                Auto
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
          </ul></li>
      </div>

      <div class="dropdown" id="instructor-dropdown">
        <button class="btn btn-secondary dropdown-toggle bordered-button" type="button" id="dropdownMenu1" data-bs-toggle="dropdown" aria-expanded="false">
          <i aria-hidden="true" class="icon" data-feather="eye"></i> Learner View <i data-feather="chevron-down"></i>
        </button>
        <ul class="dropdown-menu" aria-labelledby="dropdownMenu1"><li><button class="dropdown-item" type="button" onclick="window.location.href='instructor/02-word_representations.html';">Instructor View</button></li>
        </ul></div>
    </div>
  </div>
  <hr></header><nav class="navbar navbar-expand-xl bottom-nav incubator" aria-label="Main Navigation"><div class="container-fluid nav-container">
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle Navigation">
      <span class="navbar-toggler-icon"></span>
      <span class="menu-title">Menu</span>
    </button>
    <div class="nav-logo">
      <img class="small-logo" alt="Carpentries Incubator" src="assets/images/incubator-logo-sm.svg"></div>
    <div class="lesson-title-md">
      Fundamentals of Natural Language Processing (NLP) in Python
    </div>
    <div class="search-icon-sm">
      <!-- TODO: do not show until we have search
        <i role="img" aria-label="Search the All In One page" data-feather="search"></i>
      -->
    </div>
    <div class="desktop-nav">
      <ul class="navbar-nav me-auto mb-2 mb-lg-0"><li class="nav-item">
          <span class="lesson-title">
            Fundamentals of Natural Language Processing (NLP) in Python
          </span>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="key-points.html">Key Points</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="reference.html#glossary">Glossary</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="profiles.html">Learner Profiles</a>
        </li>
        <li class="nav-item dropdown">
          <button class="nav-link dropdown-toggle" id="navbarDropdown" data-bs-toggle="dropdown" aria-expanded="false">
            More <i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu" aria-labelledby="navbarDropdown"></ul></li>
      </ul></div>
    <!--
    <form class="d-flex col-md-2 search-form">
      <fieldset disabled>
      <input class="form-control me-2 searchbox" type="search" placeholder="" aria-label="">
        <button class="btn btn-outline-success tablet-search-button"  type="submit">
          <i class="search-icon" data-feather="search" role="img" aria-label="Search the All In One page"></i>
        </button>
      </fieldset>
    </form>
    -->
    <a id="search-button" class="btn btn-primary" href="aio.html" role="button" aria-label="Search the All In One page">Search the All In One page</a>
  </div><!--/div.container-fluid -->
</nav><div class="col-md-12 mobile-title">
  Fundamentals of Natural Language Processing (NLP) in Python
</div>

<aside class="col-md-12 lesson-progress"><div style="width: 29%" class="percentage">
    29%
  </div>
  <div class="progress incubator">
    <div class="progress-bar incubator" role="progressbar" style="width: 29%" aria-valuenow="29" aria-label="Lesson Progress" aria-valuemin="0" aria-valuemax="100">
    </div>
  </div>
</aside><div class="container">
      <div class="row">
        <!-- START: inst/pkgdown/templates/navbar.html -->
<div id="sidebar-col" class="col-lg-4">
  <div id="sidebar" class="sidebar">
      <nav aria-labelledby="flush-headingEleven"><button role="button" aria-label="close menu" alt="close menu" aria-expanded="true" aria-controls="sidebar" class="collapse-toggle" data-collapse="Collapse " data-episodes="Episodes ">
          <i class="search-icon" data-feather="x" role="img"></i>
        </button>
        <div class="sidebar-inner">
          <div class="row mobile-row" id="theme-row-mobile">
            <div class="col" id="theme-selector">
              <li class="nav-item dropdown" id="theme-button-list">
                <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
                  <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><span class="d-lg-none ms-1" id="bd-theme-text">Toggle Theme</span>
                </button>
                <ul class="dropdown-menu dropdown-menu-right" aria-labelledby="bd-theme-text"><li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                      Light
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                      Dark
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                      <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                      Auto
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                </ul></li>
            </div>
          </div>
          <div class="row mobile-row">
            <div class="col">
              <div class="sidenav-view-selector">
                <div class="accordion accordion-flush" id="accordionFlush9">
                  <div class="accordion-item">
                    <h2 class="accordion-header" id="flush-headingNine">
                      <button class="accordion-button collapsed" id="instructor" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseNine" aria-expanded="false" aria-controls="flush-collapseNine">
                        <i id="eye" aria-hidden="true" class="icon" data-feather="eye"></i> Learner View
                      </button>
                    </h2>
                    <div id="flush-collapseNine" class="accordion-collapse collapse" aria-labelledby="flush-headingNine" data-bs-parent="#accordionFlush2">
                      <div class="accordion-body">
                        <a href="instructor/02-word_representations.html">Instructor View</a>
                      </div>
                    </div>
                  </div><!--/div.accordion-item-->
                </div><!--/div.accordion-flush-->
              </div><!--div.sidenav-view-selector -->
            </div><!--/div.col -->

            <hr></div><!--/div.mobile-row -->

          <div class="accordion accordion-flush" id="accordionFlush11">
            <div class="accordion-item">

              <button id="chapters" class="accordion-button show" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseEleven" aria-expanded="false" aria-controls="flush-collapseEleven">
                <h2 class="accordion-header chapters" id="flush-headingEleven">
                  EPISODES
                </h2>
              </button>
              <div id="flush-collapseEleven" class="accordion-collapse show collapse" aria-labelledby="flush-headingEleven" data-bs-parent="#accordionFlush11">

                <div class="accordion-body">
                  <div class="accordion accordion-flush" id="accordionFlush1">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading1">
        <a href="index.html">Summary and Setup</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush2">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading2">
        <a href="01-introduction.html">1. Introduction</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlushcurrent">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-headingcurrent">
      <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapsecurrent" aria-expanded="true" aria-controls="flush-collapsecurrent">
        <span class="visually-hidden">Current Chapter</span>
        <span class="current-chapter">
        2. From words to vectors
        </span>
      </button>
    </div><!--/div.accordion-header-->

    <div id="flush-collapsecurrent" class="accordion-collapse collapse show" aria-labelledby="flush-headingcurrent" data-bs-parent="#accordionFlushcurrent">
      <div class="accordion-body">
        <ul><li><a href="#introduction">Introduction</a></li>
<li><a href="#preprocessing-operations">Preprocessing Operations</a></li>
<li><a href="#nlp-pipeline">NLP Pipeline</a></li>
<li><a href="#word-embeddings">Word Embeddings</a></li>
<li><a href="#the-word2vec-vector-space">The Word2Vec Vector Space</a></li>
<li><a href="#train-your-own-word2vec">Train your own Word2Vec</a></li>
        </ul></div><!--/div.accordion-body-->
    </div><!--/div.accordion-collapse-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush4">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading4">
        <a href="03-transformers.html">3. Transformers: BERT and Beyond</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush5">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading5">
        <a href="04-LargeLanguageModels.html">4. Episode 3: Using large language models</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

                </div>
              </div>
            </div>

            <hr class="half-width"><div class="accordion accordion-flush lesson-resources" id="accordionFlush12">
              <div class="accordion-item">
                <h2 class="accordion-header" id="flush-headingTwelve">
                  <button class="accordion-button collapsed" id="lesson-resources" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseTwelve" aria-expanded="false" aria-controls="flush-collapseTwelve">
                    RESOURCES
                  </button>
                </h2>
                <div id="flush-collapseTwelve" class="accordion-collapse collapse" aria-labelledby="flush-headingTwelve" data-bs-parent="#accordionFlush12">
                  <div class="accordion-body">
                    <ul><li>
                        <a href="key-points.html">Key Points</a>
                      </li>
                      <li>
                        <a href="reference.html#glossary">Glossary</a>
                      </li>
                      <li>
                        <a href="profiles.html">Learner Profiles</a>
                      </li>

                    </ul></div>
                </div>
              </div>
            </div>
            <hr class="half-width lesson-resources"><a href="aio.html">See all in one page</a>


            <hr class="d-none d-sm-block d-md-none"><div class="d-grid gap-1">

            </div>
          </div><!-- /div.accordion -->
        </div><!-- /div.sidebar-inner -->
      </nav></div><!-- /div.sidebar -->
  </div><!-- /div.sidebar-col -->
<!-- END:   inst/pkgdown/templates/navbar.html-->

        <!-- START: inst/pkgdown/templates/content-instructor.html -->
  <div class="col-xl-8 col-lg-12 primary-content">
    <nav class="lesson-content mx-md-4" aria-label="Previous and Next Chapter"><!-- content for small screens --><div class="d-block d-sm-block d-md-none">
        <a class="chapter-link" href="01-introduction.html"><i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>Previous</a>
        <a class="chapter-link float-end" href="03-transformers.html">Next<i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i></a>
      </div>
      <!-- content for large screens -->
      <div class="d-none d-sm-none d-md-block">
        <a class="chapter-link" href="01-introduction.html" rel="prev">
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>
          Previous: Introduction
        </a>
        <a class="chapter-link float-end" href="03-transformers.html" rel="next">
          Next: Transformers: BERT...
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i>
        </a>
      </div>
      <hr></nav><main id="main-content" class="main-content"><div class="container lesson-content">
        <h1>From words to vectors</h1>
        <p>Last updated on 2025-10-24 |

        <a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/edit/main/episodes/02-word_representations.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>



        <div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>

        

<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul><li>What operations should I perform to get clean text?</li>
<li>What properties do word embeddings have?</li>
<li>What is a word2vec model?</li>
<li>What insights can I get from word embeddings?</li>
<li>How do I train my own word2vec model?</li>
</ul></div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<p>After following this lesson, learners will be able to:</p>
<ul><li>Learn basic preprocessing operations</li>
<li>Implement a basic NLP Pipeline</li>
<li>Understand the concept of word embeddings</li>
<li>Use and Explore word2vec models</li>
<li>Train your own word2vec</li>
</ul></div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a></h2>
<hr class="half-width"><p>In this episode, we will learn to apply preprocessing operations to
our text files. We will visit the concept of NLP Pipelines and learn
about their basic components.</p>
<p>We will then explain the transition from text-based NLP into word
embeddings. We will visit the Word2Vec vector space, a method to
represent words proposed in 2013 by <a href="https://arxiv.org/pdf/1301.3781" class="external-link">Mikolov et al</a>, where instead
of counting word co-occurrences they trained a neural network on large
amounts of text to predict the context windows. By doing this, they
obtained continuous vectors that represent words, which hold interesting
semantic properties.</p>
</section><section><h2 class="section-heading" id="preprocessing-operations">Preprocessing Operations<a class="anchor" aria-label="anchor" href="#preprocessing-operations"></a></h2>
<hr class="half-width"><p>As in most data science and machine learning scenarios, usually your
text data sources won’t be in the exact shape that you need.
Preprocessing operations in NLP are analogue to the data cleaning and
sanitation step in any Machine Learning task. Whether you need to
perform certain preprocessing operations and the order of them will
depend on your NLP task at hand.</p>
<p>Also note that preprocessing can differ significantly if you work
with different languages. This is both in terms of which steps to apply,
but also which methods to use for a specific step.</p>
<p>Here we will analyze with more detail the most common pre-processing
steps when dealing with unstructured English text data:</p>
<div class="section level3">
<h3 id="data-formatting">Data Formatting<a class="anchor" aria-label="anchor" href="#data-formatting"></a></h3>
<p>Text comes in different formats (Microsoft Word documents, PDF
documents, ePub files, plain text etc…). The first step is to obtain a
clean text representation that can be transferred into python UTF-8
strings that our scripts can manipulate.</p>
<p>Take a look at the
<code>episodes/data/84_frankenstein_or_the_modern_prometheus.txt</code>
file:</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a>filename <span class="op">=</span> <span class="st">"data/84_frankenstein_or_the_modern_prometheus.txt"</span></span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(filename, <span class="st">'r'</span>, encoding<span class="op">=</span><span class="st">'utf-8'</span>) <span class="im">as</span> <span class="bu">file</span>:</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a>    text <span class="op">=</span> <span class="bu">file</span>.read()</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="bu">print</span>(text[:<span class="dv">300</span>]) <span class="co"># print the first 300 characters</span></span></code></pre>
</div>
<p>Our file is already in plain text so it might seem we do not need to
do anything; however, if we look closer we see new line characters
separating not only paragraphs but breaking the lines in the middle of
sentences. While this is useful to keep the text in a narrow space to
help the human reader, it introduces artificial breaks that can confuse
any automatic analysis (for example to identify where sentences start
and end).</p>
<p>One straightforward way is to replace the new lines with spaces so
all the text is in a single line:</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>text_flat <span class="op">=</span> text.replace(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span>, <span class="st">" "</span>)</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a><span class="bu">print</span>(text_flat[:<span class="dv">300</span>]) <span class="co"># print the first 300 characters</span></span></code></pre>
</div>
<p>Other data formatting operations might include: - Remove special or
noisy characters (think of documents obtained by OCR) - Remove HTML tags
- Strip non-meaningful punctuation (think of dashes separating words
when the line ends) - Strip footnotes and headers - Remove URLs or phone
numbers</p>
<div id="callout1" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div class="callout-inner">
<div class="callout-content">
<p>Another important choice at the data formatting level is to decide at
what granularity do you need to perform the NLP task:</p>
<ul><li>Are you analyzing phenomena at the <strong>word level</strong>?</li>
<li>Do you need to first extract sentences from the text and do analysis
at the <strong>sentence level</strong>?</li>
<li>Do you need full <strong>chunks of text</strong>? (e.g. paragraphs
or chapters?)</li>
<li>Or perhaps you want to extract patterns at the <strong>document
level</strong>? For example each book should have a genre tag.</li>
</ul><p>Sometimes your data will be already available at the desired
granularity level. If this is not the case, then during the tokenization
step you will need to figure out how to obtain the desired granularity
level.</p>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="tokenization">Tokenization<a class="anchor" aria-label="anchor" href="#tokenization"></a></h3>
<p>Tokenization is essential in NLP, as it helps to create structure
from raw text. It involves the segmentation of the text into smaller
units referred as <code>tokens</code>. Tokens can be sentences
(e.g. <code>'the happy cat'</code>), words
(<code>'the', 'happy', 'cat'</code>), subwords
(<code>'un', 'happiness'</code>) or characters
(<code>'c','a', 't'</code>). The choice of tokens depends by the
requirement of the model used for training, and the text.</p>
<p>Python strings are by definition sequences of characters, thus we can
iterate the string char by char:</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">type</span>(text_flat))  <span class="co"># Should be &lt;class 'str'&gt;</span></span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a><span class="cf">for</span> ch <span class="kw">in</span> text_flat:</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>    <span class="bu">print</span>(ch)</span></code></pre>
</div>
<p>However, it is more advantageous if our atomic units are words. As we
know already, the task of extracting words from texts is not trivial,
therefore pre-trained models such as sPaCy can help us with this step.
In this case we will use the small English model that was trained on a
web corpus:</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="im">import</span> spacy</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>nlp <span class="op">=</span> spacy.load(<span class="st">"en_core_web_sm"</span>)</span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a>doc <span class="op">=</span> nlp(text)</span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">type</span>(doc))  <span class="co"># Should be &lt;class 'spacy.tokens.doc.Doc'&gt;</span></span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(doc))</span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a><span class="bu">print</span>(doc)</span></code></pre>
</div>
<div id="callout2" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div class="callout-inner">
<div class="callout-content">
<p>A good word tokenizer for example, does not simply break up a text
based on spaces and punctuation, but it should be able to
distinguish:</p>
<ul><li>abbreviations that include points (e.g.: <em>e.g.</em>)</li>
<li>times (<em>11:15</em>) and dates written in various formats
(<em>01/01/2024</em> or <em>01-01-2024</em>)</li>
<li>word contractions such as <em>don’t</em>, these should be split into
<em>do</em> and <em>n’t</em>
</li>
<li>URLs</li>
</ul><p>Many older tokenizers are rule-based, meaning that they iterate over
a number of predefined rules to split the text into tokens, which is
useful for splitting text into word tokens for example. Modern large
language models use subword tokenization, which learn to break text into
pieces that are statistically convenient, this makes them more flexible
but less human-readable.</p>
</div>
</div>
</div>
<p>We can access the tokens by iterating the document and getting its
<code>.text</code> property:</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a>tokens_txt <span class="op">=</span> [token.text <span class="cf">for</span> token <span class="kw">in</span> doc]</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a><span class="bu">print</span>(tokens_txt[:<span class="dv">15</span>])</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>['Letter', '1', '\n\n\n', 'St.', 'Petersburgh', ',', 'Dec.', '11th', ',', '17', '-', '-', '\n\n', 'TO', 'Mrs.']</code></pre>
</div>
<p>This shows us the individual tokens, including new lines and
punctuation (in case we didn’t run the previous cleaning step). SpaCy
allows us to filter based in token properties. For example, assume we
are not interested in the newlines, punctuation nor in numeric tokens,
so in one single step we can keep only the token objects that contain
alphabetical:</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a>tokens <span class="op">=</span> [token <span class="cf">for</span> token <span class="kw">in</span> doc <span class="cf">if</span> token.is_alpha]</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a><span class="bu">print</span>(tokens[:<span class="dv">15</span>])</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[Letter, Petersburgh, TO, Saville, England, You, will, rejoice, to, hear, that, no, disaster, has, accompanied]</code></pre>
</div>
<p>We do not have to depend necessarily on the <code>Doc</code> and
<code>Token</code> spaCy objects. Once we tokenized the text with the
spaCy model, we can extract the list of words as a list of strings and
continue our text analysis:</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a>words <span class="op">=</span> [token.text <span class="cf">for</span> token <span class="kw">in</span> doc]</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a><span class="bu">print</span>(words[:<span class="dv">20</span>])</span></code></pre>
</div>
<p>Again, it all depends on what your requirements are. For example,
sometimes it is more useful if our atomic units are sentences. Think of
the NLP task of classifying each whole sentence inside a text as
Positive/Negative/Neutral. SpaCy also helps with this using a sentence
segmentation model:</p>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a>sentences <span class="op">=</span> [sent.text <span class="cf">for</span> sent <span class="kw">in</span> doc.sents]</span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a>[<span class="bu">print</span>(s) <span class="cf">for</span> s <span class="kw">in</span> sentences[:<span class="dv">5</span>]]</span></code></pre>
</div>
<p>Note that in this case each sentence is an untokenized string. If you
later are interested in accessing the tokens inside each sentence, you
have to run a word tokenizer on each sentence.</p>
</div>
<div class="section level3">
<h3 id="lowercasing">Lowercasing<a class="anchor" aria-label="anchor" href="#lowercasing"></a></h3>
<p>Removing uppercases to e.g. avoid treating “Dog” and “dog” as two
different words is also a common step, for example to train word vector
representations, we want to merge both occurrences as they represent
exactly the same concept. Lowercasing can be done with python directly
as:</p>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a>lower_text <span class="op">=</span> text_flat.lower()</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a>lower_text[:<span class="dv">100</span>] <span class="co"># Beware that this is a python string operation</span></span></code></pre>
</div>
<p>Beware that lowercasing the whole string as a first step might affect
the tokenizer since tokenization benefits from information provided by
case-sensitive strings. We can therefore tokenize first using spaCy and
then obtain the lowercase strings of each token using the
<code>.lower_</code> property:</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a>lower_text <span class="op">=</span> [token.lower_ <span class="cf">for</span> token <span class="kw">in</span> doc]</span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a>lower_text[:<span class="dv">10</span>] <span class="co"># Beware that this is a list of strings now!</span></span></code></pre>
</div>
<p>In other cases, such as with Named Entity Recognition, lowercasing
can actually damage the performance of your model, since words that
start with an uppercase (not preceded by a period) tend to be proper
nouns that map into Entities, for example:</p>
<pre class="text"><code>My next laptop will be from Apple, Will said.

my next laptop will be from apple, will said.</code></pre>
</div>
<div class="section level3">
<h3 id="lemmatization">Lemmatization<a class="anchor" aria-label="anchor" href="#lemmatization"></a></h3>
<p>Although it has become less frequent, normalizing words into their
<em>dictionary form</em> can help to focus on relevant aspects of text.
Think how “eating”, “ate”, “eaten” are all a variation of the verb
“eat”.</p>
<p>Lemmatization is a NLP task on its own therefore we also tend to use
ready pre-trained models to obtain the lemmas. Using spaCy we can access
the lemmmatized version of each token with the <code>lemma_</code>
(notice the underscore!) property:</p>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a>lemmas <span class="op">=</span> [token.lemma_ <span class="cf">for</span> token <span class="kw">in</span> doc]</span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a><span class="bu">print</span>(lemmas)</span></code></pre>
</div>
<p>Note that the list of lemmas is now a list of strings.</p>
<p>Having a lemmatized text allows us to merge into a single token the
different surface occurrences of the same concept. This can be very
useful for count based methods, or for generating word representations.
For example, you can have a single vector for <code>eat</code> instead
of one vector per verb tense.</p>
<p>As with each preprocessing operation, this step is optional. Think
for example, the cases where the differences of verb usage according to
tense is informative, or the difference between singular and plural
usage of nouns, in those cases lemmatizing will get rid of important
information for your task.</p>
</div>
<div class="section level3">
<h3 id="stop-word-removal">Stop Word Removal<a class="anchor" aria-label="anchor" href="#stop-word-removal"></a></h3>
<p>The most frequent words in texts are those which contribute little
semantic value on their own: articles (‘the’, ‘a’, ‘an’), conjunctions
(‘and’, ‘or’, ‘but’), prepositions (‘on’, ‘by’), auxiliary verbs (‘is’,
‘am’), pronouns (‘he’, ‘which’), or any highly frequent word that might
not be of interest in several <em>content-only</em> related tasks. Let’s
define a small list of stop words for this specific case:</p>
<div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a>STOP_WORDS <span class="op">=</span> [<span class="st">"the"</span>, <span class="st">"you"</span>, <span class="st">"will"</span>] <span class="co"># This list can be customized to your needs...</span></span></code></pre>
</div>
<p>Using python directly, we need to manually define a list of what we
consider to be stop words and directly filter the tokens that match
this. Note that doing the lemmatization step was crucial to get more
coverage with the stop word filtering:</p>
<div class="codewrapper sourceCode" id="cb16">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a>lemmas <span class="op">=</span> [token.lemma_ <span class="cf">for</span> token <span class="kw">in</span> doc]</span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a>content_words <span class="op">=</span> []</span>
<span id="cb16-3"><a href="#cb16-3" tabindex="-1"></a><span class="cf">for</span> lemma <span class="kw">in</span> lemmas:</span>
<span id="cb16-4"><a href="#cb16-4" tabindex="-1"></a>    <span class="cf">if</span> lemma <span class="kw">not</span> <span class="kw">in</span> STOP_WORDS:</span>
<span id="cb16-5"><a href="#cb16-5" tabindex="-1"></a>        content_words.append(lemma)</span>
<span id="cb16-6"><a href="#cb16-6" tabindex="-1"></a><span class="bu">print</span>(content_words[:<span class="dv">20</span>])</span></code></pre>
</div>
<p>Using spaCy we can filter the stop words based on the token
properties:</p>
<div class="codewrapper sourceCode" id="cb17">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a>tokens_nostop <span class="op">=</span> [token <span class="cf">for</span> token <span class="kw">in</span> tokens <span class="cf">if</span> <span class="kw">not</span> token.is_stop]</span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a><span class="bu">print</span>(tokens[:<span class="dv">15</span>])</span></code></pre>
</div>
<p>There is no canonical definition of stop words because what you
consider to be a stop word is directly linked to the objective of your
task at hand. For example, pronouns are usually considered stopwords,
but if you want to do gender bias analysis then pronouns are actually a
key element of your text processing pipeline.</p>
<p>Another special case is the word ‘not’ which carries a highly
significant semantic value, the mere presence of this token changes the
meaning of the sentences.</p>
</div>
</section><section><h2 class="section-heading" id="nlp-pipeline">NLP Pipeline<a class="anchor" aria-label="anchor" href="#nlp-pipeline"></a></h2>
<hr class="half-width"><p>The concept of NLP pipeline refers to the sequence of operations that
we apply to our data in order to go from the original data
(e.g. original raw documents) to the expected outputs of our NLP Task at
hand. The components of the pipeline refer to any manipulation we apply
to the text, and do not necessarily need to be complex models, they
involve preprocessing operations, application of rules or machine
learning models, as well as formatting the outputs in a desired way.</p>
<div class="section level3">
<h3 id="a-simple-rule-based-classifier">A simple rule-based classifier<a class="anchor" aria-label="anchor" href="#a-simple-rule-based-classifier"></a></h3>
<p>Imagine we want to build a very lightweight sentiment classifier. A
basic approach is to design the following pipeline:</p>
<ol style="list-style-type: decimal"><li>Clean Original Text File</li>
<li>Apply a sentence segmentation model</li>
<li>Define a set of positive and negative words</li>
<li>For each sentence:
<ul><li>If it contains one or more of the positive words, classify as
<code>POSITIVE</code>
</li>
<li>If it contains one or more of the negative words, classify as
<code>NEGATIVE</code>
</li>
<li>Otherwise classify as <code>NEUTRAL</code>
</li>
</ul></li>
<li>Output a table with the original sentence and the assigned
label</li>
</ol><p>This is implemented with the following code:</p>
<ol style="list-style-type: decimal"><li>Read the text and normalize it into a single line</li>
</ol><div class="codewrapper sourceCode" id="cb18">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" tabindex="-1"></a><span class="im">import</span> spacy</span>
<span id="cb18-2"><a href="#cb18-2" tabindex="-1"></a>nlp <span class="op">=</span> spacy.load(<span class="st">"en_core_web_sm"</span>)</span>
<span id="cb18-3"><a href="#cb18-3" tabindex="-1"></a></span>
<span id="cb18-4"><a href="#cb18-4" tabindex="-1"></a>filename <span class="op">=</span> <span class="st">"data/84_frankenstein_or_the_modern_prometheus.txt"</span></span>
<span id="cb18-5"><a href="#cb18-5" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(filename, <span class="st">'r'</span>, encoding<span class="op">=</span><span class="st">'utf-8'</span>) <span class="im">as</span> <span class="bu">file</span>:</span>
<span id="cb18-6"><a href="#cb18-6" tabindex="-1"></a>    text <span class="op">=</span> <span class="bu">file</span>.read()</span>
<span id="cb18-7"><a href="#cb18-7" tabindex="-1"></a></span>
<span id="cb18-8"><a href="#cb18-8" tabindex="-1"></a>text <span class="op">=</span> text.replace(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span>, <span class="st">" "</span>)</span></code></pre>
</div>
<ol start="2" style="list-style-type: decimal"><li>Apply Sentence segmentation</li>
</ol><div class="codewrapper sourceCode" id="cb19">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" tabindex="-1"></a>doc <span class="op">=</span> nlp(text)</span>
<span id="cb19-2"><a href="#cb19-2" tabindex="-1"></a>sentences <span class="op">=</span> [sent.text <span class="cf">for</span> sent <span class="kw">in</span> doc.sents]</span></code></pre>
</div>
<ol start="3" style="list-style-type: decimal"><li>Define the positive and negative words you care about:</li>
</ol><div class="codewrapper sourceCode" id="cb20">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" tabindex="-1"></a>positive_words <span class="op">=</span> [<span class="st">"happy"</span>, <span class="st">"excited"</span>, <span class="st">"delighted"</span>, <span class="st">"content"</span>, <span class="st">"love"</span>, <span class="st">"enjoyment"</span>]</span>
<span id="cb20-2"><a href="#cb20-2" tabindex="-1"></a>negative_words <span class="op">=</span> [<span class="st">"unhappy"</span>, <span class="st">"sad"</span>, <span class="st">"anxious"</span>, <span class="st">"miserable"</span>, <span class="st">"fear"</span>, <span class="st">"horror"</span>]</span></code></pre>
</div>
<ol start="4" style="list-style-type: decimal"><li>Apply the rules to each sentence and collect the labels</li>
</ol><div class="codewrapper sourceCode" id="cb21">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" tabindex="-1"></a>classified_sentences <span class="op">=</span> []</span>
<span id="cb21-2"><a href="#cb21-2" tabindex="-1"></a></span>
<span id="cb21-3"><a href="#cb21-3" tabindex="-1"></a><span class="cf">for</span> sent <span class="kw">in</span> sentences:</span>
<span id="cb21-4"><a href="#cb21-4" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">any</span>(word <span class="kw">in</span> sent.lower() <span class="cf">for</span> word <span class="kw">in</span> positive_words):</span>
<span id="cb21-5"><a href="#cb21-5" tabindex="-1"></a>        classified_sentences.append((sent, <span class="st">'POSITIVE'</span>))</span>
<span id="cb21-6"><a href="#cb21-6" tabindex="-1"></a>    <span class="cf">elif</span> <span class="bu">any</span>(word <span class="kw">in</span> sent.lower() <span class="cf">for</span> word <span class="kw">in</span> negative_words):</span>
<span id="cb21-7"><a href="#cb21-7" tabindex="-1"></a>        classified_sentences.append((sent, <span class="st">'NEGATIVE'</span>))</span>
<span id="cb21-8"><a href="#cb21-8" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb21-9"><a href="#cb21-9" tabindex="-1"></a>        classified_sentences.append((sent, <span class="st">'NEUTRAL'</span>))</span></code></pre>
</div>
<ol start="5" style="list-style-type: decimal"><li>Save the classified data</li>
</ol><div class="codewrapper sourceCode" id="cb22">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb22-2"><a href="#cb22-2" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(classified_sentences, columns<span class="op">=</span>[<span class="st">'sentence'</span>, <span class="st">'label'</span>])</span>
<span id="cb22-3"><a href="#cb22-3" tabindex="-1"></a>df.to_csv(<span class="st">'results_naive_rule_classifier.csv'</span>, sep<span class="op">=</span><span class="st">'</span><span class="ch">\t</span><span class="st">'</span>)</span></code></pre>
</div>
<div id="challenge1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div class="callout-inner">
<h3 class="callout-title">Challenge</h3>
<div class="callout-content">
<p>Discuss the pros and cons of the proposed NLP pipeline: 1. Do you
think it will give accurate results? 2. What do you think about the
coverage of this approach? What cases will it miss? 3. Think of possible
drawbacks of chaining components in a pipeline.</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<ol style="list-style-type: decimal"><li>This classifier only considers the presence of one word to apply a
label. It does not analyze sentence semantics or even syntax.</li>
<li>Given how the rules are defined, if both positive and negative words
are present in the same sentence it will assign the
<code>POSITIVE</code> label. It will generate a lot of false positives
because of the simplistic rules</li>
<li>The errors from previous steps get carried over to the next steps
increasing the likelihood of noisy outputs.</li>
</ol></div>
</div>
</div>
</div>
<p>So far we’ve seen how to format and segment the text to have atomic
data at the word level or sentence level. We then apply operations to
the word and sentence strings. This approach still depends on counting
and exact keyword matching. And as we have already seen it has several
limitations.</p>
<p>One way to combat this is by transforming each word into numeric
representations and then perform math operations on them. We then can
define similarity measures for the word representations which will allow
more advance models, such as machine learning models, exploit patterns
beyond the sting matching level. This is where the concept of word
embeddings comes in handy.</p>
</div>
</section><section><h2 class="section-heading" id="word-embeddings">Word Embeddings<a class="anchor" aria-label="anchor" href="#word-embeddings"></a></h2>
<hr class="half-width"><p>As we have seen, count-based methods can be directly applied to learn
statistical co-occurrences in raw texts. Count-based methods are limited
because they rapidly become unmanageable. The idea behind word
embeddings is built on top of exploiting statistical co-occurrences in
context but <strong>instead of holding explicit counts for every single
word and document, a neural network is trained to predict missing words
in context</strong>.</p>
<p>A shallow neural network is optimized with the task of language
modeling and the final trained network holds vectors of a fixed size
whose values can be mapped into linguistic properties (since the
training objective was language modeling). Since similar words occur in
similar contexts, or have same characteristics, a properly trained model
will learn to assign similar vectors to similar words.</p>
<p>By representing words with vectors, we can mathematically manipulate
them through vector arithmetic and express semantic similarity in terms
of vector distance. Because the size of the learned vectors is not
proportional to the amount of documents we can learn the representations
from larger collections of texts, obtaining more robust representations,
that are less corpus-dependent.</p>
</section><section><h2 class="section-heading" id="the-word2vec-vector-space">The Word2Vec Vector Space<a class="anchor" aria-label="anchor" href="#the-word2vec-vector-space"></a></h2>
<hr class="half-width"><p>There are two main architectures for training Word2Vec:</p>
<ul><li>Continuous Bag-of-Words (CBOW): Predicts a target word based on its
surrounding context words.</li>
<li>Continuous Skip-Gram: Predicts surrounding context words given a
target word.</li>
</ul><figure><img src="fig/emb13.png" class="figure mx-auto d-block"></figure><div id="callout3" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div class="callout-inner">
<div class="callout-content">
<p>CBOW is faster to train, while Skip-Gram is more effective for
infrequent words. Increasing context size improves embeddings but
increases training time.</p>
</div>
</div>
</div>
<p>The python module <code>gensim</code> offers a very useful interface
to interact with pre-trained word2vec models and also to train our own.
First we will explore the model from the original Word2Vec paper, which
was trained on a big corpus from Google News. We will see what
functionalities are available to explore a vector space. Then we will
step by step prepare our own text to train our own word2vec models and
save them.</p>
<div class="section level3">
<h3 id="load-the-embeddings-and-inspect-them">Load the embeddings and inspect them<a class="anchor" aria-label="anchor" href="#load-the-embeddings-and-inspect-them"></a></h3>
<p>The library <code>gensim</code> has a repository with English
pre-trained models. We can take a look at the models:</p>
<div class="codewrapper sourceCode" id="cb23">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" tabindex="-1"></a><span class="im">import</span> gensim.downloader</span>
<span id="cb23-2"><a href="#cb23-2" tabindex="-1"></a>available_models <span class="op">=</span> gensim.downloader.info()[<span class="st">'models'</span>].keys()</span>
<span id="cb23-3"><a href="#cb23-3" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">list</span>(available_models))</span></code></pre>
</div>
<p>We will download the google News model with:</p>
<div class="codewrapper sourceCode" id="cb24">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" tabindex="-1"></a>w2v_model <span class="op">=</span> gensim.downloader.load(<span class="st">'word2vec-google-news-300'</span>)</span></code></pre>
</div>
<p>We can do some basic checkups such as showing how many words are in
the vocabulary (i.e. for how many words do we have an available vector),
what is the dimension of each vector and pick at how the vectors look
inside:</p>
<div class="codewrapper sourceCode" id="cb25">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(w2v_model.key_to_index.keys())) <span class="co"># 3 million words</span></span>
<span id="cb25-2"><a href="#cb25-2" tabindex="-1"></a><span class="bu">print</span>(w2v_model.vector_size) <span class="co"># 300 dimensions. This can be chosen when training your own model</span></span>
<span id="cb25-3"><a href="#cb25-3" tabindex="-1"></a><span class="bu">print</span>(w2v_model[<span class="st">'car'</span>][:<span class="dv">10</span>]) <span class="co"># The first 10 dimensions of the vector representing 'car'.</span></span>
<span id="cb25-4"><a href="#cb25-4" tabindex="-1"></a><span class="bu">print</span>(w2v_model[<span class="st">'cat'</span>][:<span class="dv">10</span>]) <span class="co"># The first 10 dimensions of the vector representing 'cat'.</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>3000000
300
[ 0.13085938  0.00842285  0.03344727 -0.05883789  0.04003906 -0.14257812
  0.04931641 -0.16894531  0.20898438  0.11962891]
[ 0.0123291   0.20410156 -0.28515625  0.21679688  0.11816406  0.08300781
  0.04980469 -0.00952148  0.22070312 -0.12597656]</code></pre>
</div>
<p>This is a very big model with 3 million words and the dimensionality
chosen at training time was 300, thus each word will have a
300-dimension vector associated to it.</p>
<p>Even with such a big vocabulary we can always find a word that won’t
be in there:</p>
<div class="codewrapper sourceCode" id="cb27">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" tabindex="-1"></a><span class="bu">print</span>(w2v_model[<span class="st">'bazzinga'</span>][:<span class="dv">10</span>])</span></code></pre>
</div>
<p>This will throw a <code>KeyError</code> as the model does not know
that word. Unfortunately this is a limitation that word2vec cannot
solve.</p>
<p>Now let’s talk about the vectors themselves. They are not easy to
interpret as they are a bunch of floating point numbers. These are the
weights that the network learned when optimizing for language modelling.
As the vectors are hard to interpret, we rely on a mathematical method
to compute how similar two vectors are. The best metric for measuring
similarity between two high-dimensional vectors is cosine
similarity.</p>
<div id="callout4" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div class="callout-inner">
<div class="callout-content">
<p><a href="https://en.wikipedia.org/wiki/Cosine_similarity" class="external-link">cosine
similarity</a> ranges between [<code>-1</code> and <code>1</code>]. It
is the cosine of the angle between two vectors, divided by the product
of their length. Mathematically speaking, when two vectors point in
exactly the same direction their cosine will be 1, and when they point
in the opposite direction their cosine will be -1. In python we can use
Numpy to compute the cosine similarity of vectors.</p>
<figure><img src="fig/emb12.png" alt="" class="figure mx-auto d-block"></figure></div>
</div>
</div>
<p>We can use <code>sklearn</code> learn to measure any pair of
high-dimensional vectors:</p>
<div class="codewrapper sourceCode" id="cb28">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" tabindex="-1"></a><span class="im">from</span> sklearn.metrics.pairwise <span class="im">import</span> cosine_similarity</span>
<span id="cb28-2"><a href="#cb28-2" tabindex="-1"></a></span>
<span id="cb28-3"><a href="#cb28-3" tabindex="-1"></a>car_vector <span class="op">=</span> w2v_model[<span class="st">'car'</span>]</span>
<span id="cb28-4"><a href="#cb28-4" tabindex="-1"></a>cat_vector <span class="op">=</span> w2v_model[<span class="st">'cat'</span>]</span>
<span id="cb28-5"><a href="#cb28-5" tabindex="-1"></a></span>
<span id="cb28-6"><a href="#cb28-6" tabindex="-1"></a>similarity <span class="op">=</span> cosine_similarity([car_vector], [cat_vector])</span>
<span id="cb28-7"><a href="#cb28-7" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Cosine similarity between 'car' and 'cat': </span><span class="sc">{</span>similarity[<span class="dv">0</span>][<span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb28-8"><a href="#cb28-8" tabindex="-1"></a></span>
<span id="cb28-9"><a href="#cb28-9" tabindex="-1"></a>similarity <span class="op">=</span> cosine_similarity([w2v_model[<span class="st">'hamburger'</span>]], [w2v_model[<span class="st">'pizza'</span>]])</span>
<span id="cb28-10"><a href="#cb28-10" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Cosine similarity between 'hamburger' and 'pizza': </span><span class="sc">{</span>similarity[<span class="dv">0</span>][<span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb29">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" tabindex="-1"></a></span>
<span id="cb29-2"><a href="#cb29-2" tabindex="-1"></a>Cosine similarity between <span class="st">'car'</span> <span class="kw">and</span> <span class="st">'cat'</span>: <span class="fl">0.21528185904026031</span></span>
<span id="cb29-3"><a href="#cb29-3" tabindex="-1"></a>Cosine similarity between <span class="st">'hamburger'</span> <span class="kw">and</span> <span class="st">'pizza'</span>: <span class="fl">0.6153676509857178</span></span></code></pre>
</div>
<p>Or you can use directly the
<code>w2v_model.similarity('car', 'cat')</code> function which gives the
same result.</p>
<p>The higher similarity score between the hamburger and pizza indicates
they are more similar based on the contexts where they appear in the
training data. Even though is hard to read all the floating numbers in
the vectors, we can trust this metric to always give us a hint of which
words are semantically closer than others</p>
<div id="challenge2" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div class="callout-inner">
<h3 class="callout-title">Challenge</h3>
<div class="callout-content">
<p>Think of different word pairs and try to guess how close or distant
they will be from each other. Use the similarity measure from the
word2vec module to compute the metric and discuss if this fits your
expectations. If not, can you come up with a reason why this was not the
case?</p>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2"> Show me the solution </h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" aria-labelledby="headingSolution2" data-bs-parent="#accordionSolution2">
<div class="accordion-body">
<p>Interesting cases are synonyms, antonyms and morphologically related
words:</p>
<div class="codewrapper sourceCode" id="cb30">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" tabindex="-1"></a><span class="bu">print</span>(w2v_model.similarity(<span class="st">'democracy'</span>, <span class="st">'democratic'</span>))</span>
<span id="cb30-2"><a href="#cb30-2" tabindex="-1"></a><span class="bu">print</span>(w2v_model.similarity(<span class="st">'queen'</span>, <span class="st">'princess'</span>))</span>
<span id="cb30-3"><a href="#cb30-3" tabindex="-1"></a><span class="bu">print</span>(w2v_model.similarity(<span class="st">'love'</span>, <span class="st">'hate'</span>)) <span class="co">#!! (think of "I love X" and "I hate X")</span></span>
<span id="cb30-4"><a href="#cb30-4" tabindex="-1"></a><span class="bu">print</span>(w2v_model.similarity(<span class="st">'love'</span>, <span class="st">'lover'</span>))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code><span><span class="fl">0.86444813</span></span>
<span><span class="fl">0.7070532</span></span>
<span><span class="fl">0.6003957</span></span>
<span><span class="fl">0.48608577</span></span></code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="vector-neighborhoods">Vector Neighborhoods<a class="anchor" aria-label="anchor" href="#vector-neighborhoods"></a></h3>
<p>Now that we have a metric we can trust, we can retrieve neighborhoods
of vectors that are close to a given word. This is analogous to
retrieving semantically related terms to a target term. Let’s explore
the neighborhood around `pizza` using the <code>most_similar()</code>
method:</p>
<div class="codewrapper sourceCode" id="cb32">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" tabindex="-1"></a><span class="bu">print</span>(w2v_model.most_similar(<span class="st">'pizza'</span>, topn<span class="op">=</span><span class="dv">10</span>))</span></code></pre>
</div>
<p>This returns a list of ranked tuples with the form (word,
similarity_score). The list is already ordered in descent, so the first
element is the closest vector in the vector space, the second element is
the second closest word and so on…</p>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[('pizzas', 0.7863470911979675),
('Domino_pizza', 0.7342829704284668),
('Pizza', 0.6988078355789185),
('pepperoni_pizza', 0.6902607083320618),
('sandwich', 0.6840401887893677),
('burger', 0.6569692492485046),
('sandwiches', 0.6495091319084167),
('takeout_pizza', 0.6491535902023315),
('gourmet_pizza', 0.6400628089904785),
('meatball_sandwich', 0.6377009749412537)]</code></pre>
</div>
<p>Exploring neighborhoods can help us understand why some vectors are
closer (or not so much). Take the case of <em>love</em> and
<em>lover</em>, originally we might think these should be very close to
each other but by looking at their neighborhoods we understand why this
is not the case:</p>
<div class="codewrapper sourceCode" id="cb34">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" tabindex="-1"></a><span class="bu">print</span>(w2v_model.most_similar(<span class="st">'love'</span>, topn<span class="op">=</span><span class="dv">10</span>))</span>
<span id="cb34-2"><a href="#cb34-2" tabindex="-1"></a><span class="bu">print</span>(w2v_model.most_similar(<span class="st">'lover'</span>, topn<span class="op">=</span><span class="dv">10</span>))</span></code></pre>
</div>
<p>This returns a list of ranked tuples with the form (word,
similarity_score). The list is already ordered in descent, so the first
element is the closest vector in the vector space, the second element is
the second closest word and so on…</p>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[('loved', 0.6907791495323181), ('adore', 0.6816874146461487), ('loves', 0.6618633270263672), ('passion', 0.6100709438323975), ('hate', 0.6003956198692322), ('loving', 0.5886634588241577), ('Ilove', 0.5702950954437256), ('affection', 0.5664337873458862), ('undying_love', 0.5547305345535278), ('absolutely_adore', 0.5536840558052063)]

[('paramour', 0.6798686385154724), ('mistress', 0.6387110352516174), ('boyfriend', 0.6375402212142944), ('lovers', 0.6339589953422546), ('girlfriend', 0.6140860915184021), ('beau', 0.609399676322937), ('fiancé', 0.5994566679000854), ('soulmate', 0.5993717312812805), ('hubby', 0.5904166102409363), ('fiancée', 0.5888950228691101)]</code></pre>
</div>
<p>The first word is a noun or a verb (depending on the context) that
denotes affection to someone/something , so it is associated with other
concepts of affection (positive or negative). The case of <em>lover</em>
is used to describe a person, hence the associated concepts are
descriptors of people with whom the lover can be associated.</p>
</div>
<div class="section level3">
<h3 id="word-analogies-with-vectors">Word Analogies with Vectors<a class="anchor" aria-label="anchor" href="#word-analogies-with-vectors"></a></h3>
<p>Another powerful property that word embeddings show is that vector
algebra can preserve semantic analogy. An analogy is a comparison
between two different things based on their similar features or
relationships, for example king is to queen as man is to woman. We can
mimic this operations directly on the vectors using the
<code>most_similar()</code> method with the <code>positive</code> and
<code>negative</code> parameters:</p>
<div class="codewrapper sourceCode" id="cb36">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" tabindex="-1"></a><span class="co"># king is to man as what is to woman?</span></span>
<span id="cb36-2"><a href="#cb36-2" tabindex="-1"></a><span class="co"># king + woman - man = queen</span></span>
<span id="cb36-3"><a href="#cb36-3" tabindex="-1"></a>w2v_model.most_similar(positive<span class="op">=</span>[<span class="st">'king'</span>, <span class="st">'woman'</span>], negative<span class="op">=</span>[<span class="st">'man'</span>])</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[('queen', 0.7118192911148071),
 ('monarch', 0.6189674735069275),
 ('princess', 0.5902431011199951),
 ('crown_prince', 0.5499460697174072),
 ('prince', 0.5377321243286133),
 ('kings', 0.5236844420433044),
 ('Queen_Consort', 0.5235945582389832),
 ('queens', 0.5181134343147278),
 ('sultan', 0.5098593235015869),
 ('monarchy', 0.5087411403656006)]</code></pre>
</div>
</div>
</section><section><h2 class="section-heading" id="train-your-own-word2vec">Train your own Word2Vec<a class="anchor" aria-label="anchor" href="#train-your-own-word2vec"></a></h2>
<hr class="half-width"><p>The <code>gensim</code> package has implemented everything for us,
this means we have to focus mostly on obtaining clean data and then
calling the <code>Word2Vec</code> object to train our own model with our
own data. This can be done like follows:</p>
<div class="codewrapper sourceCode" id="cb38">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" tabindex="-1"></a><span class="im">import</span> gensim </span>
<span id="cb38-2"><a href="#cb38-2" tabindex="-1"></a><span class="im">from</span> gensim.models <span class="im">import</span> Word2Vec </span>
<span id="cb38-3"><a href="#cb38-3" tabindex="-1"></a></span>
<span id="cb38-4"><a href="#cb38-4" tabindex="-1"></a>model <span class="op">=</span> Word2Vec([clean_tokens], sg<span class="op">=</span><span class="dv">0</span> , vector_size<span class="op">=</span><span class="dv">300</span>, window<span class="op">=</span><span class="dv">5</span>, min_count<span class="op">=</span><span class="dv">1</span>, workers<span class="op">=</span><span class="dv">4</span>)</span></code></pre>
</div>
<p>With this line code we are configuring our whole Word2Vec training
schema. We will be using CBOW (<code>sg=0</code> means CBOW,
<code>sg=1</code> means Skip-gram). We are interested in having vectors
with 300 dimensions <code>vector_size=300</code> and a context size of 5
surrounding words <code>window=5</code>. Because we already filtered our
tokens, we include all words present in the filtered corpora, regardless
of their frequency of occurrence <code>min_count=1</code>. The last
parameters tells python to use 4 CPU cores for training.</p>
<p>See the Gensim <a href="https://radimrehurek.com/gensim/models/word2vec.html" class="external-link">documentation</a>
for more training options.</p>
<div class="section level3">
<h3 id="save-and-retrieve-your-model">Save and Retrieve your model<a class="anchor" aria-label="anchor" href="#save-and-retrieve-your-model"></a></h3>
<p>Once your model is trained it is useful to save the checkpoint in
order to retrieve it next time instead of having to train it every time.
You can save it with:</p>
<div class="codewrapper sourceCode" id="cb39">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" tabindex="-1"></a>model.save(<span class="st">"word2vec_mini_books.model"</span>)</span></code></pre>
</div>
<p>Let’s put everything together. We have now the following NLP task:
train our own Word2Vec model. We are interested on having vectors for
content words only, so even though our preprocessing will unfortunately
loose a lot of the original information, in exchange we will be able
manipulate the most relevant conceptual words as individual numeric
representations.</p>
<div id="challenge3" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div class="callout-inner">
<h3 class="callout-title">Challenge</h3>
<div class="callout-content">
<p>Let’s apply this step by step on a longer text. In this case, because
we are learning the process, our corpus will be only one book but in
reality we would like to train a network with thousands of them. We will
use two books: Frankenstein and Dracula to train a model of word
vectors.</p>
<p>Write the code to follow the proposed pipeline and train the word2vec
model. The proposed pipeline for this task is:</p>
<ul><li>load the text files</li>
<li>tokenize files</li>
<li>keep only alphanumerical tokens</li>
<li>lowercase words</li>
<li>lemmatize words</li>
<li>Remove stop words</li>
<li>Train a Word2Vec model (feed the clean tokens to the
<code>Word2Vec</code> object)</li>
<li>Save the trained model</li>
</ul></div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3"> Show me the solution </h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" aria-labelledby="headingSolution3" data-bs-parent="#accordionSolution3">
<div class="accordion-body">
<div class="codewrapper sourceCode" id="cb40">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" tabindex="-1"></a><span class="cf">pass</span></span></code></pre>
</div>
</div>
</div>
</div>
</div>
<p>To load back the pre-trained vectors you just created you can use the
following code:</p>
<div class="codewrapper sourceCode" id="cb41">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" tabindex="-1"></a>model <span class="op">=</span> Word2Vec.load(<span class="st">"word2vec_mini_books.model"</span>)</span>
<span id="cb41-2"><a href="#cb41-2" tabindex="-1"></a>w2v <span class="op">=</span> model.wv</span>
<span id="cb41-3"><a href="#cb41-3" tabindex="-1"></a><span class="co"># Test:</span></span>
<span id="cb41-4"><a href="#cb41-4" tabindex="-1"></a>w2v.most_similar(<span class="st">'monster'</span>)</span></code></pre>
</div>
<div id="dataset-size-in-training" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="dataset-size-in-training" class="callout-inner">
<h3 class="callout-title">Dataset size in training</h3>
<div class="callout-content">
<p>To obtain your own high-quality embeddings, the size/length of the
training dataset plays a crucial role. Generally <a href="https://cs.stanford.edu/~quocle/paragraph_vector.pdf" class="external-link">tens of
thousands of documents</a> are considered a reasonable amount of data
for decent results.</p>
<p>Is there however a strict minimum? Not really. Things to keep in mind
is that <code>vocabulary size</code>, <code>document length</code> and
<code>desired vector size</code> interacts with each other. The higher
the dimensional vectors (e.g. 200-300 dimensions) the more data is
required, and of high quality, i.e. that allows the learning of words in
a variety of contexts.</p>
<p>While word2vec models typically perform better with large datasets
containing millions of words, using a single page is sufficient for
demonstration and learning purposes. This smaller dataset allows us to
train the model quickly and understand how word2vec works without the
need for extensive computational resources.</p>
</div>
</div>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<span class="callout-header">Key Points</span>
<div class="callout-inner">
<div class="callout-content">
<ul><li>We can represent text as vectors of numbers (which makes it
interpretable for machines)</li>
<li>We can run a preprocessing pipeline to obtain clear words that can
be used as features</li>
<li>We can easily compute how words are similar to each other with the
cosine similarity</li>
<li>Using gensim we can train our own word2vec models</li>
</ul></div>
</div>
</div>
</div>
</section></div> <!-- / div.lesson-content -->
    </main><!-- / main#main-content.main-content --><nav class="bottom-pagination mx-md-4" aria-label="Previous and Next Chapter"><div class="d-block d-sm-block d-md-none">
        <a class="chapter-link" href="01-introduction.html"><i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>Previous</a>
        <a class="chapter-link float-end" href="03-transformers.html">Next<i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i></a>
      </div>
      <!-- content for large screens -->
      <div class="d-none d-sm-none d-md-block">
        <a class="chapter-link" href="01-introduction.html" rel="prev">
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>
          Previous: Introduction
        </a>
        <a class="chapter-link float-end" href="03-transformers.html" rel="next">
          Next: Transformers: BERT...
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i>
        </a>
      </div>
    </nav></div> <!-- / div.primary-content.col-xs-12 -->
<!-- END:   inst/pkgdown/templates/content-instructor.html-->

      </div><!--/div.row-->
      		<footer class="row footer mx-md-3"><hr><div class="col-md-6">
        <p>This lesson is subject to the <a href="CODE_OF_CONDUCT.html">Code of Conduct</a></p>
        <p>

        <a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/edit/main/episodes/02-word_representations.md" class="external-link">Edit on GitHub</a>

	
        | <a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/blob/main/CONTRIBUTING.md" class="external-link">Contributing</a>
        | <a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/" class="external-link">Source</a></p>
				<p><a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/blob/main/CITATION.cff" class="external-link">Cite</a> | <a href="mailto:team@carpentries.org">Contact</a> | <a href="https://carpentries.org/about/" class="external-link">About</a></p>
			</div>
			<div class="col-md-6">

        <p>Materials licensed under <a href="LICENSE.html">CC-BY 4.0</a> by the authors</p>

        <p>Template licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/" class="external-link">CC-BY 4.0</a> by <a href="https://carpentries.org/" class="external-link">The Carpentries</a></p>
        <p>Built with <a href="https://github.com/carpentries/sandpaper/tree/0.17.1" class="external-link">sandpaper (0.17.1)</a>, <a href="https://github.com/carpentries/pegboard/tree/0.7.9" class="external-link">pegboard (0.7.9)</a>, and <a href="https://github.com/carpentries/varnish/tree/1.0.7" class="external-link">varnish (1.0.7)</a></p>
			</div>
		</footer></div> <!-- / div.container -->
	<div id="to-top">
		<a href="#top">
      <i class="search-icon" data-feather="arrow-up" role="img" aria-label="Back To Top"></i><br><!-- <span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top --><span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top
		</a>
	</div>
  <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "LearningResource",
  "@id": "https://esciencecenter-digital-skills.github.io/Natural-language-processing/02-word_representations.html",
  "inLanguage": "en",
  "dct:conformsTo": "https://bioschemas.org/profiles/LearningResource/1.0-RELEASE",
  "description": "A Carpentries Lesson teaching foundational data and coding skills to researchers worldwide",
  "keywords": "software, data, lesson, The Carpentries, NLP, English, social sciences, pre-alpha",
  "name": "From words to vectors",
  "creativeWorkStatus": "active",
  "url": "https://esciencecenter-digital-skills.github.io/Natural-language-processing/02-word_representations.html",
  "identifier": "https://esciencecenter-digital-skills.github.io/Natural-language-processing/02-word_representations.html",
  "dateCreated": "2024-04-24",
  "dateModified": "2025-10-24",
  "datePublished": "2025-10-28"
}

  </script><script>
		feather.replace();
	</script></body></html><!-- END:   inst/pkgdown/templates/layout.html-->

