<!DOCTYPE html>
<!-- START: inst/pkgdown/templates/layout.html --><!-- Generated by pkgdown: do not edit by hand --><html lang="en" data-bs-theme="auto"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><title>Fundamentals of Natural Language Processing (NLP) in Python: From words to vectors</title><meta name="viewport" content="width=device-width, initial-scale=1"><script src="assets/themetoggle.js"></script><link rel="stylesheet" type="text/css" href="assets/styles.css"><script src="assets/scripts.js" type="text/javascript"></script><!-- mathjax --><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      config: ["MMLorHTML.js"],
      jax: ["input/TeX","input/MathML","output/HTML-CSS","output/NativeMML", "output/PreviewHTML"],
      extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
      TeX: {
        extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
      },
      tex2jax: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [ ['$$','$$'], ['\\[', '\\]'] ],
        processEscapes: true
      }
    });
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><!-- Responsive Favicon for The Carpentries --><link rel="apple-touch-icon" sizes="180x180" href="favicons/incubator/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="favicons/incubator/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="favicons/incubator/favicon-16x16.png"><link rel="manifest" href="favicons/incubator/site.webmanifest"><link rel="mask-icon" href="favicons/incubator/safari-pinned-tab.svg" color="#5bbad5"><meta name="msapplication-TileColor" content="#da532c"><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"><meta name="theme-color" media="(prefers-color-scheme: dark)" content="black"></head><body>
    <header id="top" class="navbar navbar-expand-md top-nav incubator"><svg xmlns="http://www.w3.org/2000/svg" class="d-none"><symbol id="check2" viewbox="0 0 16 16"><path d="M13.854 3.646a.5.5 0 0 1 0 .708l-7 7a.5.5 0 0 1-.708 0l-3.5-3.5a.5.5 0 1 1 .708-.708L6.5 10.293l6.646-6.647a.5.5 0 0 1 .708 0z"></path></symbol><symbol id="circle-half" viewbox="0 0 16 16"><path d="M8 15A7 7 0 1 0 8 1v14zm0 1A8 8 0 1 1 8 0a8 8 0 0 1 0 16z"></path></symbol><symbol id="moon-stars-fill" viewbox="0 0 16 16"><path d="M6 .278a.768.768 0 0 1 .08.858 7.208 7.208 0 0 0-.878 3.46c0 4.021 3.278 7.277 7.318 7.277.527 0 1.04-.055 1.533-.16a.787.787 0 0 1 .81.316.733.733 0 0 1-.031.893A8.349 8.349 0 0 1 8.344 16C3.734 16 0 12.286 0 7.71 0 4.266 2.114 1.312 5.124.06A.752.752 0 0 1 6 .278z"></path><path d="M10.794 3.148a.217.217 0 0 1 .412 0l.387 1.162c.173.518.579.924 1.097 1.097l1.162.387a.217.217 0 0 1 0 .412l-1.162.387a1.734 1.734 0 0 0-1.097 1.097l-.387 1.162a.217.217 0 0 1-.412 0l-.387-1.162A1.734 1.734 0 0 0 9.31 6.593l-1.162-.387a.217.217 0 0 1 0-.412l1.162-.387a1.734 1.734 0 0 0 1.097-1.097l.387-1.162zM13.863.099a.145.145 0 0 1 .274 0l.258.774c.115.346.386.617.732.732l.774.258a.145.145 0 0 1 0 .274l-.774.258a1.156 1.156 0 0 0-.732.732l-.258.774a.145.145 0 0 1-.274 0l-.258-.774a1.156 1.156 0 0 0-.732-.732l-.774-.258a.145.145 0 0 1 0-.274l.774-.258c.346-.115.617-.386.732-.732L13.863.1z"></path></symbol><symbol id="sun-fill" viewbox="0 0 16 16"><path d="M8 12a4 4 0 1 0 0-8 4 4 0 0 0 0 8zM8 0a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 0zm0 13a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 13zm8-5a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2a.5.5 0 0 1 .5.5zM3 8a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2A.5.5 0 0 1 3 8zm10.657-5.657a.5.5 0 0 1 0 .707l-1.414 1.415a.5.5 0 1 1-.707-.708l1.414-1.414a.5.5 0 0 1 .707 0zm-9.193 9.193a.5.5 0 0 1 0 .707L3.05 13.657a.5.5 0 0 1-.707-.707l1.414-1.414a.5.5 0 0 1 .707 0zm9.193 2.121a.5.5 0 0 1-.707 0l-1.414-1.414a.5.5 0 0 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .707zM4.464 4.465a.5.5 0 0 1-.707 0L2.343 3.05a.5.5 0 1 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .708z"></path></symbol></svg><a class="visually-hidden-focusable skip-link" href="#main-content">Skip to main content</a>
  <div class="container-fluid top-nav-container">
    <div class="col-md-8">
      <div class="large-logo">
        <img id="incubator-logo" alt="Carpentries Incubator" src="assets/images/incubator-logo.svg"><span class="badge text-bg-danger">
          <abbr title="This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught.">
            <a href="https://docs.carpentries.org/resources/curriculum/lesson-life-cycle.html" class="external-link alert-link">
              <i aria-hidden="true" class="icon" data-feather="alert-octagon" style="border-radius: 5px"></i>
              Pre-Alpha
            </a>
            <span class="visually-hidden">This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught.</span>
          </abbr>
        </span>

      </div>
    </div>
    <div class="selector-container">
      <div id="theme-selector">
        <li class="nav-item dropdown" id="theme-button-list">
          <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
            <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="bd-theme-text"><li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                Light
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                Dark
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                Auto
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
          </ul></li>
      </div>


      <div class="dropdown" id="instructor-dropdown">
        <button class="btn btn-secondary dropdown-toggle bordered-button" type="button" id="dropdownMenu1" data-bs-toggle="dropdown" aria-expanded="false">
          <i aria-hidden="true" class="icon" data-feather="eye"></i> Learner View <i data-feather="chevron-down"></i>
        </button>
        <ul class="dropdown-menu" aria-labelledby="dropdownMenu1"><li><button class="dropdown-item" type="button" onclick="window.location.href='instructor/02-word_representations.html';">Instructor View</button></li>
        </ul></div>
    </div>
  </div>
  <hr></header><nav class="navbar navbar-expand-xl bottom-nav incubator" aria-label="Main Navigation"><div class="container-fluid nav-container">
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle Navigation">
      <span class="navbar-toggler-icon"></span>
      <span class="menu-title">Menu</span>
    </button>
    <div class="nav-logo">
      <img class="small-logo" alt="Carpentries Incubator" src="assets/images/incubator-logo-sm.svg"></div>
    <div class="lesson-title-md">
      Fundamentals of Natural Language Processing (NLP) in Python
    </div>
    <div class="search-icon-sm">
      <!-- TODO: do not show until we have search
        <i role="img" aria-label="Search the All In One page" data-feather="search"></i>
      -->
    </div>
    <div class="desktop-nav">
      <ul class="navbar-nav me-auto mb-2 mb-lg-0"><li class="nav-item">
          <span class="lesson-title">
            Fundamentals of Natural Language Processing (NLP) in Python
          </span>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="key-points.html">Key Points</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="reference.html#glossary">Glossary</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="profiles.html">Learner Profiles</a>
        </li>
        <li class="nav-item dropdown">
          <button class="nav-link dropdown-toggle" id="navbarDropdown" data-bs-toggle="dropdown" aria-expanded="false">
            More <i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu" aria-labelledby="navbarDropdown"></ul></li>
      </ul></div>
    <!--
    <form class="d-flex col-md-2 search-form">
      <fieldset disabled>
      <input class="form-control me-2 searchbox" type="search" placeholder="" aria-label="">
        <button class="btn btn-outline-success tablet-search-button"  type="submit">
          <i class="search-icon" data-feather="search" role="img" aria-label="Search the All In One page"></i>
        </button>
      </fieldset>
    </form>
    -->
    <a id="search-button" class="btn btn-primary" href="aio.html" role="button" aria-label="Search the All In One page">Search the All In One page</a>
  </div><!--/div.container-fluid -->
</nav><div class="col-md-12 mobile-title">
  Fundamentals of Natural Language Processing (NLP) in Python
</div>

<aside class="col-md-12 lesson-progress"><div style="width: 25%" class="percentage">
    25%
  </div>
  <div class="progress incubator">
    <div class="progress-bar incubator" role="progressbar" style="width: 25%" aria-valuenow="25" aria-label="Lesson Progress" aria-valuemin="0" aria-valuemax="100">
    </div>
  </div>
</aside><div class="container">
      <div class="row">
        <!-- START: inst/pkgdown/templates/navbar.html -->
<div id="sidebar-col" class="col-lg-4">
  <div id="sidebar" class="sidebar">
      <nav aria-labelledby="flush-headingEleven"><button role="button" aria-label="close menu" alt="close menu" aria-expanded="true" aria-controls="sidebar" class="collapse-toggle" data-collapse="Collapse " data-episodes="Episodes ">
          <i class="search-icon" data-feather="x" role="img"></i>
        </button>
        <div class="sidebar-inner">
          <div class="row mobile-row" id="theme-row-mobile">
            <div class="col" id="theme-selector">
              <li class="nav-item dropdown" id="theme-button-list">
                <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
                  <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><span class="d-lg-none ms-1" id="bd-theme-text">Toggle Theme</span>
                </button>
                <ul class="dropdown-menu dropdown-menu-right" aria-labelledby="bd-theme-text"><li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                      Light
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                      Dark
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                      <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                      Auto
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                </ul></li>
            </div>
          </div>
          <div class="row mobile-row">
            <div class="col">
              <div class="sidenav-view-selector">
                <div class="accordion accordion-flush" id="accordionFlush9">
                  <div class="accordion-item">
                    <h2 class="accordion-header" id="flush-headingNine">
                      <button class="accordion-button collapsed" id="instructor" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseNine" aria-expanded="false" aria-controls="flush-collapseNine">
                        <i id="eye" aria-hidden="true" class="icon" data-feather="eye"></i> Learner View
                      </button>
                    </h2>
                    <div id="flush-collapseNine" class="accordion-collapse collapse" aria-labelledby="flush-headingNine" data-bs-parent="#accordionFlush2">
                      <div class="accordion-body">
                        <a href="instructor/02-word_representations.html">Instructor View</a>
                      </div>
                    </div>
                  </div><!--/div.accordion-item-->
                </div><!--/div.accordion-flush-->
              </div><!--div.sidenav-view-selector -->
            </div><!--/div.col -->

            <hr></div><!--/div.mobile-row -->

          <div class="accordion accordion-flush" id="accordionFlush11">
            <div class="accordion-item">

              <button id="chapters" class="accordion-button show" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseEleven" aria-expanded="false" aria-controls="flush-collapseEleven">
                <h2 class="accordion-header chapters" id="flush-headingEleven">
                  EPISODES
                </h2>
              </button>
              <div id="flush-collapseEleven" class="accordion-collapse show collapse" aria-labelledby="flush-headingEleven" data-bs-parent="#accordionFlush11">

                <div class="accordion-body">
                  <div class="accordion accordion-flush" id="accordionFlush1">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading1">
        <a href="index.html">Summary and Setup</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush2">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading2">
        <a href="01-introduction.html">1. Introduction</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlushcurrent">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-headingcurrent">
      <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapsecurrent" aria-expanded="true" aria-controls="flush-collapsecurrent">
        <span class="visually-hidden">Current Chapter</span>
        <span class="current-chapter">
        2. From words to vectors
        </span>
      </button>
    </div><!--/div.accordion-header-->

    <div id="flush-collapsecurrent" class="accordion-collapse collapse show" aria-labelledby="flush-headingcurrent" data-bs-parent="#accordionFlushcurrent">
      <div class="accordion-body">
        <ul><li><a href="#introduction">Introduction</a></li>
<li><a href="#preprocessing-operations">Preprocessing Operations</a></li>
<li><a href="#nlp-pipeline">NLP Pipeline</a></li>
<li><a href="#word-embeddings">Word Embeddings</a></li>
<li><a href="#train-your-own-word2vec">Train your own Word2Vec</a></li>
        </ul></div><!--/div.accordion-body-->
    </div><!--/div.accordion-collapse-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush4">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading4">
        <a href="03-transformers.html">3. Transformers: BERT and Beyond</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush5">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading5">
        <a href="04-LargeLanguageModels.html">4. Using large language models</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

                </div>
              </div>
            </div>

            <hr class="half-width"><div class="accordion accordion-flush lesson-resources" id="accordionFlush12">
              <div class="accordion-item">
                <h2 class="accordion-header" id="flush-headingTwelve">
                  <button class="accordion-button collapsed" id="lesson-resources" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseTwelve" aria-expanded="false" aria-controls="flush-collapseTwelve">
                    RESOURCES
                  </button>
                </h2>
                <div id="flush-collapseTwelve" class="accordion-collapse collapse" aria-labelledby="flush-headingTwelve" data-bs-parent="#accordionFlush12">
                  <div class="accordion-body">
                    <ul><li>
                        <a href="key-points.html">Key Points</a>
                      </li>
                      <li>
                        <a href="reference.html#glossary">Glossary</a>
                      </li>
                      <li>
                        <a href="profiles.html">Learner Profiles</a>
                      </li>

                    </ul></div>
                </div>
              </div>
            </div>
            <hr class="half-width lesson-resources"><a href="aio.html">See all in one page</a>


            <hr class="d-none d-sm-block d-md-none"><div class="d-grid gap-1">

            </div>
          </div><!-- /div.accordion -->
        </div><!-- /div.sidebar-inner -->
      </nav></div><!-- /div.sidebar -->
  </div><!-- /div.sidebar-col -->
<!-- END:   inst/pkgdown/templates/navbar.html-->

        <!-- START: inst/pkgdown/templates/content-instructor.html -->
  <div class="col-xl-8 col-lg-12 primary-content">
    <nav class="lesson-content mx-md-4" aria-label="Previous and Next Chapter"><!-- content for small screens --><div class="d-block d-sm-block d-md-none">
        <a class="chapter-link" href="01-introduction.html"><i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>Previous</a>
        <a class="chapter-link float-end" href="03-transformers.html">Next<i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i></a>
      </div>
      <!-- content for large screens -->
      <div class="d-none d-sm-none d-md-block">
        <a class="chapter-link" href="01-introduction.html" rel="prev">
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>
          Previous: Introduction
        </a>
        <a class="chapter-link float-end" href="03-transformers.html" rel="next">
          Next: Transformers: BERT...
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i>
        </a>
      </div>
      <hr></nav><main id="main-content" class="main-content"><div class="container lesson-content">
        <h1>From words to vectors</h1>
        <p>Last updated on 2025-12-01 |

        <a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/edit/main/episodes/02-word_representations.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>



        <div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>

        

<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul><li>Why do we need to preprocess text in NLP?</li>
<li>What are the most common preprocessing operations and in which
contexts should each be used?</li>
<li>What properties do word embeddings have?</li>
<li>What is a word2vec model?</li>
<li>What insights can I get from word embeddings?</li>
<li>How do I train my own word2vec model?</li>
</ul></div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<p>After following this lesson, learners will be able to:</p>
<ul><li>Perform basic NLP preprocessing operations</li>
<li>Implement a basic NLP Pipeline</li>
<li>Explain the motivation for vectorization in modern NLP</li>
<li>Train a custom Word2Vec model using the <a href="https://radimrehurek.com/gensim/" class="external-link">Gensim</a> library</li>
<li>Apply a Word2Vec model to interpret and analyze semantics of text
(either a pre-trained model or custom model)</li>
<li>Describe the kinds of semantic relationships captured by Word2Vec,
and identify NLP tasks it is suited to support</li>
<li>Explain, with examples, what the limitations are for the Word2Vec
representation</li>
</ul></div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a></h2>
<hr class="half-width"><p>In this episode, we will learn about the importance of preprocessing
text in NLP, and how to apply common preprocessing operations to text
files. We will also learn more about <em>NLP Pipelines</em>, learn about
their basic components and how to construct such pipelines.</p>
<p>We will then address the transition from rule-based NLP to
distributional semantics approaches which encode text into numerical
representations based on statistical relationships between tokens. We
will introduce one particular algorithm for this kind of encoding called
Word2Vec proposed in 2013 by <a href="https://arxiv.org/pdf/1301.3781" class="external-link">Mikolov et al</a>. We will show
what kind of useful semantic relationships these representations encode
in text, and how we can use them to solve specific NLP tasks. We will
also discuss some of the limitations of Word2Vec which are addressed in
the next lesson on transformers before concluding with a summary of what
we covered in this lesson.</p>
</section><section><h2 class="section-heading" id="preprocessing-operations">Preprocessing Operations<a class="anchor" aria-label="anchor" href="#preprocessing-operations"></a></h2>
<hr class="half-width"><p>As is common in data science and machine learning, raw textual data
often comes in a form that is not readily suitable for downstream NLP
tasks. Preprocessing operations in NLP are analogous to the data
cleaning and sanitation steps in traditional non-NLP Machine Learning
tasks. Sometimes you are extracting text from PDF files which contain
line breaks, headers, tables etc. that are not relevant to NLP tasks and
which need to be removed. You may need to remove punctuation and special
characters, or lowercase text for some NLP tasks etc. Whether you need
to perform certain preprocessing operations, and the order in which you
should perform them, will depend on the NLP task at hand.</p>
<p>Also note that preprocessing can differ significantly if you work
with different languages. This is both in terms of which steps to apply,
but also which methods to use for a specific step.</p>
<p>Here we will analyze with more detail the most common pre-processing
steps when dealing with unstructured English text data:</p>
<div class="section level3">
<h3 id="data-formatting">Data Formatting<a class="anchor" aria-label="anchor" href="#data-formatting"></a></h3>
<p>Text comes from various sources and are available in different
formats (e.g., Microsoft Word documents, PDF documents, ePub files,
plain text files, Web pages etc…). The first step is to obtain a clean
text representation that can be transferred into python UTF-8 strings
that our scripts can manipulate.</p>
<p>Take a look at the
<code>data/84_frankenstein_or_the_modern_prometheus.txt</code> file:</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a>filename <span class="op">=</span> <span class="st">"data/84_frankenstein_or_the_modern_prometheus.txt"</span></span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(filename, <span class="st">'r'</span>, encoding<span class="op">=</span><span class="st">'utf-8'</span>) <span class="im">as</span> <span class="bu">file</span>:</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a>    text <span class="op">=</span> <span class="bu">file</span>.read()</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="bu">print</span>(text[:<span class="dv">300</span>]) <span class="co"># print the first 300 characters</span></span></code></pre>
</div>
<p>Our file is already in plain text so it might seem we do not need to
do anything; however, if we look closer we see new line characters
separating not only paragraphs but breaking the lines in the middle of
sentences. While this is useful to keep the text in a narrow space to
help the human reader, it introduces artificial breaks that can confuse
any automatic analysis (for example to identify where sentences start
and end).</p>
<p>One straightforward way is to replace the new lines with spaces so
all the text is in a single line:</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>text_flat <span class="op">=</span> text.replace(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span>, <span class="st">" "</span>)</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a><span class="bu">print</span>(text_flat[:<span class="dv">300</span>]) <span class="co"># print the first 300 characters</span></span></code></pre>
</div>
<p>Other data formatting operations might include: - Removal of special
or noisy characters. For example:</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">SH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode sh" tabindex="0"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="ex">-</span> Random symbols: <span class="st">"The total cost is </span><span class="va">$1</span><span class="st">20.00#"</span> → remove <span class="co">#</span></span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a><span class="ex">-</span> Incorrectly recognized letters or numbers: 1 misread as l, 0 as O, etc. Example: <span class="st">"l0ve"</span> → should be <span class="st">"love"</span></span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a><span class="ex">-</span> Control or formatting characters: <span class="dt">\n</span>, <span class="dt">\t</span>, <span class="dt">\r</span> appearing in the middle of sentences. Example: <span class="st">"Please\nsubmit\tyour form."</span> → <span class="st">"Please submit your form."</span></span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a><span class="ex">-</span> Non-standard Unicode characters: �, �, or other placeholder symbols where OCR failed. Example: <span class="st">"Th� quick brown fox"</span> → <span class="st">"The quick brown fox"</span></span></code></pre>
</div>
<ul><li>Remove HTML tags (e.g., if you are extracting text from Web
pages)</li>
<li>Strip non-meaningful punctuation (e.g., “The quick brown fox jumps
over the lazy dog and con- tinues to run across the field.)</li>
<li>Strip footnotes, headers, tables, images etc.</li>
<li>Remove URLs or phone numbers</li>
</ul><div id="callout1" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div class="callout-inner">
<div class="callout-content">
<p>What if I need to extract text from MS Word docs or PDF files or Web
pages There are various Python libraries for helping you extract and
manipulate text from these kinds of sources.</p>
<ul><li>For MS Word documents <a href="https://python-docx.readthedocs.io/en/latest/" class="external-link">python-docx</a> is
popular.</li>
<li>For (text-based) PDF files <a href="https://pypi.org/project/PyPDF2/" class="external-link">PyPDF2</a> and <a href="https://pymupdf.readthedocs.io/en/latest/" class="external-link">PyMuPDF</a> are widely
used. Note that some PDF files are encoded as images (pixels) and not
text. If the text in these files is digital (as opposed to scanned
handwriting), you can use OCR (Optical Character Recognition) libraries
such as <a href="https://pypi.org/project/pytesseract/" class="external-link">pytesseract</a>
to convert the image to machine-readable text.</li>
<li>For scraping text from websites, <a href="https://pypi.org/project/beautifulsoup4/" class="external-link">BeautifulSoup</a> and <a href="https://docs.scrapy.org/en/latest/" class="external-link">Scrapy</a> are some common
options.</li>
<li>LLMs also have something to offer here, and the field is moving
pretty fast. There are some interesting open source LLM-based document
parsers and OCR-like extractors such as <a href="https://github.com/datalab-to/marker" class="external-link">Marker</a>, or <a href="https://github.com/pymupdf/PyMuPDF4LLM" class="external-link">PyMuPDF4LLM</a>, just to
mention a couple.</li>
</ul></div>
</div>
</div>
<div id="callout2" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div class="callout-inner">
<div class="callout-content">
<p>Another important choice at the data formatting level is to decide at
what granularity do you need to perform the NLP task:</p>
<ul><li>Are you analyzing phenomena at the <strong>word level</strong>? For
example, detecting abusive language (based on a known vocabulary).</li>
<li>Do you need to first extract sentences from the text and do analysis
at the <strong>sentence level</strong>? For example, extracting entities
in each sentence.</li>
<li>Do you need full <strong>chunks of text</strong>? (e.g. paragraphs
or chapters?) For example, summarizing each paragraph in a
document.</li>
<li>Or perhaps you want to extract patterns at the <strong>document
level</strong>? For example each full book should have one genre tag
(Romance, History, Poetry).</li>
</ul><p>Sometimes your data will be already available at the desired
granularity level. If this is not the case, then during the tokenization
step you will need to figure out how to obtain the desired granularity
level.</p>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="tokenization">Tokenization<a class="anchor" aria-label="anchor" href="#tokenization"></a></h3>
<p>Tokenization is a foundational operation in NLP, as it helps to
create structure from raw text. This structure is a basic requirement
and input for modern NLP algorithms to attribute and interpret meaning
from text. This operation involves the segmentation of the text into
smaller units referred to as <code>tokens</code>. Tokens can be
sentences (e.g. <code>'the happy cat'</code>), words
(<code>'the', 'happy', 'cat'</code>), subwords
(<code>'un', 'happiness'</code>) or characters
(<code>'c','a', 't'</code>). Different NLP algorithms may require
different choices for the token unit. And different languages may
require different approaches to identify or segment these tokens.</p>
<p>Python strings are by definition sequences of characters, thus we can
iterate through the string character by character:</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">type</span>(text_flat))  <span class="co"># Should be &lt;class 'str'&gt;</span></span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a><span class="cf">for</span> ch <span class="kw">in</span> text_flat:</span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>    <span class="bu">print</span>(ch)</span></code></pre>
</div>
<p>However, it is often more advantageous if our atomic units are words.
As we saw in Lesson 1, the task of extracting word tokens from texts is
not trivial, therefore pre-trained models such as spaCy can help with
this step. In this case we will use the small English model that was
trained on a web corpus:</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="im">import</span> spacy</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>nlp <span class="op">=</span> spacy.load(<span class="st">"en_core_web_sm"</span>)</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a>doc <span class="op">=</span> nlp(text)</span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">type</span>(doc))  <span class="co"># Should be &lt;class 'spacy.tokens.doc.Doc'&gt;</span></span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(doc))</span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a><span class="bu">print</span>(doc)</span></code></pre>
</div>
<div id="callout3" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div class="callout-inner">
<div class="callout-content">
<p>A good word tokenizer for example, does not simply break up a text
based on spaces and punctuation, but it should be able to
distinguish:</p>
<ul><li>abbreviations that include points (e.g.: <em>e.g.</em>)</li>
<li>times (<em>11:15</em>) and dates written in various formats
(<em>01/01/2024</em> or <em>01-01-2024</em>)</li>
<li>word contractions such as <em>don’t</em>, these should be split into
<em>do</em> and <em>n’t</em>
</li>
<li>URLs</li>
</ul><p>Many older tokenizers are rule-based, meaning that they iterate over
a number of predefined rules to split the text into tokens, which is
useful for splitting text into word tokens for example. Modern large
language models use subword tokenization, which learn to break text into
pieces that are statistically convenient, this makes them more flexible
but less human-readable.</p>
</div>
</div>
</div>
<p>We can access the tokens by iterating the document and getting its
<code>.text</code> property:</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a>tokens_txt <span class="op">=</span> [token.text <span class="cf">for</span> token <span class="kw">in</span> doc]</span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a><span class="bu">print</span>(tokens_txt[:<span class="dv">15</span>])</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>['Letter', '1', '\n\n\n', 'St.', 'Petersburgh', ',', 'Dec.', '11th', ',', '17', '-', '-', '\n\n', 'TO', 'Mrs.']</code></pre>
</div>
<p>This shows us the individual tokens, including new lines and
punctuation (in case we didn’t run the previous cleaning step). spaCy
allows us to filter based on token properties. For example, assuming we
are not interested in the newlines, punctuation nor in numeric tokens,
in one single step we can keep only the token objects that contain
alphabetical characters:</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a>tokens <span class="op">=</span> [token <span class="cf">for</span> token <span class="kw">in</span> doc <span class="cf">if</span> token.is_alpha]</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a><span class="bu">print</span>(tokens[:<span class="dv">15</span>])</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[Letter, Petersburgh, TO, Saville, England, You, will, rejoice, to, hear, that, no, disaster, has, accompanied]</code></pre>
</div>
<p>We do not have to depend necessarily on the <code>Doc</code> and
<code>Token</code> spaCy objects. Once we tokenized the text with the
spaCy model, we can extract the list of words as a list of strings and
continue our text analysis:</p>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a>words <span class="op">=</span> [token.text <span class="cf">for</span> token <span class="kw">in</span> doc]</span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a><span class="bu">print</span>(words[:<span class="dv">20</span>])</span></code></pre>
</div>
<p>Again, it all depends on what your requirements are. For example,
sometimes it is more useful if our atomic units are sentences. Think of
the NLP task of classifying each whole sentence inside a text as
Positive/Negative/Neutral in terms of sentiment (e.g., within movie
reviews). spaCy also helps with this using a sentence segmentation
model:</p>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a>sentences <span class="op">=</span> [sent.text <span class="cf">for</span> sent <span class="kw">in</span> doc.sents]</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a>[<span class="bu">print</span>(s) <span class="cf">for</span> s <span class="kw">in</span> sentences[:<span class="dv">5</span>]]</span></code></pre>
</div>
<p>Note that in this case each sentence is a python object, and the
property <code>.text</code> returns an untokenized string (in terms of
words). But we can still access the list of word tokens inside each
sentence object if we want:</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a>sents_sample <span class="op">=</span> <span class="bu">list</span>(doc.sents)[:<span class="dv">10</span>]</span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a><span class="cf">for</span> sent <span class="kw">in</span> sents_sample:</span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Sentence:"</span>, sent.text)</span>
<span id="cb12-4"><a href="#cb12-4" tabindex="-1"></a>    <span class="cf">for</span> token <span class="kw">in</span> sent:</span>
<span id="cb12-5"><a href="#cb12-5" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"</span><span class="ch">\t</span><span class="st">Token:"</span>, token.text)</span></code></pre>
</div>
<p>This will give us enough flexibility to work at the sentence and word
level at the same time. In terms of what we can do with these sentences
once spaCy has identified them, we could ask humans to label each
sentence as either Positive/Negative/Neutral and train a supervised
model for sentiment classification on the set of sentences. Or if we
have a pre-trained model for sentiment classification on sentences, we
could load this model and classify each of our input sentences as either
Positive/Negative/Neutral.</p>
</div>
<div class="section level3">
<h3 id="lowercasing">Lowercasing<a class="anchor" aria-label="anchor" href="#lowercasing"></a></h3>
<p>Removing uppercases to e.g. avoid treating “Dog” and “dog” as two
different words is also a common step, for example to train word vector
representations, we want to merge both occurrences as they represent
exactly the same concept. Lowercasing can be done with Python directly
as:</p>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a>lower_text <span class="op">=</span> text_flat.lower()</span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a>lower_text[:<span class="dv">100</span>] <span class="co"># Beware that this is a python string operation</span></span></code></pre>
</div>
<p>Beware that lowercasing the whole string as a first step might affect
the tokenizer behavior since tokenization benefits from information
provided by case-sensitive strings. We can therefore tokenize first
using spaCy and then obtain the lowercase strings of each token using
the <code>.lower_</code> property:</p>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a>lower_text <span class="op">=</span> [token.lower_ <span class="cf">for</span> token <span class="kw">in</span> doc]</span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a>lower_text[:<span class="dv">10</span>] <span class="co"># Beware that this is a list of strings now!</span></span></code></pre>
</div>
<p>In other tasks, such as Named Entity Recognition (NER), lowercasing
before training can actually lower the performance of your model. This
is because words that start with an uppercase (not preceded by a period)
can represent proper nouns that map into Entities, for example:</p>
<div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a><span class="im">import</span> spacy</span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a><span class="co"># Preserving uppercase characters increases the likelihood that an NER model</span></span>
<span id="cb15-3"><a href="#cb15-3" tabindex="-1"></a><span class="co"># will correctly identify Apple and Will as a company (ORG) and a person (PER)</span></span>
<span id="cb15-4"><a href="#cb15-4" tabindex="-1"></a><span class="co"># respectively.</span></span>
<span id="cb15-5"><a href="#cb15-5" tabindex="-1"></a>str1 <span class="op">=</span> <span class="st">"My next laptop will be from Apple, Will said."</span> </span>
<span id="cb15-6"><a href="#cb15-6" tabindex="-1"></a><span class="co"># Lowercasing can reduce the likelihood of accurate labeling</span></span>
<span id="cb15-7"><a href="#cb15-7" tabindex="-1"></a>str2 <span class="op">=</span> <span class="st">"my next laptop will be from apple, will said."</span></span>
<span id="cb15-8"><a href="#cb15-8" tabindex="-1"></a></span>
<span id="cb15-9"><a href="#cb15-9" tabindex="-1"></a>nlp <span class="op">=</span> spacy.load(<span class="st">"en_core_web_sm"</span>)</span>
<span id="cb15-10"><a href="#cb15-10" tabindex="-1"></a>ents1 <span class="op">=</span> [ent.text <span class="cf">for</span> ent <span class="kw">in</span> nlp(str1).ents]</span>
<span id="cb15-11"><a href="#cb15-11" tabindex="-1"></a>ents2 <span class="op">=</span> [ent.text <span class="cf">for</span> ent <span class="kw">in</span> nlp(str2).ents]</span>
<span id="cb15-12"><a href="#cb15-12" tabindex="-1"></a></span>
<span id="cb15-13"><a href="#cb15-13" tabindex="-1"></a><span class="bu">print</span>(ents1)</span>
<span id="cb15-14"><a href="#cb15-14" tabindex="-1"></a><span class="bu">print</span>(ents2)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>['Apple', 'Will']
[]</code></pre>
</div>
</div>
<div class="section level3">
<h3 id="lemmatization">Lemmatization<a class="anchor" aria-label="anchor" href="#lemmatization"></a></h3>
<p>Although it has become less widely used in modern NLP approaches,
normalizing words into their <em>dictionary form</em> can help to focus
on relevant aspects of text. Consider how “eating”, “ate”, “eaten” are
all variations of the root verb “eat”. Each variation is sometimes known
as an <em>inflection</em> of the root word. Conversely, we say that the
word “eat” is the <em>lemma</em> for the words “eating”, “eats”,
“eaten”, “ate” etc. Lemmatization is therefore the process of rewriting
each token or word in a given input text as its lemma.</p>
<p>Lemmatization is not only a possible preprocessing step in NLP but
also an NLP task on its own, with different algorithms for it. Therefore
we also tend to use pre-trained models to perform lemmatization. Using
spaCy we can access the lemmmatized version of each token with the
<code>lemma_</code> property (notice the underscore!):</p>
<div class="codewrapper sourceCode" id="cb17">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a>lemmas <span class="op">=</span> [token.lemma_ <span class="cf">for</span> token <span class="kw">in</span> doc]</span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a><span class="bu">print</span>(lemmas[:<span class="dv">50</span>])</span></code></pre>
</div>
<p>Note that the list of lemmas is now a list of strings.</p>
<p>Having a lemmatized text allows us to merge the different surface
occurrences of the same concept into a single token. This can be very
useful for count-based NLP methods such as topic modelling approaches
which count the frequency of certain words to see how prevalent a given
topic is within a document. If you condense “eat”, “eating”, “ate”,
“eaten” to the same token “eat” then you can count four occurrences of
the same “topic” in a text, instead of treating these four tokens as
distinct or unrelated topics just because they are spelled differently.
You can also use lemmatization for generating word embeddings. For
example, you can have a single vector for <code>eat</code> instead of
one vector per verb tense.</p>
<p>As with each preprocessing operation, this step is optional.
Consider, for example, the cases where the differences of verb usage
according to tense is informative, or the difference between singular
and plural usage of nouns, in those cases lemmatizing will get rid of
important information for your task. For example, if your chosen NLP
task is to detect past tense verbs from a document, then lemmatizing
“eaten” into “eat” loses crucial information about tense that your model
requires.</p>
</div>
<div class="section level3">
<h3 id="stop-word-removal">Stop Word Removal<a class="anchor" aria-label="anchor" href="#stop-word-removal"></a></h3>
<p>The most frequent words in texts are those which contribute little
semantic value on their own: articles (‘the’, ‘a’, ‘an’), conjunctions
(‘and’, ‘or’, ‘but’), prepositions (‘on’, ‘by’), auxiliary verbs (‘is’,
‘am’), pronouns (‘he’, ‘which’), or any highly frequent word that might
not be of interest in several <em>content-only</em> related tasks. Let’s
define a small list of stop words for this specific case:</p>
<div class="codewrapper sourceCode" id="cb18">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" tabindex="-1"></a>STOP_WORDS <span class="op">=</span> [<span class="st">"the"</span>, <span class="st">"you"</span>, <span class="st">"will"</span>] <span class="co"># This list can be customized to your needs...</span></span></code></pre>
</div>
<p>Using Python directly, we need to manually define a list of what we
consider to be stop words and directly filter the tokens that match
this. Notice that lemmatization was a crucial step to get more coverage
with the stop word filtering:</p>
<div class="codewrapper sourceCode" id="cb19">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" tabindex="-1"></a>lemmas <span class="op">=</span> [token.lemma_ <span class="cf">for</span> token <span class="kw">in</span> doc]</span>
<span id="cb19-2"><a href="#cb19-2" tabindex="-1"></a>content_words <span class="op">=</span> []</span>
<span id="cb19-3"><a href="#cb19-3" tabindex="-1"></a><span class="cf">for</span> lemma <span class="kw">in</span> lemmas:</span>
<span id="cb19-4"><a href="#cb19-4" tabindex="-1"></a>    <span class="cf">if</span> lemma <span class="kw">not</span> <span class="kw">in</span> STOP_WORDS:</span>
<span id="cb19-5"><a href="#cb19-5" tabindex="-1"></a>        content_words.append(lemma)</span>
<span id="cb19-6"><a href="#cb19-6" tabindex="-1"></a><span class="bu">print</span>(content_words[:<span class="dv">20</span>])</span></code></pre>
</div>
<p>Using spaCy we can filter the stop words based on the token
properties:</p>
<div class="codewrapper sourceCode" id="cb20">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" tabindex="-1"></a>tokens_nostop <span class="op">=</span> [token <span class="cf">for</span> token <span class="kw">in</span> tokens <span class="cf">if</span> <span class="kw">not</span> token.is_stop]</span>
<span id="cb20-2"><a href="#cb20-2" tabindex="-1"></a><span class="bu">print</span>(tokens[:<span class="dv">15</span>])</span></code></pre>
</div>
<p>There is no canonical definition of stop words because what you
consider to be a stop word is directly linked to the objective of your
task at hand. For example, pronouns are usually considered stopwords,
but if you want to do gender bias analysis then pronouns are actually a
key element of your text processing pipeline. Similarly, removing
articles and prepositions from text is obviously not advised if you are
doing <em>dependency parsing</em> (the task of identifying the parts of
speech in a given text).</p>
<p>Another special case is the word ‘not’ which may encode the semantic
notion of <em>negation</em>. Removing such tokens can drastically change
the meaning of sentences and therefore affect the accuracy of models for
which negation is important to preserve (e.g., sentiment classification
“this movie was NOT great” vs. “this movie was great”).</p>
</div>
</section><section><h2 class="section-heading" id="nlp-pipeline">NLP Pipeline<a class="anchor" aria-label="anchor" href="#nlp-pipeline"></a></h2>
<hr class="half-width"><p>The concept of NLP pipeline refers to the sequence of operations that
we apply to our data in order to go from the original data
(e.g. original raw documents) to the expected outputs of our NLP Task at
hand. The components of the pipeline refer to any manipulation we apply
to the text, and do not necessarily need to be complex models, they
involve preprocessing operations, application of rules or machine
learning models, as well as formatting the outputs in a desired way.</p>
<div class="section level3">
<h3 id="a-simple-rule-based-classifier">A simple rule-based classifier<a class="anchor" aria-label="anchor" href="#a-simple-rule-based-classifier"></a></h3>
<p>Imagine we want to build a very lightweight sentiment classifier. A
basic approach is to design the following pipeline:</p>
<ol style="list-style-type: decimal"><li>Clean the original text file (as we saw in the Data Formatting
section)</li>
<li>Apply a sentence segmentation or tokenization model</li>
<li>Define a set of positive and negative words (a hard coded
dictionary)</li>
<li>For each sentence:
<ul><li>If it contains one or more of the positive words, classify as
<code>POSITIVE</code>
</li>
<li>If it contains one or more of the negative words, classify as
<code>NEGATIVE</code>
</li>
<li>Otherwise classify as <code>NEUTRAL</code>
</li>
</ul></li>
<li>Output a table with the original sentence and the assigned
label</li>
</ol><p>This is implemented with the following code:</p>
<ol style="list-style-type: decimal"><li>Read the text and normalize it into a single line</li>
</ol><div class="codewrapper sourceCode" id="cb21">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" tabindex="-1"></a><span class="im">import</span> spacy</span>
<span id="cb21-2"><a href="#cb21-2" tabindex="-1"></a>nlp <span class="op">=</span> spacy.load(<span class="st">"en_core_web_sm"</span>)</span>
<span id="cb21-3"><a href="#cb21-3" tabindex="-1"></a></span>
<span id="cb21-4"><a href="#cb21-4" tabindex="-1"></a>filename <span class="op">=</span> <span class="st">"data/84_frankenstein_or_the_modern_prometheus.txt"</span></span>
<span id="cb21-5"><a href="#cb21-5" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(filename, <span class="st">'r'</span>, encoding<span class="op">=</span><span class="st">'utf-8'</span>) <span class="im">as</span> <span class="bu">file</span>:</span>
<span id="cb21-6"><a href="#cb21-6" tabindex="-1"></a>    text <span class="op">=</span> <span class="bu">file</span>.read()</span>
<span id="cb21-7"><a href="#cb21-7" tabindex="-1"></a></span>
<span id="cb21-8"><a href="#cb21-8" tabindex="-1"></a>text <span class="op">=</span> text.replace(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span>, <span class="st">" "</span>) <span class="co"># some cleaning by removing new line characters</span></span></code></pre>
</div>
<ol start="2" style="list-style-type: decimal"><li>Apply Sentence segmentation</li>
</ol><div class="codewrapper sourceCode" id="cb22">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" tabindex="-1"></a>doc <span class="op">=</span> nlp(text)</span>
<span id="cb22-2"><a href="#cb22-2" tabindex="-1"></a>sentences <span class="op">=</span> [sent.text <span class="cf">for</span> sent <span class="kw">in</span> doc.sents]</span></code></pre>
</div>
<ol start="3" style="list-style-type: decimal"><li>Define the positive and negative words you care about:</li>
</ol><div class="codewrapper sourceCode" id="cb23">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" tabindex="-1"></a>positive_words <span class="op">=</span> [<span class="st">"happy"</span>, <span class="st">"excited"</span>, <span class="st">"delighted"</span>, <span class="st">"content"</span>, <span class="st">"love"</span>, <span class="st">"enjoyment"</span>]</span>
<span id="cb23-2"><a href="#cb23-2" tabindex="-1"></a>negative_words <span class="op">=</span> [<span class="st">"unhappy"</span>, <span class="st">"sad"</span>, <span class="st">"anxious"</span>, <span class="st">"miserable"</span>, <span class="st">"fear"</span>, <span class="st">"horror"</span>]</span></code></pre>
</div>
<ol start="4" style="list-style-type: decimal"><li>Apply the rules to each sentence and collect the labels</li>
</ol><div class="codewrapper sourceCode" id="cb24">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" tabindex="-1"></a>classified_sentences <span class="op">=</span> []</span>
<span id="cb24-2"><a href="#cb24-2" tabindex="-1"></a></span>
<span id="cb24-3"><a href="#cb24-3" tabindex="-1"></a><span class="cf">for</span> sent <span class="kw">in</span> sentences:</span>
<span id="cb24-4"><a href="#cb24-4" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">any</span>(word <span class="kw">in</span> sent.lower() <span class="cf">for</span> word <span class="kw">in</span> positive_words):</span>
<span id="cb24-5"><a href="#cb24-5" tabindex="-1"></a>        classified_sentences.append((sent, <span class="st">'POSITIVE'</span>))</span>
<span id="cb24-6"><a href="#cb24-6" tabindex="-1"></a>    <span class="cf">elif</span> <span class="bu">any</span>(word <span class="kw">in</span> sent.lower() <span class="cf">for</span> word <span class="kw">in</span> negative_words):</span>
<span id="cb24-7"><a href="#cb24-7" tabindex="-1"></a>        classified_sentences.append((sent, <span class="st">'NEGATIVE'</span>))</span>
<span id="cb24-8"><a href="#cb24-8" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb24-9"><a href="#cb24-9" tabindex="-1"></a>        classified_sentences.append((sent, <span class="st">'NEUTRAL'</span>))</span></code></pre>
</div>
<ol start="5" style="list-style-type: decimal"><li>Save the classified data</li>
</ol><div class="codewrapper sourceCode" id="cb25">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb25-2"><a href="#cb25-2" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(classified_sentences, columns<span class="op">=</span>[<span class="st">'sentence'</span>, <span class="st">'label'</span>])</span>
<span id="cb25-3"><a href="#cb25-3" tabindex="-1"></a>df.to_csv(<span class="st">'results_naive_rule_classifier.csv'</span>, sep<span class="op">=</span><span class="st">'</span><span class="ch">\t</span><span class="st">'</span>)</span></code></pre>
</div>
<div id="challenge1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div class="callout-inner">
<h3 class="callout-title">Challenge</h3>
<div class="callout-content">
<p>Discuss the pros and cons of the proposed NLP pipeline:</p>
<ol style="list-style-type: decimal"><li>Do you think it will give accurate results?</li>
<li>What do you think about the coverage of this approach? What cases
will it miss?</li>
<li>Think of possible drawbacks of chaining components in a
pipeline.</li>
</ol></div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<ol style="list-style-type: decimal"><li>This classifier only considers the presence of one word to apply a
label. It does not analyze sentence semantics or even syntax.</li>
<li>Given how the rules are defined, if both positive and negative words
are present in the same sentence it will assign the
<code>POSITIVE</code> label. It will generate a lot of false positives
because of the simplistic rules</li>
<li>The errors from previous steps get carried over to the next steps
increasing the likelihood of noisy outputs.</li>
</ol></div>
</div>
</div>
</div>
<p>So far we’ve seen how to format and segment the text to have atomic
data at the word level or sentence level. We then apply operations to
the word and sentence strings. This approach still depends on counting
and exact keyword matching. And as we have already seen it has several
limitations. The method cannot interpret words outside the dictionary
defined for example.</p>
<p>One way to combat this is by transforming each word into numeric
representation and study statistical patterns in how these words are
distributed in text. For example, what words tend to occur “close” to a
given word in my data? For example, if we analyze restaurant menus we
find that “cheese”, “mozzarella”, “base” etc. frequently occur near the
token “pizza”. We can then exploit these statistical patterns to inform
various NLP tasks. This concept is commonly known as <a href="https://arxiv.org/pdf/1905.01896" class="external-link">distributional semantics</a>. It
is based on the assumption “words that appear in similar contexts have
similar meanings.”</p>
<p>This concept is powerful for enabling, for example, the measurement
of semantic similarity of words, sentences, phrases etc. in text. And
this, in turn, can help with other downstream NLP tasks, as we shall see
in the next section on word embeddings.</p>
</div>
</section><section><h2 class="section-heading" id="word-embeddings">Word Embeddings<a class="anchor" aria-label="anchor" href="#word-embeddings"></a></h2>
<hr class="half-width"><div class="section level3">
<h3 id="reminder-neural-networks">Reminder: Neural Networks<a class="anchor" aria-label="anchor" href="#reminder-neural-networks"></a></h3>
<p>Understanding how neural networks work is out of the scope of this
course. For our purposes we will simplify the explanation in order to
conceptually understand how Neural Network works. A Neural Network (NN)
is a pattern-finding machine with layers (a <em>deep</em> neural network
is the same concept but scaled to dozens or even hundreds of layers). In
a neural network, each layer has several interconnected
<em>neurons</em>, each one corresponding to a random number initially.
The deeper the network is, the more complex patterns it can learn. As
the neural netork gets trained (that is, as it sees several labeled
examples that we provide), each neuron value will be updated in order to
maximize the probability of getting the answers right. A well trained
neural network will be able to predict the right labels on completely
new data with certain accuracy.</p>
<figure><img src="fig/emb_neuralnet.png" alt="After seeing thousands of examples, each layer represents different “features” that maximize the success of the task, but they are not human-readable. The last layer acts as a classifier and outputs the most likely label given the input" class="figure mx-auto d-block"><div class="figcaption">After seeing thousands of examples, each layer
represents different “features” that maximize the success of the task,
but they are not human-readable. The last layer acts as a classifier and
outputs the most likely label given the input</div>
</figure><p>The main difference with traditional machine learning models is that
we do not need to design explicitly any features, rather the network
will <em>adjust itself</em> by looking at the data alone and executing
the back-propagation algorithm. The main job when using NNs is to encode
our data properly so it can be fed into the network.</p>
</div>
<div class="section level3">
<h3 id="rationale-behind-embeddings">Rationale behind Embeddings<a class="anchor" aria-label="anchor" href="#rationale-behind-embeddings"></a></h3>
<p><strong>A word embedding is a numeric vector that represents a
word</strong>. Word2Vec exploits the “feature agnostic” power of neural
networks to transform word strings into trained word numeric
representations. Hence we still use words as features but instead of
using the string directly, we transform that string into its
corresponding vector in the pre-trained Word2Vec model. And because both
the network input and output are the words themselves in text, we
basically have billions of <em>labeled</em> training datapoints for
free.</p>
<figure><img src="fig/emb_embeddings.png" class="figure mx-auto d-block"></figure><p>To obtained the word embeddings, a shallow neural network is
optimized with the task of language modeling and the final hidden layer
inside the trained network holds the fixed size vectors whose values can
be mapped into linguistic properties (since the training objective was
language modeling). Since similar words occur in similar contexts, or
have same characteristics, a properly trained model will learn to assign
similar vectors to similar words.</p>
<p>By representing words with vectors, we can mathematically manipulate
them through vector arithmetic and express semantic similarity in terms
of vector distance. Because the size of the learned vectors is not
proportional to the amount of documents we can learn the representations
from larger collections of texts, obtaining more robust representations,
that are less corpus-dependent.</p>
<p>There are two main algorithms for training Word2Vec:</p>
<ul><li>Continuous Bag-of-Words (CBOW): Predicts a target word based on its
surrounding context words.</li>
<li>Continuous Skip-Gram: Predicts surrounding context words given a
target word.</li>
</ul><figure><img src="fig/emb13.png" class="figure mx-auto d-block"></figure><p>If you want to know more about the technicl aspecs of training
Word2Vec you can visit this <a href="https://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/" class="external-link">tutorial</a></p>
</div>
<div class="section level3">
<h3 id="the-word2vec-vector-space">The Word2Vec Vector Space<a class="anchor" aria-label="anchor" href="#the-word2vec-vector-space"></a></h3>
<p>The python module <code>gensim</code> offers a user-friendly
interface to interact with pre-trained Word2vec models and also to train
our own. First we will explore the model from the original Word2Vec
paper, which was trained on a big corpus from Google News (English news
articles). We will see what functionalities are available to explore a
vector space. Then we will prepare our own text step-by-step to train
our own Word2vec models and save them.</p>
</div>
<div class="section level3">
<h3 id="load-the-embeddings-and-inspect-them">Load the embeddings and inspect them<a class="anchor" aria-label="anchor" href="#load-the-embeddings-and-inspect-them"></a></h3>
<p>The library <code>gensim</code> has a repository with English
pre-trained models. We can take a look at the models:</p>
<div class="codewrapper sourceCode" id="cb26">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" tabindex="-1"></a><span class="im">import</span> gensim.downloader</span>
<span id="cb26-2"><a href="#cb26-2" tabindex="-1"></a>available_models <span class="op">=</span> gensim.downloader.info()[<span class="st">'models'</span>].keys()</span>
<span id="cb26-3"><a href="#cb26-3" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">list</span>(available_models))</span></code></pre>
</div>
<p>We will download the google News model with:</p>
<div class="codewrapper sourceCode" id="cb27">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" tabindex="-1"></a>w2v_model <span class="op">=</span> gensim.downloader.load(<span class="st">'word2vec-google-news-300'</span>)</span></code></pre>
</div>
<p>We can do some basic checkups such as showing how many words are in
the vocabulary (i.e., for how many words do we have an available
vector), what is the total number of dimensions in each vector, and
print the components of a vector for a given word:</p>
<div class="codewrapper sourceCode" id="cb28">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(w2v_model.key_to_index.keys())) <span class="co"># 3 million words</span></span>
<span id="cb28-2"><a href="#cb28-2" tabindex="-1"></a><span class="bu">print</span>(w2v_model.vector_size) <span class="co"># 300 dimensions. This can be chosen when training your own model</span></span>
<span id="cb28-3"><a href="#cb28-3" tabindex="-1"></a><span class="bu">print</span>(w2v_model[<span class="st">'car'</span>][:<span class="dv">10</span>]) <span class="co"># The first 10 dimensions of the vector representing 'car'.</span></span>
<span id="cb28-4"><a href="#cb28-4" tabindex="-1"></a><span class="bu">print</span>(w2v_model[<span class="st">'cat'</span>][:<span class="dv">10</span>]) <span class="co"># The first 10 dimensions of the vector representing 'cat'.</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>3000000
300
[ 0.13085938  0.00842285  0.03344727 -0.05883789  0.04003906 -0.14257812
  0.04931641 -0.16894531  0.20898438  0.11962891]
[ 0.0123291   0.20410156 -0.28515625  0.21679688  0.11816406  0.08300781
  0.04980469 -0.00952148  0.22070312 -0.12597656]</code></pre>
</div>
<p>As we can see, this is a very large model with 3 million words and
the dimensionality chosen at training time was 300, thus each word will
have a 300-dimension vector associated with it.</p>
<p>Even with such a big vocabulary we can always find a word that won’t
be in there:</p>
<div class="codewrapper sourceCode" id="cb30">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" tabindex="-1"></a><span class="bu">print</span>(w2v_model[<span class="st">'bazzinga'</span>][:<span class="dv">10</span>])</span></code></pre>
</div>
<p>This will throw a <code>KeyError</code> as the model does not know
that word. Unfortunately this is a limitation of Word2vec - unseen words
(words that were not included in the training data) cannot be
interpreted by the model.</p>
<p>Now let’s talk about the vectors themselves. They are not easy to
interpret as they are a bunch of floating point numbers. These are the
weights that the network learned when optimizing for language modelling.
As the vectors are hard to interpret, we rely on a mathematical method
to compute how similar two vectors are. Generally speaking, the
recommended metric for measuring similarity between two high-dimensional
vectors is <a href="https://en.wikipedia.org/wiki/Cosine_similarity" class="external-link">cosine
similarity</a> .</p>
<div id="callout4" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div class="callout-inner">
<div class="callout-content">
<p><a href="https://en.wikipedia.org/wiki/Cosine_similarity" class="external-link">cosine
similarity</a> ranges between [<code>-1</code> and <code>1</code>]. It
is the cosine of the angle between two vectors, divided by the product
of their length. Mathematically speaking, when two vectors point in
exactly the same direction their cosine will be 1, and when they point
in the opposite direction their cosine will be -1. In python we can use
Numpy to compute the cosine similarity of vectors.</p>
<figure><img src="fig/emb12.png" alt="" class="figure mx-auto d-block"></figure></div>
</div>
</div>
<p>We can use <code>sklearn</code> learn to measure any pair of
high-dimensional vectors:</p>
<div class="codewrapper sourceCode" id="cb31">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" tabindex="-1"></a><span class="im">from</span> sklearn.metrics.pairwise <span class="im">import</span> cosine_similarity</span>
<span id="cb31-2"><a href="#cb31-2" tabindex="-1"></a></span>
<span id="cb31-3"><a href="#cb31-3" tabindex="-1"></a>car_vector <span class="op">=</span> w2v_model[<span class="st">'car'</span>]</span>
<span id="cb31-4"><a href="#cb31-4" tabindex="-1"></a>cat_vector <span class="op">=</span> w2v_model[<span class="st">'cat'</span>]</span>
<span id="cb31-5"><a href="#cb31-5" tabindex="-1"></a></span>
<span id="cb31-6"><a href="#cb31-6" tabindex="-1"></a>similarity <span class="op">=</span> cosine_similarity([car_vector], [cat_vector])</span>
<span id="cb31-7"><a href="#cb31-7" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Cosine similarity between 'car' and 'cat': </span><span class="sc">{</span>similarity[<span class="dv">0</span>][<span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb31-8"><a href="#cb31-8" tabindex="-1"></a></span>
<span id="cb31-9"><a href="#cb31-9" tabindex="-1"></a>similarity <span class="op">=</span> cosine_similarity([w2v_model[<span class="st">'hamburger'</span>]], [w2v_model[<span class="st">'pizza'</span>]])</span>
<span id="cb31-10"><a href="#cb31-10" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Cosine similarity between 'hamburger' and 'pizza': </span><span class="sc">{</span>similarity[<span class="dv">0</span>][<span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb32">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" tabindex="-1"></a></span>
<span id="cb32-2"><a href="#cb32-2" tabindex="-1"></a>Cosine similarity between <span class="st">'car'</span> <span class="kw">and</span> <span class="st">'cat'</span>: <span class="fl">0.21528185904026031</span></span>
<span id="cb32-3"><a href="#cb32-3" tabindex="-1"></a>Cosine similarity between <span class="st">'hamburger'</span> <span class="kw">and</span> <span class="st">'pizza'</span>: <span class="fl">0.6153676509857178</span></span></code></pre>
</div>
<p>Or you can use directly the
<code>w2v_model.similarity('car', 'cat')</code> function which gives the
same result.</p>
<p>The higher similarity score between the hamburger and pizza indicates
they are more similar based on the contexts where they appear in the
training data. Even though is hard to read all the floating numbers in
the vectors, we can trust this metric to always give us a hint of which
words are semantically closer than others</p>
<div id="challenge2" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div class="callout-inner">
<h3 class="callout-title">Challenge</h3>
<div class="callout-content">
<p>Think of different word pairs and try to guess how close or distant
they will be from each other. Use the similarity measure from the
word2vec module to compute the metric and discuss if this fits your
expectations. If not, can you come up with a reason why this was not the
case?</p>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2"> Show me the solution </h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" data-bs-parent="#accordionSolution2" aria-labelledby="headingSolution2">
<div class="accordion-body">
<p>Some interesting cases include synonyms, antonyms and morphologically
related words:</p>
<div class="codewrapper sourceCode" id="cb33">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" tabindex="-1"></a><span class="bu">print</span>(w2v_model.similarity(<span class="st">'democracy'</span>, <span class="st">'democratic'</span>))</span>
<span id="cb33-2"><a href="#cb33-2" tabindex="-1"></a><span class="bu">print</span>(w2v_model.similarity(<span class="st">'queen'</span>, <span class="st">'princess'</span>))</span>
<span id="cb33-3"><a href="#cb33-3" tabindex="-1"></a><span class="bu">print</span>(w2v_model.similarity(<span class="st">'love'</span>, <span class="st">'hate'</span>)) <span class="co">#!! (think of "I love X" and "I hate X")</span></span>
<span id="cb33-4"><a href="#cb33-4" tabindex="-1"></a><span class="bu">print</span>(w2v_model.similarity(<span class="st">'love'</span>, <span class="st">'lover'</span>))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code><span><span class="fl">0.86444813</span></span>
<span><span class="fl">0.7070532</span></span>
<span><span class="fl">0.6003957</span></span>
<span><span class="fl">0.48608577</span></span></code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="vector-neighborhoods">Vector Neighborhoods<a class="anchor" aria-label="anchor" href="#vector-neighborhoods"></a></h3>
<p>Now that we have a metric we can trust, we can retrieve neighborhoods
of vectors that are close to a given word. This is analogous to
retrieving semantically related terms to a target term. Let’s explore
the neighborhood around `pizza` using the <code>most_similar()</code>
method:</p>
<div class="codewrapper sourceCode" id="cb35">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" tabindex="-1"></a><span class="bu">print</span>(w2v_model.most_similar(<span class="st">'pizza'</span>, topn<span class="op">=</span><span class="dv">10</span>))</span></code></pre>
</div>
<p>This returns a list of ranked tuples with the form (word,
similarity_score). The list is already ordered in descent, so the first
element is the closest vector in the vector space, the second element is
the second closest word and so on…</p>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[('pizzas', 0.7863470911979675),
('Domino_pizza', 0.7342829704284668),
('Pizza', 0.6988078355789185),
('pepperoni_pizza', 0.6902607083320618),
('sandwich', 0.6840401887893677),
('burger', 0.6569692492485046),
('sandwiches', 0.6495091319084167),
('takeout_pizza', 0.6491535902023315),
('gourmet_pizza', 0.6400628089904785),
('meatball_sandwich', 0.6377009749412537)]</code></pre>
</div>
<p>Exploring neighborhoods can help us understand why some vectors are
closer (or not so much). Take the case of <em>love</em> and
<em>lover</em>, originally we might think these should be very close to
each other but by looking at their neighborhoods we understand why this
is not the case:</p>
<div class="codewrapper sourceCode" id="cb37">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" tabindex="-1"></a><span class="bu">print</span>(w2v_model.most_similar(<span class="st">'love'</span>, topn<span class="op">=</span><span class="dv">10</span>))</span>
<span id="cb37-2"><a href="#cb37-2" tabindex="-1"></a><span class="bu">print</span>(w2v_model.most_similar(<span class="st">'lover'</span>, topn<span class="op">=</span><span class="dv">10</span>))</span></code></pre>
</div>
<p>This returns a list of ranked tuples with the form (word,
similarity_score). The list is already ordered in descent, so the first
element is the closest vector in the vector space, the second element is
the second closest word and so on…</p>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[('loved', 0.6907791495323181), ('adore', 0.6816874146461487), ('loves', 0.6618633270263672), ('passion', 0.6100709438323975), ('hate', 0.6003956198692322), ('loving', 0.5886634588241577), ('Ilove', 0.5702950954437256), ('affection', 0.5664337873458862), ('undying_love', 0.5547305345535278), ('absolutely_adore', 0.5536840558052063)]

[('paramour', 0.6798686385154724), ('mistress', 0.6387110352516174), ('boyfriend', 0.6375402212142944), ('lovers', 0.6339589953422546), ('girlfriend', 0.6140860915184021), ('beau', 0.609399676322937), ('fiancé', 0.5994566679000854), ('soulmate', 0.5993717312812805), ('hubby', 0.5904166102409363), ('fiancée', 0.5888950228691101)]</code></pre>
</div>
<p>The first word is a noun or a verb (depending on the context) that
denotes affection to someone/something , so it is associated with other
concepts of affection (positive or negative). The case of <em>lover</em>
is used to describe a person, hence the associated concepts are
descriptors of people with whom the lover can be associated.</p>
</div>
<div class="section level3">
<h3 id="word-analogies-with-vectors">Word Analogies with Vectors<a class="anchor" aria-label="anchor" href="#word-analogies-with-vectors"></a></h3>
<p>Another powerful property that word embeddings show is that vector
algebra can preserve semantic analogy. An analogy is a comparison
between two different things based on their similar features or
relationships, for example king is to queen as man is to woman. We can
mimic this operations directly on the vectors using the
<code>most_similar()</code> method with the <code>positive</code> and
<code>negative</code> parameters:</p>
<div class="codewrapper sourceCode" id="cb39">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" tabindex="-1"></a><span class="co"># king is to man as what is to woman?</span></span>
<span id="cb39-2"><a href="#cb39-2" tabindex="-1"></a><span class="co"># king + woman - man = queen</span></span>
<span id="cb39-3"><a href="#cb39-3" tabindex="-1"></a>w2v_model.most_similar(positive<span class="op">=</span>[<span class="st">'king'</span>, <span class="st">'woman'</span>], negative<span class="op">=</span>[<span class="st">'man'</span>])</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[('queen', 0.7118192911148071),
 ('monarch', 0.6189674735069275),
 ('princess', 0.5902431011199951),
 ('crown_prince', 0.5499460697174072),
 ('prince', 0.5377321243286133),
 ('kings', 0.5236844420433044),
 ('Queen_Consort', 0.5235945582389832),
 ('queens', 0.5181134343147278),
 ('sultan', 0.5098593235015869),
 ('monarchy', 0.5087411403656006)]</code></pre>
</div>
</div>
</section><section><h2 class="section-heading" id="train-your-own-word2vec">Train your own Word2Vec<a class="anchor" aria-label="anchor" href="#train-your-own-word2vec"></a></h2>
<hr class="half-width"><p>The <code>gensim</code> package has implemented everything for us,
this means we have to focus mostly on obtaining clean data and then
calling the <code>Word2Vec</code> object to train our own model with our
own data. This can be done like follows:</p>
<div class="codewrapper sourceCode" id="cb41">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" tabindex="-1"></a><span class="im">import</span> spacy</span>
<span id="cb41-2"><a href="#cb41-2" tabindex="-1"></a><span class="im">from</span> gensim.models <span class="im">import</span> Word2Vec </span>
<span id="cb41-3"><a href="#cb41-3" tabindex="-1"></a></span>
<span id="cb41-4"><a href="#cb41-4" tabindex="-1"></a><span class="co"># Load and Tokenize the Text using spacy</span></span>
<span id="cb41-5"><a href="#cb41-5" tabindex="-1"></a>spacy_model <span class="op">=</span> spacy.load(<span class="st">"en_core_web_sm"</span>)</span>
<span id="cb41-6"><a href="#cb41-6" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">"data/84_frankenstein_clean.txt"</span>) <span class="im">as</span> f:</span>
<span id="cb41-7"><a href="#cb41-7" tabindex="-1"></a>    book_text <span class="op">=</span> f.read()    </span>
<span id="cb41-8"><a href="#cb41-8" tabindex="-1"></a>book_doc <span class="op">=</span> spacy_model(book_text)</span>
<span id="cb41-9"><a href="#cb41-9" tabindex="-1"></a>clean_tokens <span class="op">=</span> [tok.text.lower() <span class="cf">for</span> tok <span class="kw">in</span> book_doc <span class="cf">if</span> tok.is_alpha <span class="kw">and</span> <span class="kw">not</span> tok.is_stop]</span>
<span id="cb41-10"><a href="#cb41-10" tabindex="-1"></a></span>
<span id="cb41-11"><a href="#cb41-11" tabindex="-1"></a><span class="co"># Call and Train the Word2Vec model</span></span>
<span id="cb41-12"><a href="#cb41-12" tabindex="-1"></a>model <span class="op">=</span> Word2Vec([clean_tokens], sg<span class="op">=</span><span class="dv">0</span> , vector_size<span class="op">=</span><span class="dv">300</span>, window<span class="op">=</span><span class="dv">5</span>, min_count<span class="op">=</span><span class="dv">1</span>, workers<span class="op">=</span><span class="dv">4</span>)</span></code></pre>
</div>
<p>With this line code we are configuring our whole Word2Vec training
schema. We will be using CBOW (<code>sg=0</code> means CBOW,
<code>sg=1</code> means Skip-gram). We are interested in having vectors
with 300 dimensions <code>vector_size=300</code> and a context size of 5
surrounding words <code>window=5</code>. Because we already filtered our
tokens, we include all words present in the filtered corpora, regardless
of their frequency of occurrence <code>min_count=1</code>. The last
parameters tells python to use 4 CPU cores for training.</p>
<p>See the Gensim <a href="https://radimrehurek.com/gensim/models/word2vec.html" class="external-link">documentation</a>
for more training options.</p>
<div class="section level3">
<h3 id="save-and-retrieve-your-model">Save and Retrieve your model<a class="anchor" aria-label="anchor" href="#save-and-retrieve-your-model"></a></h3>
<p>Once your model is trained it is useful to save the checkpoint in
order to retrieve it next time instead of having to train it every time.
You can save it with:</p>
<div class="codewrapper sourceCode" id="cb42">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" tabindex="-1"></a>model.save(<span class="st">"word2vec_mini_books.model"</span>)</span></code></pre>
</div>
<p>Let’s put everything together. We have now the following NLP task:
train our own Word2Vec model. We are interested on having vectors for
content words only, so even though our preprocessing will unfortunately
lose a lot of the original information, in exchange we will be able to
manipulate the most relevant conceptual words as individual numeric
representations.</p>
<p>To load back the pre-trained vectors you just created you can use the
following code:</p>
<div class="codewrapper sourceCode" id="cb43">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" tabindex="-1"></a>model <span class="op">=</span> Word2Vec.load(<span class="st">"word2vec_mini_books.model"</span>)</span>
<span id="cb43-2"><a href="#cb43-2" tabindex="-1"></a>w2v <span class="op">=</span> model.wv</span>
<span id="cb43-3"><a href="#cb43-3" tabindex="-1"></a><span class="co"># Test:</span></span>
<span id="cb43-4"><a href="#cb43-4" tabindex="-1"></a>w2v.most_similar(<span class="st">'monster'</span>)</span></code></pre>
</div>
<div id="challenge3" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div class="callout-inner">
<h3 class="callout-title">Challenge</h3>
<div class="callout-content">
<p>Let’s apply this step by step on a longer text. In this case, because
we are learning the process, our corpus will be only one book but in
reality we would like to train a network with thousands of them. We will
use two books: Frankenstein and Dracula to train a model of word
vectors.</p>
<p>Write the code to follow the proposed pipeline and train the word2vec
model. The proposed pipeline for this task is:</p>
<ul><li>load the text files</li>
<li>tokenize files</li>
<li>keep only alphanumerical tokens</li>
<li>lemmatize words</li>
<li>Remove stop words</li>
<li>Train a Word2Vec model (feed the clean tokens to the
<code>Word2Vec</code> object) with <code>vector_size=50</code>
</li>
<li>Save the trained model</li>
</ul></div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3"> Show me the solution </h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" data-bs-parent="#accordionSolution3" aria-labelledby="headingSolution3">
<div class="accordion-body">
<div class="codewrapper sourceCode" id="cb44">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" tabindex="-1"></a><span class="im">import</span> spacy</span>
<span id="cb44-2"><a href="#cb44-2" tabindex="-1"></a><span class="im">from</span> gensim.models <span class="im">import</span> Word2Vec </span>
<span id="cb44-3"><a href="#cb44-3" tabindex="-1"></a></span>
<span id="cb44-4"><a href="#cb44-4" tabindex="-1"></a><span class="kw">def</span> process_book(book_filename: <span class="bu">str</span>, spacy_model: spacy.lang) <span class="op">-&gt;</span> <span class="bu">list</span>[<span class="bu">str</span>]:</span>
<span id="cb44-5"><a href="#cb44-5" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(book_filename) <span class="im">as</span> f:</span>
<span id="cb44-6"><a href="#cb44-6" tabindex="-1"></a>        book_text <span class="op">=</span> f.read()</span>
<span id="cb44-7"><a href="#cb44-7" tabindex="-1"></a>    </span>
<span id="cb44-8"><a href="#cb44-8" tabindex="-1"></a>    book_doc <span class="op">=</span> spacy_model(book_text)</span>
<span id="cb44-9"><a href="#cb44-9" tabindex="-1"></a>    valid_tokens <span class="op">=</span> [tok <span class="cf">for</span> tok <span class="kw">in</span> book_doc <span class="cf">if</span> tok.is_alpha <span class="kw">and</span> <span class="kw">not</span> tok.is_stop]</span>
<span id="cb44-10"><a href="#cb44-10" tabindex="-1"></a>    lemmas <span class="op">=</span> [tok.lemma_ <span class="cf">for</span> tok <span class="kw">in</span> valid_tokens] </span>
<span id="cb44-11"><a href="#cb44-11" tabindex="-1"></a>    <span class="cf">return</span> lemmas</span>
<span id="cb44-12"><a href="#cb44-12" tabindex="-1"></a>    </span>
<span id="cb44-13"><a href="#cb44-13" tabindex="-1"></a>nlp <span class="op">=</span> spacy.load(<span class="st">"en_core_web_sm"</span>)</span>
<span id="cb44-14"><a href="#cb44-14" tabindex="-1"></a></span>
<span id="cb44-15"><a href="#cb44-15" tabindex="-1"></a><span class="co"># Load the Tokens</span></span>
<span id="cb44-16"><a href="#cb44-16" tabindex="-1"></a>franken_tokens <span class="op">=</span> process_book(<span class="st">"data/84_frankenstein_clean.txt"</span>, nlp)</span>
<span id="cb44-17"><a href="#cb44-17" tabindex="-1"></a>dracula_tokens <span class="op">=</span> process_book(<span class="st">"data/345_dracula_clean.txt"</span>, nlp)</span>
<span id="cb44-18"><a href="#cb44-18" tabindex="-1"></a></span>
<span id="cb44-19"><a href="#cb44-19" tabindex="-1"></a><span class="co"># Train our own model</span></span>
<span id="cb44-20"><a href="#cb44-20" tabindex="-1"></a>spooky_model <span class="op">=</span> Word2Vec([franken_tokens, dracula_tokens], sg<span class="op">=</span><span class="dv">0</span> , vector_size<span class="op">=</span><span class="dv">50</span>, window<span class="op">=</span><span class="dv">5</span>, min_count<span class="op">=</span><span class="dv">1</span>, workers<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb44-21"><a href="#cb44-21" tabindex="-1"></a></span>
<span id="cb44-22"><a href="#cb44-22" tabindex="-1"></a><span class="co"># Test the vectors</span></span>
<span id="cb44-23"><a href="#cb44-23" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(spooky_model.wv[<span class="st">'Frankenstein'</span>]))</span>
<span id="cb44-24"><a href="#cb44-24" tabindex="-1"></a><span class="bu">print</span>(spooky_model.wv[<span class="st">'Frankenstein'</span>][:<span class="dv">30</span>])</span>
<span id="cb44-25"><a href="#cb44-25" tabindex="-1"></a><span class="bu">print</span>(spooky_model.wv.most_similar(<span class="st">"Frankenstein"</span>))</span></code></pre>
</div>
</div>
</div>
</div>
</div>
<div id="dataset-size-in-training" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="dataset-size-in-training" class="callout-inner">
<h3 class="callout-title">Dataset size in training</h3>
<div class="callout-content">
<p>To obtain your own high-quality embeddings, the size/length of the
training dataset plays a crucial role. Generally <a href="https://cs.stanford.edu/~quocle/paragraph_vector.pdf" class="external-link">tens of
thousands of documents</a> are considered a reasonable amount of data
for decent results.</p>
<p>Is there a strict minimum? Not really. It’s important to keep in mind
that <code>vocabulary size</code>, <code>document length</code>, and
<code>desired vector size</code> all interact with each other.
Higher-dimensional vectors (e.g., 200–300 dimensions) provide more
features to capture a word’s meaning, resulting in higher-quality
embeddings that can represent words across a finer-grained and more
diverse set of contexts.</p>
<p>While Word2vec models typically perform better with large datasets
containing millions of words, using a single page is sufficient for
demonstration and learning purposes. This smaller dataset allows us to
train the model quickly and understand how word2vec works without the
need for extensive computational resources.</p>
</div>
</div>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<span class="callout-header">Key Points</span>
<div class="callout-inner">
<div class="callout-content">
<ul><li>We can run a preprocessing pipeline to obtain clear words that can
be used as features</li>
<li>We learned how are words converted into vectors of numbers (which
makes them interpretable for machines)</li>
<li>We can easily compute how words are similar to each other with the
cosine similarity</li>
<li>Using gensim we can train our own word2vec models</li>
</ul></div>
</div>
</div>
</div>
</section></div> <!-- / div.lesson-content -->
    </main><!-- / main#main-content.main-content --><nav class="bottom-pagination mx-md-4" aria-label="Previous and Next Chapter"><div class="d-block d-sm-block d-md-none">
        <a class="chapter-link" href="01-introduction.html"><i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>Previous</a>
        <a class="chapter-link float-end" href="03-transformers.html">Next<i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i></a>
      </div>
      <!-- content for large screens -->
      <div class="d-none d-sm-none d-md-block">
        <a class="chapter-link" href="01-introduction.html" rel="prev">
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>
          Previous: Introduction
        </a>
        <a class="chapter-link float-end" href="03-transformers.html" rel="next">
          Next: Transformers: BERT...
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i>
        </a>
      </div>
    </nav></div> <!-- / div.primary-content.col-xs-12 -->
<!-- END:   inst/pkgdown/templates/content-instructor.html-->

      </div><!--/div.row-->
      		<footer class="row footer mx-md-3"><hr><div class="col-md-6">
        <p>This lesson is subject to the <a href="CODE_OF_CONDUCT.html">Code of Conduct</a></p>
        <p>

        <a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/edit/main/episodes/02-word_representations.md" class="external-link">Edit on GitHub</a>

	
        | <a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/blob/main/CONTRIBUTING.md" class="external-link">Contributing</a>
        | <a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/" class="external-link">Source</a></p>
        <p>
        <a href="citation.html">Cite</a>
        | <a href="mailto:team@carpentries.org">Contact</a> | <a href="https://carpentries.org/about/" class="external-link">About</a></p>
		</div>
		<div class="col-md-6">

        <p>Materials licensed under <a href="LICENSE.html">CC-BY 4.0</a> by the authors</p>

        <p>Template licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/" class="external-link">CC-BY 4.0</a> by <a href="https://carpentries.org/" class="external-link">The Carpentries</a></p>
        <p>Built with <a href="https://github.com/carpentries/sandpaper/tree/0.17.3" class="external-link">sandpaper (0.17.3)</a>, <a href="https://github.com/carpentries/pegboard/tree/0.7.9" class="external-link">pegboard (0.7.9)</a>, and <a href="https://github.com/carpentries/varnish/tree/1.0.9" class="external-link">varnish (1.0.9)</a></p>
		</div>
		</footer></div> <!-- / div.container -->
	<div id="to-top">
		<a href="#top">
      <i class="search-icon" data-feather="arrow-up" role="img" aria-label="Back To Top"></i><br><!-- <span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top --><span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top
		</a>
	</div>
  <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "LearningResource",
  "@id": "https://esciencecenter-digital-skills.github.io/Natural-language-processing/02-word_representations.html",
  "inLanguage": "en",
  "dct:conformsTo": "https://bioschemas.org/profiles/LearningResource/1.0-RELEASE",
  "description": "A Carpentries Lesson teaching foundational data and coding skills to researchers worldwide",
  "keywords": "software, data, lesson, The Carpentries, NLP, English, social sciences, pre-alpha",
  "name": "From words to vectors",
  "creativeWorkStatus": "active",
  "url": "https://esciencecenter-digital-skills.github.io/Natural-language-processing/02-word_representations.html",
  "identifier": "https://esciencecenter-digital-skills.github.io/Natural-language-processing/02-word_representations.html",
  "dateCreated": "2024-04-24",
  "dateModified": "2025-12-01",
  "datePublished": "2025-12-30"
}

  </script><script>
		feather.replace();
	</script></body></html><!-- END:   inst/pkgdown/templates/layout.html-->

