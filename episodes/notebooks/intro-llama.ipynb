{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77c81437-3304-4842-b175-5e20ffa3302d",
   "metadata": {},
   "source": [
    "## Intro to LLMs\n",
    "\n",
    "This notebooks is a gentle introduction to LLMs and the implications of their usage. It is intended for those with no background in Machine Learning or Deep Learning but some background in data science or a methodologically related field in quantitative analysis.\n",
    "\n",
    "Use: https://huggingface.co/ggml-org/SmolLM3-3B-GGUF/tree/main (small and good quality)\n",
    "\n",
    "Explain extra step on top of transformers architecture to make LLMs (reinforcement learning step)\n",
    "\n",
    "§§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ffe598a-5549-4d82-a4b7-6e3330a759b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (512) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "llama_context: n_ctx_per_seq (512) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n"
     ]
    }
   ],
   "source": [
    "# import llama_cpp library\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# Path to your GGUF model file (replace with actual path)\n",
    "model_path = \"llama-2-13b-chat.Q4_K_M.gguf\"\n",
    "\n",
    "# Load model\n",
    "llm = Llama(model_path=model_path, verbose=False)\n",
    "llm_nondete = Llama(model_path=model_path, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236b3d13-b2be-4ac9-8bea-7fda87518bc6",
   "metadata": {},
   "source": [
    "### Example 1: first prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7fa10bc-2d64-4895-b04f-65a7fdc25443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Repsonse ===\n",
      "The short answer is that the sky appears blue because of a phenomenon called Rayleigh scattering, which is the scattering of light or other electromagnetic radiation by small particles in the atmosphere. The particles in the atmosphere, such as nitrogen molecules and aerosols, scatter the sun's light in all directions, but they scatter shorter (blue) wavelengths more than longer (red) wavelengths. This is why the sky appears blue during the daytime, when the sun is overhead and the light we see is predominantly blue.\n",
      "\n",
      "However, the question of why the sky is blue is more complex than this simple explanation suggests. There are many factors that can affect the color of\n",
      "=== Full output ===\n",
      "[{'text': \" The short answer is that the sky appears blue because of a phenomenon called Rayleigh scattering, which is the scattering of light or other electromagnetic radiation by small particles in the atmosphere. The particles in the atmosphere, such as nitrogen molecules and aerosols, scatter the sun's light in all directions, but they scatter shorter (blue) wavelengths more than longer (red) wavelengths. This is why the sky appears blue during the daytime, when the sun is overhead and the light we see is predominantly blue.\\n\\nHowever, the question of why the sky is blue is more complex than this simple explanation suggests. There are many factors that can affect the color of\", 'index': 0, 'logprobs': None, 'finish_reason': 'length'}]\n"
     ]
    }
   ],
   "source": [
    "# Define a prompt\n",
    "prompt = \"Why is the sky blue?\"\n",
    "\n",
    "# Generate text\n",
    "output = llm(\n",
    "    prompt,\n",
    "    max_tokens=150,        # max length of generated text\n",
    "    temperature=1,       # randomness (0 = deterministic)\n",
    "    top_p=0.9              # nucleus sampling\n",
    ")\n",
    "\n",
    "print(\"=== Repsonse ===\")\n",
    "# In llama-cpp-python, the output is a dict with 'choices'\n",
    "print(output[\"choices\"][0][\"text\"].strip())\n",
    "\n",
    "print(\"=== Full output ===\")\n",
    "# In llama-cpp-python, the output is a dict with 'choices'\n",
    "print(output[\"choices\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d8caba-0dba-4b44-b55f-227bc5c1c4ab",
   "metadata": {},
   "source": [
    "### Example 2: Hallucination\n",
    "When the model outputs information that is factually false or fabricated. E.g. Conspiracy theory about Grok (elon musk LLM) ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39b9fe0d-5717-47cc-aff7-a8b650a190ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Repsonse ===\n",
      "Railen Ackerby is a 23 year old professional esports player from the United Kingdom. He is best known for his expertise in the popular multiplayer game, Apex Legends, where he competes under the alias \"Raiden.\"\n",
      "\n",
      "Ackerby began his esports career in 2019, playing for various teams and participating in online tournaments. He quickly gained a reputation as one of the top players in the UK and was soon recruited by professional teams. In 2020, he joined the prestigious esports organization, FaZe Clan, and has since become one of the team's top performers.\n",
      "\n",
      "Acker\n",
      "=== Full output ===\n",
      "[{'text': '\\n\\nRailen Ackerby is a 23 year old professional esports player from the United Kingdom. He is best known for his expertise in the popular multiplayer game, Apex Legends, where he competes under the alias \"Raiden.\"\\n\\nAckerby began his esports career in 2019, playing for various teams and participating in online tournaments. He quickly gained a reputation as one of the top players in the UK and was soon recruited by professional teams. In 2020, he joined the prestigious esports organization, FaZe Clan, and has since become one of the team\\'s top performers.\\n\\nAcker', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}]\n"
     ]
    }
   ],
   "source": [
    "# Define a prompt\n",
    "halluc_prompt = \"Who is Railen Ackerby?\"\n",
    "\n",
    "# Generate text\n",
    "output = llm(\n",
    "    halluc_prompt,\n",
    "    max_tokens=150,        # max length of generated text\n",
    "    temperature=0.7,       # randomness (0 = deterministic)\n",
    "    top_p=0.9              # nucleus sampling\n",
    ")\n",
    "\n",
    "print(\"=== Repsonse ===\")\n",
    "# In llama-cpp-python, the output is a dict with 'choices'\n",
    "print(output[\"choices\"][0][\"text\"].strip())\n",
    "\n",
    "print(\"=== Full output ===\")\n",
    "# In llama-cpp-python, the output is a dict with 'choices'\n",
    "print(output[\"choices\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb4c9b1-4ab6-4403-b812-e4bf38f4c8ba",
   "metadata": {},
   "source": [
    "### Example 3: Non-determinism\n",
    "The model can output distinctly different answers for the same or equivalent input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b8c7aaf-58c7-4304-b21d-1e29ad81d58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Repsonse 1 ===\n",
      "Atoms are the building blocks of everything around us, from the air we breathe to the stars in the sky. Atoms are made up of three main parts: protons, neutrons, and electrons. Protons and neutrons are found in the nucleus (center) of the atom, while electrons orbit the nucleus in energy levels or electron shells.\n",
      "\n",
      "1. Protons: Protons are positively charged particles that reside in the nucleus of an atom. The number of protons in an atom determines the element the atom represents, with each element having a unique number of protons in its atoms. For example, hydrogen has one proton, while carbon has six protons.\n",
      "=== Repsonse 2 ===\n",
      "Atoms are the building blocks of matter. They are made up of three main parts: protons, neutrons, and electrons. Protons and neutrons are found in the nucleus of the atom, which is the central, positively charged part of the atom. Protons are positively charged and neutrons have no charge. Electrons are negatively charged and orbit around the nucleus in energy levels or electron shells. The number of protons in an atom determines the element of an atom, and the number of electrons determines the chemical properties of an atom.\n"
     ]
    }
   ],
   "source": [
    "# Define a prompt\n",
    "nondete_prompt = \"Give a description of what is inside an atom.\"\n",
    "\n",
    "# Generate text\n",
    "output_1 = llm(\n",
    "    nondete_prompt,\n",
    "    max_tokens=150,        # max length of generated text\n",
    "    temperature=1,       # randomness (0 = deterministic)\n",
    "    top_p=0.9              # nucleus sampling\n",
    ")\n",
    "\n",
    "# Generate text\n",
    "output_2 = llm_nondete(\n",
    "    nondete_prompt,\n",
    "    max_tokens=150,        # max length of generated text\n",
    "    temperature=1,       # randomness (0 = deterministic)\n",
    "    top_p=0.9              # nucleus sampling\n",
    ")\n",
    "\n",
    "print(\"=== Repsonse 1 ===\")\n",
    "# In llama-cpp-python, the output is a dict with 'choices'\n",
    "print(output_1[\"choices\"][0][\"text\"].strip())\n",
    "\n",
    "print(\"=== Repsonse 2 ===\")\n",
    "# In llama-cpp-python, the output is a dict with 'choices'\n",
    "print(output_2[\"choices\"][0][\"text\"].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d022b2a-ccc4-4dce-a88a-8df9ba5f3ac9",
   "metadata": {},
   "source": [
    "### Example 4: Outdated knowledge\n",
    "The model can be outdated to answer questions that are dependent on very recent knowledge which was not known at the time the model was trained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c30b5f-8b9e-4d53-8d88-18db738b92fc",
   "metadata": {},
   "source": [
    "### Example 5: Biases\n",
    "Model can be biased towards certain stereotypes represented in training data\n",
    "\n",
    "Prompt specific to country e.g. top authors in Mexico\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22becbd7-87c7-4093-bdd5-a5013a187c2b",
   "metadata": {},
   "source": [
    "### Example 6: Inconsistency\n",
    "The model can sometimes give contradictory answers to logically equivalent prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e306537-bdc9-48d8-822e-a5b3a1699d0c",
   "metadata": {},
   "source": [
    "### Example 7: Prompt engineering\n",
    "Calculation example. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641f7312-5534-4bce-adbc-1ddb0a8e48af",
   "metadata": {},
   "source": [
    "### Example 8: Compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968f0a60-5a16-43c9-bb8c-b285ce092557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bias_prompt = \"Write a two paragraph story where a nurse, a pilot, and a CEO are having lunch together.\"\n",
    "# response = chatbot(bias_prompt, max_new_tokens=500, do_sample=True, top_k=50, temperature=0.7)[0][\"generated_text\"]\n",
    "# print(response.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e6da9f-950b-40dd-9adf-c24667aece4e",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Discuss evaluation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmlesson",
   "language": "python",
   "name": "llmlesson"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
