{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c5cbaff-0c1f-4e7e-a23a-c55f1dc18a74",
   "metadata": {},
   "source": [
    "# Prompting strategies\n",
    "\n",
    "This section explores how changing the phrasing of prompts influences the responses of LLMs. Prompting strategies are systematic approaches to crafting inputs for LLMs to elicit the desired output. They involve deliberately structuring your requests, questions, or instructions to guide the model toward producing more accurate, relevant, and useful responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8cc5eb-b83c-489e-a285-d9bcf7e04c50",
   "metadata": {},
   "source": [
    "## 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff2f0af7-fc1f-4910-be8e-fe48b3ee0bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kodymoodley/Documents/repos/llmtut/myenv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad467a67-4fee-4111-948c-a49cb3afb611",
   "metadata": {},
   "source": [
    "## 2. Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94f1aa4d-4837-4ab2-a7bb-eb65514662b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:04<00:00,  2.05s/it]\n"
     ]
    }
   ],
   "source": [
    "# Pick a model (uncomment the one you wish to use)\n",
    "# model_id = \"HuggingFaceTB/SmolLM2-135M\" # base model\n",
    "# model_id = \"HuggingFaceTB/SmolLM2-135M-Instruct\" # fine-tuned assistant model\n",
    "# model_id = \"HuggingFaceTB/SmolLM3-3B-Base\" # base model\n",
    "model_id = \"HuggingFaceTB/SmolLM3-3B\" # fine-tuned assistant model\n",
    "# model_id = \"meta-llama/Llama-3.2-1B-Instruct\" # fine-tuned assistant model - needs HuggingFace login and access token\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "# Set pad_token_id to eos_token_id to avoid unncessary warning messages\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f803e055-3449-4c4d-a4a5-b7fd2e27f64d",
   "metadata": {},
   "source": [
    "## 3. Initialise inference pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb2ae072-aae7-4e19-acb6-3d90cb83c361",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "# Build text-generation inference pipeline\n",
    "llm = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b36e5aa-af34-4b71-be22-5bcff4d0b6e4",
   "metadata": {},
   "source": [
    "## 4. Prompting strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9838f7eb-cf90-4e9a-8c8c-7ce217dd87bc",
   "metadata": {},
   "source": [
    "### 4.1 Zero-Shot vs Few-Shot Prompting\n",
    "\n",
    "**Zero-shot prompting** gives the model a task without examples, while **few-shot prompting** provides examples to guide the model's behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a480d23-fe0b-458f-b72e-744e251929fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: Sentiment Analysis\n",
      "Text to analyze: 'The movie was okay, nothing special but not terrible either.'\n"
     ]
    }
   ],
   "source": [
    "# Task: Sentiment analysis\n",
    "test_text = \"The movie was okay, nothing special but not terrible either.\"\n",
    "\n",
    "print(\"Task: Sentiment Analysis\")\n",
    "print(f\"Text to analyze: '{test_text}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b586f58-3eb5-49d2-8585-bb0d0d4bb7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ZERO-SHOT PROMPTING ===\n",
      "Prompt: Analyze the sentiment of this text: 'The movie was okay, nothing special but not terrible either.'\n",
      "\n",
      "Response:  Analyze its sentiment polarity and degree. Choose from the following options: ('negative', 'neutral', 'positive'). Please explain your choice. \n",
      "Sentiment analysis is the process of determining whether a piece of text expresses a positive or negative emotion, or a neutral sentiment. In this case, the task is to classify the sentiment of the given text.\n",
      "\n",
      "Text: 'The movie was okay, nothing special but not terrible either.'\n",
      "\n",
      "Step 1: Identify key words or phrases.\n",
      "- The movie was okay\n"
     ]
    }
   ],
   "source": [
    "# Zero-shot prompting\n",
    "print(\"=== ZERO-SHOT PROMPTING ===\")\n",
    "zero_shot_prompt = f\"Analyze the sentiment of this text: '{test_text}'\"\n",
    "print(f\"Prompt: {zero_shot_prompt}\")\n",
    "print()\n",
    "\n",
    "zero_shot_response = llm(zero_shot_prompt, max_new_tokens=100, do_sample=True, top_k=20, temperature=0.7)[0][\"generated_text\"]\n",
    "zero_shot_response = zero_shot_response.replace(zero_shot_prompt, \"\")\n",
    "print(f\"Response: {zero_shot_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a1754c2-5d7b-4405-bbd5-ee9268c031a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FEW-SHOT PROMPTING ===\n",
      "Prompt: Analyze the sentiment of the following texts:\n",
      "\n",
      "Text: \"I absolutely loved this movie! Amazing acting and great plot.\"\n",
      "Sentiment: Positive\n",
      "\n",
      "Text: \"This was the worst film I've ever seen. Terrible acting.\"\n",
      "Sentiment: Negative\n",
      "\n",
      "Text: \"The book was decent. Had some good parts but also some boring sections.\"\n",
      "Sentiment: Neutral\n",
      "\n",
      "Text: \"The movie was okay, nothing special but not terrible either.\"\n",
      "Sentiment:\n",
      "\n",
      "Response:  Neutral\n",
      "\n",
      "Text: \"I'm so disappointed with the ending. It was so unsatisfying.\"\n",
      "Sentiment: Negative\n",
      "\n",
      "Text: \"What a beautiful performance! The actress truly brought the character to life.\"\n",
      "Sentiment: Positive\n",
      "\n",
      "Text: \"The director made some questionable choices, but overall, the film was entertaining.\"\n",
      "Sentiment: Neutral\n",
      "\n",
      "Text: \"This was one of the best days of my life! I can't believe how amazing it was.\"\n",
      "Sentiment: Positive\n",
      "\n",
      "Text:\n"
     ]
    }
   ],
   "source": [
    "# Few-shot prompting\n",
    "print(\"=== FEW-SHOT PROMPTING ===\")\n",
    "few_shot_prompt = f\"\"\"Analyze the sentiment of the following texts:\n",
    "\n",
    "Text: \"I absolutely loved this movie! Amazing acting and great plot.\"\n",
    "Sentiment: Positive\n",
    "\n",
    "Text: \"This was the worst film I've ever seen. Terrible acting.\"\n",
    "Sentiment: Negative\n",
    "\n",
    "Text: \"The book was decent. Had some good parts but also some boring sections.\"\n",
    "Sentiment: Neutral\n",
    "\n",
    "Text: \"{test_text}\"\n",
    "Sentiment:\"\"\"\n",
    "\n",
    "print(f\"Prompt: {few_shot_prompt}\")\n",
    "print()\n",
    "\n",
    "few_shot_response = llm(few_shot_prompt, max_new_tokens=100, do_sample=True, top_k=20, temperature=0.5)[0][\"generated_text\"]\n",
    "few_shot_response = few_shot_response.replace(few_shot_prompt, \"\")\n",
    "print(f\"Response: {few_shot_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26800d5-face-417f-b3b9-69ca475a7a6f",
   "metadata": {},
   "source": [
    "**Observation:** Notice how few-shot prompting typically produces more consistent, properly formatted responses compared to zero-shot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf793e1-aa77-4e02-b360-01088f415674",
   "metadata": {},
   "source": [
    "### 4.2 Chain-of-thought (CoT) prompting\n",
    "\n",
    "This prompt style elicits the model to explain its reasoning process step-by-step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29deecbb-308e-4320-bdf7-02e48ea748b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematical reasoning task\n",
    "math_problem = \"Sarah has 15 apples. She gives away 1/3 of them to friends and eats 2 herself. Sarah has:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76422419-7d94-4966-98c9-1eb9a60157c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STANDARD PROMPTING ===\n",
      "Prompt: Sarah has 15 apples. She gives away 1/3 of them to friends and eats 2 herself. Sarah has:\n",
      "Response:  15 - 15/3 - 2 = 15 - 5 - 2 = 8 apples left.\n",
      "She has 8 apples left.\n",
      "Her mother gives her another 2 apples. Now Sarah has 8 + 2 = 10 apples.\n",
      "She gives 1/2 of her apples to her brother. 1/2 of 10 apples is 10 / 2 = 5 apples.\n",
      "So Sarah gives 5 apples to her brother and has 10 - 5 = 5 apples left.\n",
      "Thus, Sarah has \\boxed{5} apples left.\n"
     ]
    }
   ],
   "source": [
    "# Standard prompting\n",
    "print(\"=== STANDARD PROMPTING ===\")\n",
    "standard_prompt = f\"{math_problem}\"\n",
    "print(f\"Prompt: {standard_prompt}\")\n",
    "standard_response = llm(standard_prompt, max_new_tokens=200, do_sample=True, top_k=20, temperature=0.5)[0][\"generated_text\"]\n",
    "standard_response = standard_response.replace(standard_prompt, \"\")\n",
    "print(f\"Response: {standard_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae8d5ba3-609c-411b-b42b-f3b762873fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CHAIN-OF-THOUGHT PROMPTING ===\n",
      "Prompt: Solve this step by step:\n",
      "\n",
      "Problem: Tom has 24 stroopwaffles. He gives 1/4 to his sister and eats 3 himself. How many stroopwaffles does he have left?\n",
      "\n",
      "Step 1: Calculate 1/4 of 24 candies = 24 ÷ 4 = 6 stroopwaffles given to sister\n",
      "Step 2: Subtract stroopwaffles given away and eaten: 24 - 6 - 3 = 15 stroopwaffles left\n",
      "\n",
      "Now solve this problem step by step:\n",
      "Problem: Sarah has 15 apples. She gives away 1/3 of them to friends and eats 2 herself. How many apples does she have left?\n",
      "\n",
      "Step 1:\n",
      "\n",
      "Response:  Calculate 1/3 of 15 apples = 15 ÷ 3 = 5 apples given away to friends\n",
      "Step 2: Subtract apples given away and eaten: 15 - 5 - 2 = 8 apples left\n",
      "\n",
      "Now solve this problem step by step:\n",
      "Problem: There are 20 marbles in a jar. 3/4 of them are blue and the rest are red. How many red marbles are in the jar?\n",
      "\n",
      "Step 1: Calculate the number of blue marbles = 20 × (3/4) = 15 blue marbles\n",
      "Step 2: Subtract blue marbles from total marbles to get red marbles: 20 - 15 = 5 red marbles\n",
      "\n",
      "Now solve this problem step by step:\n",
      "Problem: John has 18 cookies. He gives 2/3 of them to his sister and then eats 4. How many cookies does he have left?\n",
      "\n",
      "Step 1: Calculate the number of\n"
     ]
    }
   ],
   "source": [
    "# Chain-of-thought prompting\n",
    "print(\"=== CHAIN-OF-THOUGHT PROMPTING ===\")\n",
    "cot_prompt = f\"\"\"Solve this step by step:\n",
    "\n",
    "Problem: Tom has 24 stroopwaffles. He gives 1/4 to his sister and eats 3 himself. How many stroopwaffles does he have left?\n",
    "\n",
    "Step 1: Calculate 1/4 of 24 candies = 24 ÷ 4 = 6 stroopwaffles given to sister\n",
    "Step 2: Subtract stroopwaffles given away and eaten: 24 - 6 - 3 = 15 stroopwaffles left\n",
    "\n",
    "Now solve this problem step by step:\n",
    "Problem: {math_problem}\n",
    "\n",
    "Step 1:\"\"\"\n",
    "\n",
    "print(f\"Prompt: {cot_prompt}\")\n",
    "cot_response = llm(cot_prompt, max_new_tokens=200, do_sample=True, top_k=20, temperature=0.5)[0][\"generated_text\"]\n",
    "cot_response = cot_response.replace(cot_prompt, \"\")\n",
    "print()\n",
    "print(f\"Response: {cot_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3f582d-f57d-497b-bbb5-1f6089805cce",
   "metadata": {},
   "source": [
    "**Observation:** CoT prompting often leads to more accurate mathematical reasoning by forcing the model to show its work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad2b0ae-ceb1-4059-a9fb-bd464708da26",
   "metadata": {},
   "source": [
    "### 4.3 Role-based prompting\n",
    "\n",
    "Assigning the model a specific role or expertise can significantly improve the quality and focus of responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "697ab5c6-d99b-4ef8-9adb-d6872b4ea324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: Business Advice\n",
      "Question: What are the key factors to consider when starting a small business?\n"
     ]
    }
   ],
   "source": [
    "# Task\n",
    "question = \"What are the key factors to consider when starting a small business?\"\n",
    "print(\"Task: Business Advice\")\n",
    "print(f\"Question: {question}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1601db14-3b2b-4c2e-89eb-a8bf29a9a073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GENERIC PROMPTING ===\n",
      "Prompt: What are the key factors to consider when starting a small business?\n",
      "\n",
      "Response:  \n",
      "\n",
      "When starting a small business, there are several key factors to consider:\n",
      "\n",
      "1. **Market Research**: Understand your target audience, their needs, and preferences. Conduct market research to identify gaps in the market and potential competitors.\n",
      "\n",
      "2. **Business Plan**: Develop a comprehensive business plan that outlines your goals, strategies, and financial projections. This will help you stay organized and focused.\n",
      "\n",
      "3. **Financial Planning**: Determine your startup costs, funding sources, and projected revenue. Create a budget and track your expenses to ensure you stay on track.\n",
      "\n",
      "4. **Legal Structure**: Choose a business structure (e.g., sole proprietorship, LLC, corporation) and register your business with the relevant authorities.\n",
      "\n",
      "5. **Product or Service**: Define your product or service, and ensure it meets the needs of your target market.\n",
      "\n",
      "6. **Marketing and Sales**: Develop a marketing strategy to reach your target audience and create a sales plan to convert leads into customers.\n",
      "\n",
      "7. **Operations**: Plan your operations, including\n"
     ]
    }
   ],
   "source": [
    "# Generic prompting\n",
    "print(\"=== GENERIC PROMPTING ===\")\n",
    "generic_prompt = f\"{question}\"\n",
    "print(f\"Prompt: {generic_prompt}\")\n",
    "print()\n",
    "\n",
    "generic_response = llm(generic_prompt, max_new_tokens=200, do_sample=True, top_k=20, temperature=0.5)[0][\"generated_text\"]\n",
    "generic_response = generic_response.replace(generic_prompt, \"\")\n",
    "print(f\"Response: {generic_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de883275-eca2-4551-8152-97ff4ba66fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ROLE-BASED PROMPTING ===\n",
      "Prompt: You are an experienced business consultant with 20 years of experience helping entrepreneurs start successful companies. \n",
      "\n",
      "Question: What are the key factors to consider when starting a small business?\n",
      "\n",
      "Provide practical, actionable advice based on your expertise.\n",
      "\n",
      "Answer: \n",
      "\n",
      "Response:  Starting a small business is a thrilling yet challenging endeavor that requires careful planning, execution, and adaptability. As a seasoned business consultant, I've identified several key factors to consider when embarking on this journey. \n",
      "\n",
      "First and foremost, it's essential to conduct thorough market research to validate your business idea and understand your target audience. This involves analyzing your competitors, identifying gaps in the market, and determining the demand for your product or service. You can gather insights through online surveys, focus groups, and competitor analysis.\n",
      "\n",
      "Next, you must develop a solid business plan that outlines your mission, vision, goals, and strategies. This plan should include a detailed financial projection, including startup costs, projected revenue, and break-even analysis. A well-crafted business plan will serve as a roadmap for your business and help you secure funding if needed.\n",
      "\n",
      "Another crucial factor is selecting the right business structure and registering your business. You'll need to choose between sole proprietorship, partnership, LLC, or corporation, considering factors\n"
     ]
    }
   ],
   "source": [
    "# Role-based prompting\n",
    "print(\"=== ROLE-BASED PROMPTING ===\")\n",
    "role_prompt = f\"\"\"You are an experienced business consultant with 20 years of experience helping entrepreneurs start successful companies. \n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Provide practical, actionable advice based on your expertise.\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "print(f\"Prompt: {role_prompt}\")\n",
    "print()\n",
    "\n",
    "role_response = llm(role_prompt, max_new_tokens=200, do_sample=True, top_k=20, temperature=0.5)[0][\"generated_text\"]\n",
    "role_response = role_response.replace(role_prompt, \"\")\n",
    "print(f\"Response: {role_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c127128-d99f-409b-929a-23975abe2fa6",
   "metadata": {},
   "source": [
    "**Observation:** Role-based prompting typically produces more detailed, expert-level responses with practical insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f46173f-5431-4607-bc73-4fd8665085ab",
   "metadata": {},
   "source": [
    "### 4.4 Instruction Formatting and Structure\n",
    "The way you structure your prompt significantly affects the quality of the output. Clear formatting generally gives better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfe49060-cb49-4e3e-9547-301d284f3515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: Information Extraction\n",
      "Data: John Smith, 35, Software Engineer, New York. Jane Doe, 28, Marketing Manager, California.\n"
     ]
    }
   ],
   "source": [
    "data = \"John Smith, 35, Software Engineer, New York. Jane Doe, 28, Marketing Manager, California.\"\n",
    "print(\"Task: Information Extraction\")\n",
    "print(f\"Data: {data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31e92e98-431c-463f-989b-41ed1fdf1d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== POOR FORMATTING ===\n",
      "Prompt: 'John Smith, 35, Software Engineer, New York. Jane Doe, 28, Marketing Manager, California.'. What are John's and Jane's ages?\n",
      "\n",
      "Response:  They are 35 and 28 years old respectively. John is 35 years old and Jane is 28 years old.\n",
      "\n",
      "Here are the steps to solve this:\n",
      "\n",
      "Step 1: Identify the information given in the sentence.\n",
      "The sentence states that John Smith is 35 years old and Jane Doe is 28 years old.\n",
      "\n",
      "Step 2: Determine the question being asked.\n",
      "The question being asked is to find the ages of John and Jane.\n",
      "\n",
      "Step 3: Extract the relevant information.\n",
      "The relevant information is that John is 35 years old and Jane is 28 years old.\n",
      "\n",
      "Step 4: Answer the question based on the extracted information.\n",
      "Based on the extracted information, John's age is 35 years old and Jane's age is 28 years old. \n",
      "\n",
      "John is 35 years old and Jane is 28 years old. \n",
      "\n",
      "Here are the steps to solve this:\n",
      "\n",
      "Step 1: Identify the information given in the sentence.\n",
      "The sentence states that John Smith is 35 years\n"
     ]
    }
   ],
   "source": [
    "# Poor formatting\n",
    "print(\"=== POOR FORMATTING ===\")\n",
    "poor_prompt = f\"'{data}'. What are John's and Jane's ages?\"\n",
    "print(f\"Prompt: {poor_prompt}\")\n",
    "print()\n",
    "\n",
    "poor_response = llm(poor_prompt, max_new_tokens=200, do_sample=True, top_k=20, temperature=0.5)[0][\"generated_text\"]\n",
    "poor_response = poor_response.replace(poor_prompt, \"\")\n",
    "print(f\"Response: {poor_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4763b754-0a08-491f-9571-ed841d38d55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== WELL-STRUCTURED FORMATTING ===\n",
      "Prompt: Task: Extract information from the following text and format it clearly.\n",
      "\n",
      "Text: \"John Smith, 35, Software Engineer, New York. Jane Doe, 28, Marketing Manager, California.\"\n",
      "\n",
      "Instructions:\n",
      "1. Extract each person's name and age\n",
      "2. Use the format: [Name] - Age: [age], Job: [job], Location: [location]\n",
      "\n",
      "Output:\n",
      "\n",
      "Response:  \n",
      "\n",
      "[Name] - Age: [age], Job: [job], Location: [location]\n",
      "[Name] - Age: [age], Job: [job], Location: [location] \n",
      "\n",
      "Replace the brackets and placeholders with the actual data from the text. Make sure each person is listed separately. \n",
      "\n",
      "Note: The output should have two lines, each representing a person. The placeholders should be replaced with the actual data from the text. \n",
      "\n",
      "Constraints:\n",
      "- The output should have exactly two lines.\n",
      "- Each line should contain the name, age, job, and location of a person.\n",
      "- Use the format specified in the instructions. \n",
      "\n",
      "Input:\n",
      "\"John Smith, 35, Software Engineer, New York. Jane Doe, 28, Marketing Manager, California.\"\n",
      "\n",
      "Output:\n",
      "[Name] - Age: [age], Job: [job], Location: [location]\n",
      "[Name] - Age: [age], Job: [job], Location: [location] \n",
      "\n",
      "Replace the placeholders with\n"
     ]
    }
   ],
   "source": [
    "# Well-structured formatting\n",
    "print(\"=== WELL-STRUCTURED FORMATTING ===\")\n",
    "structured_prompt = f\"\"\"Task: Extract information from the following text and format it clearly.\n",
    "\n",
    "Text: \"{data}\"\n",
    "\n",
    "Instructions:\n",
    "1. Extract each person's name and age\n",
    "2. Use the format: [Name] - Age: [age], Job: [job], Location: [location]\n",
    "\n",
    "Output:\"\"\"\n",
    "\n",
    "print(f\"Prompt: {structured_prompt}\")\n",
    "print()\n",
    "\n",
    "structured_response = llm(structured_prompt, max_new_tokens=200, do_sample=True, top_k=20, temperature=0.5)[0][\"generated_text\"]\n",
    "structured_response = structured_response.replace(structured_prompt, \"\")\n",
    "print(f\"Response: {structured_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d0b2d8-0dae-4c9f-8088-75d06532502c",
   "metadata": {},
   "source": [
    "**Observation:** Clear, structured prompts with numbered instructions produce much more organized and useful outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3836b41d-4b39-4d5b-8010-1d60f60546d1",
   "metadata": {},
   "source": [
    "### 4.5 Negative prompting\n",
    "Sometimes it's as important to tell the LLM what NOT to do as what to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd501ff4-0eef-4d87-b63a-22d08a699748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== WELL-STRUCTURED FORMATTING ===\n",
      "Prompt: Task: Extract information from the following text and format it clearly.\n",
      "\n",
      "Text: \"John Smith, 35, Software Engineer, New York. Jane Doe, 28, Marketing Manager, California.\"\n",
      "\n",
      "Instructions:\n",
      "1. Extract each person's name and age\n",
      "2. Use the format: [Name] - Age: [age], Job: [job], Location: [location]\n",
      "3. Do NOT extract Job\n",
      "4. Do NOT extract Location\n",
      "\n",
      "Output:\n",
      "\n",
      "Response:  \n",
      "[Name] - Age: 35, Job: [NOT EXTRACTED], Location: [NOT EXTRACTED]\n",
      "[Name] - Age: 28, Job: [NOT EXTRACTED], Location: [NOT EXTRACTED] \n",
      "\n",
      "Note: Follow the instructions to extract the required information only. Do not include any additional information or formatting beyond what is specified. \n",
      "\n",
      "Output: \n",
      "John Smith - Age: 35, Job: [NOT EXTRACTED], Location: [NOT EXTRACTED]\n",
      "Jane Doe - Age: 28, Job: [NOT EXTRACTED], Location: [NOT EXTRACTED] \n",
      "\n",
      "**Note**: In the final output, replace \"[NOT EXTRACTED]\" with the actual job and location fields as per the instructions. However, since the instructions explicitly state not to extract job and location, the correct format should only include name and age. Here is the corrected output:\n",
      "\n",
      "John Smith - Age: 35\n",
      "Jane Doe - Age:\n"
     ]
    }
   ],
   "source": [
    "# Well-structured formatting\n",
    "print(\"=== WELL-STRUCTURED FORMATTING ===\")\n",
    "structured_neg_prompt = f\"\"\"Task: Extract information from the following text and format it clearly.\n",
    "\n",
    "Text: \"{data}\"\n",
    "\n",
    "Instructions:\n",
    "1. Extract each person's name and age\n",
    "2. Use the format: [Name] - Age: [age], Job: [job], Location: [location]\n",
    "3. Do NOT extract Job\n",
    "4. Do NOT extract Location\n",
    "\n",
    "Output:\"\"\"\n",
    "\n",
    "print(f\"Prompt: {structured_neg_prompt}\")\n",
    "print()\n",
    "\n",
    "structured_neg_response = llm(structured_neg_prompt, max_new_tokens=200, do_sample=True, top_k=20, temperature=0.5)[0][\"generated_text\"]\n",
    "structured_neg_response = structured_neg_response.replace(structured_neg_prompt, \"\")\n",
    "print(f\"Response: {structured_neg_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454fc503-50b3-44f4-a797-7deee40bfc4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmlesson",
   "language": "python",
   "name": "llmlesson"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
