{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16c922aa-b710-4a0c-8b5b-bde6e8310174",
   "metadata": {},
   "source": [
    "# Using LLMs for Text Classification\n",
    "Apart from being used as the basis for conversational agents and chat assistants for accomplishing every day tasks, LLMs can also be used for solving many of the traditional NLP tasks that we mentioned in Lesson 1. They thus can serve as an alternative to prior approaches such as supervised text classification models. In this section, we demonstrate example usage of LLMs for two text classification tasks: 1) sentiment classification of product or movie reviews and 2) topic classification of news article sentences. To highlight the important of evaluation, we also measure the performance of selected LLMs for this task on simple datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9666e2b0-2e68-4c8d-b1ed-d0c1984d8777",
   "metadata": {},
   "source": [
    "## 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "631acc64-c676-48e5-a1b5-53eb2ea62310",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f50c95-c31e-44a4-9582-0229a4341370",
   "metadata": {},
   "source": [
    "## 2. Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fef791d-9920-4644-967f-3e2e45dd72f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:04<00:00,  2.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded! It has 3,075,098,624 parameters\n"
     ]
    }
   ],
   "source": [
    "# Pick a model\n",
    "# model_id = \"HuggingFaceTB/SmolLM2-135M\" # base model\n",
    "# model_id = \"HuggingFaceTB/SmolLM2-135M-Instruct\" # fine-tuned assistant model\n",
    "# model_id = \"HuggingFaceTB/SmolLM3-3B-Base\" # base model\n",
    "model_id= \"HuggingFaceTB/SmolLM3-3B\" # fine-tuned assistant model\n",
    "# model_id = \"HuggingFaceTB/SmolLM2-1.7B-Instruct\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "# Check if model is loaded correctly\n",
    "print(f\"Model loaded! It has {model.num_parameters():,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62846e8-db2f-458f-82e7-d49bb1390611",
   "metadata": {},
   "source": [
    "## 3. Initialise inference pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edfaaa5a-c791-4e13-8682-58e27f038639",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "# Build pipeline\n",
    "llm = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4758d42-42bb-472a-8421-04156e62534a",
   "metadata": {},
   "source": [
    "## 4. Load classification dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d614e10d-1599-4d92-9e74-8e98b468062d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 20 sentences\n",
      "Sample text: The football team secured a last-minute victory in the championship game.\n",
      "Labels: {'sports', 'politics', 'entertainment', 'finance', 'science'}\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "data_file = \"topic_classification_dataset.csv\" # news topic classification\n",
    "# data_file = \"sentiment_classification_dataset.csv\" # sentiment classification\n",
    "texts_df = pd.read_csv(data_file)\n",
    "texts = texts_df['sentence'].tolist()\n",
    "ground_truth_labels = texts_df['ground_truth_label'].tolist()\n",
    "ground_truth_labels_unique = list(set(ground_truth_labels))\n",
    "print(f\"Dataset: {len(texts)} sentences\")\n",
    "print(f\"Sample text: {texts[0]}\")\n",
    "print(f\"Labels: {set(ground_truth_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c395b14e-651c-49b1-8d8e-9c49ab15b5d8",
   "metadata": {},
   "source": [
    "## 5. Setup classification code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7a0cea4-745e-46b9-b54f-2df9a96e7237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to do sentiment classification\n",
    "def classify_sentiment(text):\n",
    "    \"\"\"Classify text using the LLM.\"\"\"\n",
    "    prompt = f\"Consider the following sentence: '{text}'. Is this a Positive, Negative or Neutral statement?\" # sentiment classification\n",
    "    response = llm(prompt, max_new_tokens=50, do_sample=True, top_k=20, temperature=0.7)[0][\"generated_text\"]\n",
    "    response_clean = response.replace(prompt, \"\").lower()\n",
    "    \n",
    "    if \"positive\" in response_clean:\n",
    "        return \"positive\"\n",
    "    elif \"negative\" in response_clean:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "# Function to do news topic classification\n",
    "def classify_topic(text):\n",
    "    \"\"\"Classify text using the LLM.\"\"\"\n",
    "    prompt = f\"Consider the following sentence: '{text}'. Is this a Sports, Finance, Politics, Entertainment or Science statement?\" # news topic classification\n",
    "    response = llm(prompt, max_new_tokens=50, do_sample=True, top_k=20, temperature=0.7)[0][\"generated_text\"]\n",
    "    response_clean = response.replace(prompt, \"\").lower()\n",
    "\n",
    "    for i in range(0, len(ground_truth_labels_unique)):    \n",
    "        if ground_truth_labels_unique[i] in response_clean:\n",
    "            return ground_truth_labels_unique[i]\n",
    "    return 'sports'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb04b80-8bf2-47ab-9aba-34575c935740",
   "metadata": {},
   "source": [
    "## 6. Do classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a5df002-f5f9-4bba-a643-2bd9e7509bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1/20\n",
      "Processed 2/20\n",
      "Processed 3/20\n",
      "Processed 4/20\n",
      "Processed 5/20\n",
      "Processed 6/20\n",
      "Processed 7/20\n",
      "Processed 8/20\n",
      "Processed 9/20\n",
      "Processed 10/20\n",
      "Processed 11/20\n",
      "Processed 12/20\n",
      "Processed 13/20\n",
      "Processed 14/20\n",
      "Processed 15/20\n",
      "Processed 16/20\n",
      "Processed 17/20\n",
      "Processed 18/20\n",
      "Processed 19/20\n",
      "Processed 20/20\n",
      "Classifications complete!\n"
     ]
    }
   ],
   "source": [
    "# Classify all sentences\n",
    "predictions = []\n",
    "for i, text in enumerate(texts):\n",
    "    print(f\"Processed {i+1}/{len(texts)}\")\n",
    "\n",
    "    if 'sentiment' in data_file:\n",
    "        pred = classify_sentiment(text)\n",
    "    else:\n",
    "        pred = classify_topic(text)\n",
    "    predictions.append(pred)\n",
    "\n",
    "print(\"Classifications complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217893c3-70e9-4dfd-9cce-750b5ab61f9c",
   "metadata": {},
   "source": [
    "## 7. Display classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19b71939-65ff-4047-a39d-41a6ac7b1370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: ['sports', 'sports', 'sports', 'sports', 'sports', 'sports', 'sports', 'sports', 'sports', 'finance', 'finance', 'sports', 'sports', 'entertainment', 'sports', 'entertainment', 'sports', 'science', 'sports', 'science']\n",
      "Ground truth: ['sports', 'sports', 'sports', 'sports', 'politics', 'politics', 'politics', 'politics', 'finance', 'finance', 'finance', 'finance', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'science', 'science', 'science', 'science']\n"
     ]
    }
   ],
   "source": [
    "print(\"Predictions:\", predictions)\n",
    "print(\"Ground truth:\", ground_truth_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8c2b0e-1a8e-441a-af49-1274541e5c2f",
   "metadata": {},
   "source": [
    "## 8. Calculate evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2058b89-98d3-4de8-913f-98f47aacf86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "Accuracy: 0.500\n",
      "Precision: 0.657\n",
      "Recall: 0.500\n",
      "F1-Score: 0.489\n",
      "\n",
      "Detailed Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "entertainment       1.00      0.50      0.67         4\n",
      "      finance       1.00      0.50      0.67         4\n",
      "     politics       0.00      0.00      0.00         4\n",
      "      science       1.00      0.50      0.67         4\n",
      "       sports       0.29      1.00      0.44         4\n",
      "\n",
      "     accuracy                           0.50        20\n",
      "    macro avg       0.66      0.50      0.49        20\n",
      " weighted avg       0.66      0.50      0.49        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute evaluation metrics\n",
    "accuracy = accuracy_score(ground_truth_labels, predictions)\n",
    "precision, recall, f1, support = precision_recall_fscore_support(ground_truth_labels, predictions, average='weighted', zero_division=0)\n",
    "\n",
    "print(f\"\\nEvaluation Results:\")\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1-Score: {f1:.3f}\")\n",
    "\n",
    "print(f\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(ground_truth_labels, predictions, zero_division=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmlesson",
   "language": "python",
   "name": "llmlesson"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
