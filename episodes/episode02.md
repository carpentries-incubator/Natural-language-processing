---
title: "Episode 2"
teaching: 10
exercises: 2
---

:::::::::::::::::::::::::::::::::::::: questions 

- How to I preprocess my text data?
- What is a vector space?

::::::::::::::::::::::::::::::::::::::::::::::::

::::::::::::::::::::::::::::::::::::: objectives

- Explain how to use markdown with The Carpentries Workbench
- Demonstrate how to include pieces of code, figures, and nested challenge blocks

::::::::::::::::::::::::::::::::::::::::::::::::

> ## Learning Objectives








from doc:
> After following this lesson, learners will be able to:
> - Apply tokenization and lemmatization techniques on a specific test case
> - Clean and pre-process textual data (lower-case text, remove stop-words)
> - Recall what different preprocessing steps there are
> - Explain what a vector space is
> - Explain what the cosinee similarity is and compute it
> - Plot word embeddings
> - Explain what document embedding and TF-IDF is
> - Explain how a word2vec model works
> - Train a word2vec model
> - Explain the difference between GPT and BERT
> - use GPT2Tokenizer from the library transformers
> - Explain the difference between word token embeddings vs word position embeddings
