---
title: "Episode 2"
teaching: 
exercises: 
---

:::::::::::::::::::::::::::::::::::::: questions 

- How to I preprocess my text data?
- What is a vector space?

::::::::::::::::::::::::::::::::::::::::::::::::

::::::::::::::::::::::::::::::::::::: objectives

- Explain how to use markdown with The Carpentries Workbench
- Demonstrate how to include pieces of code, figures, and nested challenge blocks

::::::::::::::::::::::::::::::::::::::::::::::::
> ## Learning Objectives
> After following this lesson, learners will be able to:
> - Explain what preprocessing means.
> - Explain why we need preprocessing.
> - Do various preprocessing steps: lowercasing, handling new lines, tokenizing, stop word removal, parts of speach, stemming/lemmatizing







---------
----------

> - Apply tokenization and lemmatization techniques on a specific test case
> - Clean and pre-process textual data (lower-case text, remove stop-words)
> - Recall what different preprocessing steps there are
> - Explain what a vector space is
> - Explain what the cosinee similarity is and compute it
> - Plot word embeddings
> - Explain what document embedding and TF-IDF is
> - Explain how a word2vec model works
> - Train a word2vec model
> - Explain the difference between GPT and BERT
> - use GPT2Tokenizer from the library transformers
> - Explain the difference between word token embeddings vs word position embeddings
