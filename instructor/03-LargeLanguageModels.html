<!DOCTYPE html>
<!-- START: inst/pkgdown/templates/layout.html --><!-- Generated by pkgdown: do not edit by hand --><html lang="en" data-bs-theme="auto"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><title>Fundamentals of Natural Language Processing (NLP) in Python: Episode 3: Using large language models</title><meta name="viewport" content="width=device-width, initial-scale=1"><script src="../assets/themetoggle.js"></script><link rel="stylesheet" type="text/css" href="../assets/styles.css"><script src="../assets/scripts.js" type="text/javascript"></script><!-- mathjax --><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      config: ["MMLorHTML.js"],
      jax: ["input/TeX","input/MathML","output/HTML-CSS","output/NativeMML", "output/PreviewHTML"],
      extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
      TeX: {
        extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
      },
      tex2jax: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [ ['$$','$$'], ['\\[', '\\]'] ],
        processEscapes: true
      }
    });
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><!-- Responsive Favicon for The Carpentries --><link rel="apple-touch-icon" sizes="180x180" href="../favicons/incubator/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="../favicons/incubator/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="../favicons/incubator/favicon-16x16.png"><link rel="manifest" href="../favicons/incubator/site.webmanifest"><link rel="mask-icon" href="../favicons/incubator/safari-pinned-tab.svg" color="#5bbad5"><meta name="msapplication-TileColor" content="#da532c"><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"><meta name="theme-color" media="(prefers-color-scheme: dark)" content="black"></head><body>
    <header id="top" class="navbar navbar-expand-md top-nav incubator"><svg xmlns="http://www.w3.org/2000/svg" class="d-none"><symbol id="check2" viewbox="0 0 16 16"><path d="M13.854 3.646a.5.5 0 0 1 0 .708l-7 7a.5.5 0 0 1-.708 0l-3.5-3.5a.5.5 0 1 1 .708-.708L6.5 10.293l6.646-6.647a.5.5 0 0 1 .708 0z"></path></symbol><symbol id="circle-half" viewbox="0 0 16 16"><path d="M8 15A7 7 0 1 0 8 1v14zm0 1A8 8 0 1 1 8 0a8 8 0 0 1 0 16z"></path></symbol><symbol id="moon-stars-fill" viewbox="0 0 16 16"><path d="M6 .278a.768.768 0 0 1 .08.858 7.208 7.208 0 0 0-.878 3.46c0 4.021 3.278 7.277 7.318 7.277.527 0 1.04-.055 1.533-.16a.787.787 0 0 1 .81.316.733.733 0 0 1-.031.893A8.349 8.349 0 0 1 8.344 16C3.734 16 0 12.286 0 7.71 0 4.266 2.114 1.312 5.124.06A.752.752 0 0 1 6 .278z"></path><path d="M10.794 3.148a.217.217 0 0 1 .412 0l.387 1.162c.173.518.579.924 1.097 1.097l1.162.387a.217.217 0 0 1 0 .412l-1.162.387a1.734 1.734 0 0 0-1.097 1.097l-.387 1.162a.217.217 0 0 1-.412 0l-.387-1.162A1.734 1.734 0 0 0 9.31 6.593l-1.162-.387a.217.217 0 0 1 0-.412l1.162-.387a1.734 1.734 0 0 0 1.097-1.097l.387-1.162zM13.863.099a.145.145 0 0 1 .274 0l.258.774c.115.346.386.617.732.732l.774.258a.145.145 0 0 1 0 .274l-.774.258a1.156 1.156 0 0 0-.732.732l-.258.774a.145.145 0 0 1-.274 0l-.258-.774a1.156 1.156 0 0 0-.732-.732l-.774-.258a.145.145 0 0 1 0-.274l.774-.258c.346-.115.617-.386.732-.732L13.863.1z"></path></symbol><symbol id="sun-fill" viewbox="0 0 16 16"><path d="M8 12a4 4 0 1 0 0-8 4 4 0 0 0 0 8zM8 0a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 0zm0 13a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 13zm8-5a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2a.5.5 0 0 1 .5.5zM3 8a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2A.5.5 0 0 1 3 8zm10.657-5.657a.5.5 0 0 1 0 .707l-1.414 1.415a.5.5 0 1 1-.707-.708l1.414-1.414a.5.5 0 0 1 .707 0zm-9.193 9.193a.5.5 0 0 1 0 .707L3.05 13.657a.5.5 0 0 1-.707-.707l1.414-1.414a.5.5 0 0 1 .707 0zm9.193 2.121a.5.5 0 0 1-.707 0l-1.414-1.414a.5.5 0 0 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .707zM4.464 4.465a.5.5 0 0 1-.707 0L2.343 3.05a.5.5 0 1 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .708z"></path></symbol></svg><a class="visually-hidden-focusable skip-link" href="#main-content">Skip to main content</a>
  <div class="container-fluid top-nav-container">
    <div class="col-md-8">
      <div class="large-logo">
        <img id="incubator-logo" alt="Carpentries Incubator" src="../assets/images/incubator-logo.svg"><span class="badge text-bg-danger">
          <abbr title="This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught.">
            <a href="https://docs.carpentries.org/resources/curriculum/lesson-life-cycle.html" class="external-link alert-link">
              <i aria-hidden="true" class="icon" data-feather="alert-octagon" style="border-radius: 5px"></i>
              Pre-Alpha
            </a>
            <span class="visually-hidden">This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught.</span>
          </abbr>
        </span>

      </div>
    </div>
    <div class="selector-container">
      <div id="theme-selector">
        <li class="nav-item dropdown" id="theme-button-list">
          <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
            <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="bd-theme-text"><li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                Light
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                Dark
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                Auto
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
          </ul></li>
      </div>

      <div class="dropdown" id="instructor-dropdown">
        <button class="btn btn-secondary dropdown-toggle bordered-button" type="button" id="dropdownMenu1" data-bs-toggle="dropdown" aria-expanded="false">
          <i aria-hidden="true" class="icon" data-feather="eye"></i> Instructor View <i data-feather="chevron-down"></i>
        </button>
        <ul class="dropdown-menu" aria-labelledby="dropdownMenu1"><li><button class="dropdown-item" type="button" onclick="window.location.href='../03-LargeLanguageModels.html';">Learner View</button></li>
        </ul></div>
    </div>
  </div>
  <hr></header><nav class="navbar navbar-expand-xl bottom-nav incubator" aria-label="Main Navigation"><div class="container-fluid nav-container">
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle Navigation">
      <span class="navbar-toggler-icon"></span>
      <span class="menu-title">Menu</span>
    </button>
    <div class="nav-logo">
      <img class="small-logo" alt="Carpentries Incubator" src="../assets/images/incubator-logo-sm.svg"></div>
    <div class="lesson-title-md">
      Fundamentals of Natural Language Processing (NLP) in Python
    </div>
    <div class="search-icon-sm">
      <!-- TODO: do not show until we have search
        <i role="img" aria-label="Search the All In One page" data-feather="search"></i>
      -->
    </div>
    <div class="desktop-nav">
      <ul class="navbar-nav me-auto mb-2 mb-lg-0"><li class="nav-item">
          <span class="lesson-title">
            Fundamentals of Natural Language Processing (NLP) in Python
          </span>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/key-points.html">Key Points</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/instructor-notes.html">Instructor Notes</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/images.html">Extract All Images</a>
        </li>
        <li class="nav-item dropdown">
          <button class="nav-link dropdown-toggle" id="navbarDropdown" data-bs-toggle="dropdown" aria-expanded="false">
            More <i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu" aria-labelledby="navbarDropdown"><hr></ul></li>
      </ul></div>
    <!--
    <form class="d-flex col-md-2 search-form">
      <fieldset disabled>
      <input class="form-control me-2 searchbox" type="search" placeholder="" aria-label="">
        <button class="btn btn-outline-success tablet-search-button"  type="submit">
          <i class="search-icon" data-feather="search" role="img" aria-label="Search the All In One page"></i>
        </button>
      </fieldset>
    </form>
    -->
    <a id="search-button" class="btn btn-primary" href="../instructor/aio.html" role="button" aria-label="Search the All In One page">Search the All In One page</a>
  </div><!--/div.container-fluid -->
</nav><div class="col-md-12 mobile-title">
  Fundamentals of Natural Language Processing (NLP) in Python
</div>

<aside class="col-md-12 lesson-progress"><div style="width: 97%" class="percentage">
    97%
  </div>
  <div class="progress incubator">
    <div class="progress-bar incubator" role="progressbar" style="width: 97%" aria-valuenow="97" aria-label="Lesson Progress" aria-valuemin="0" aria-valuemax="100">
    </div>
  </div>
</aside><div class="container">
      <div class="row">
        <!-- START: inst/pkgdown/templates/navbar.html -->
<div id="sidebar-col" class="col-lg-4">
  <div id="sidebar" class="sidebar">
      <nav aria-labelledby="flush-headingEleven"><button role="button" aria-label="close menu" alt="close menu" aria-expanded="true" aria-controls="sidebar" class="collapse-toggle" data-collapse="Collapse " data-episodes="Episodes ">
          <i class="search-icon" data-feather="x" role="img"></i>
        </button>
        <div class="sidebar-inner">
          <div class="row mobile-row" id="theme-row-mobile">
            <div class="col" id="theme-selector">
              <li class="nav-item dropdown" id="theme-button-list">
                <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
                  <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><span class="d-lg-none ms-1" id="bd-theme-text">Toggle Theme</span>
                </button>
                <ul class="dropdown-menu dropdown-menu-right" aria-labelledby="bd-theme-text"><li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                      Light
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                      Dark
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                      <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                      Auto
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                </ul></li>
            </div>
          </div>
          <div class="row mobile-row">
            <div class="col">
              <div class="sidenav-view-selector">
                <div class="accordion accordion-flush" id="accordionFlush9">
                  <div class="accordion-item">
                    <h2 class="accordion-header" id="flush-headingNine">
                      <button class="accordion-button collapsed" id="instructor" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseNine" aria-expanded="false" aria-controls="flush-collapseNine">
                        <i id="eye" aria-hidden="true" class="icon" data-feather="eye"></i> Instructor View
                      </button>
                    </h2>
                    <div id="flush-collapseNine" class="accordion-collapse collapse" aria-labelledby="flush-headingNine" data-bs-parent="#accordionFlush2">
                      <div class="accordion-body">
                        <a href="../03-LargeLanguageModels.html">Learner View</a>
                      </div>
                    </div>
                  </div><!--/div.accordion-item-->
                </div><!--/div.accordion-flush-->
              </div><!--div.sidenav-view-selector -->
            </div><!--/div.col -->

            <hr></div><!--/div.mobile-row -->

          <div class="accordion accordion-flush" id="accordionFlush11">
            <div class="accordion-item">

              <button id="chapters" class="accordion-button show" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseEleven" aria-expanded="false" aria-controls="flush-collapseEleven">
                <h2 class="accordion-header chapters" id="flush-headingEleven">
                  EPISODES
                </h2>
              </button>
              <div id="flush-collapseEleven" class="accordion-collapse show collapse" aria-labelledby="flush-headingEleven" data-bs-parent="#accordionFlush11">

                <div class="accordion-body">
                  <div class="accordion accordion-flush" id="accordionFlush1">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading1">
        <a href="index.html">Summary and Schedule</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush2">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading2">
        <a href="00-introduction.html">1. Introduction</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush3">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading3">
        <a href="01-preprocessing.html">2. Episode 1: From text to vectors</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush4">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading4">
        <a href="02-transformers.html">3. Episode 2: BERT and Transformers</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlushcurrent">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-headingcurrent">
      <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapsecurrent" aria-expanded="true" aria-controls="flush-collapsecurrent">
        <span class="visually-hidden">Current Chapter</span>
        <span class="current-chapter">
        4. Episode 3: Using large language models
        </span>
      </button>
    </div><!--/div.accordion-header-->

    <div id="flush-collapsecurrent" class="accordion-collapse collapse show" aria-labelledby="flush-headingcurrent" data-bs-parent="#accordionFlushcurrent">
      <div class="accordion-body">
        <ul><li><a href="#what-are-large-language-models">What are Large Language Models?</a></li>
<li><a href="#how-are-llms-different-from-other-nlp-techniques">How are LLMs different from other NLP techniques?</a></li>
<li><a href="#what-llms-are-good-at">What LLMs are good at</a></li>
<li><a href="#what-llms-struggle-with">What LLMs struggle with</a></li>
<li><a href="#how-do-llms-work">How do LLMs work?</a></li>
<li><a href="#a-zoo-of-large-language-models">A zoo of Large Language Models</a></li>
<li><a href="#which-one-to-chose-when">Which one to chose when?</a></li>
<li><a href="#building-a-chatbot">Building a chatbot</a></li>
<li><a href="#retrieval-augmented-generation---build-a-rag">Retrieval Augmented Generation - Build a RAG</a></li>
        </ul></div><!--/div.accordion-body-->
    </div><!--/div.accordion-collapse-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

                </div>
              </div>
            </div>

            <hr class="half-width"><div class="accordion accordion-flush lesson-resources" id="accordionFlush12">
              <div class="accordion-item">
                <h2 class="accordion-header" id="flush-headingTwelve">
                  <button class="accordion-button collapsed" id="lesson-resources" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseTwelve" aria-expanded="false" aria-controls="flush-collapseTwelve">
                    RESOURCES
                  </button>
                </h2>
                <div id="flush-collapseTwelve" class="accordion-collapse collapse" aria-labelledby="flush-headingTwelve" data-bs-parent="#accordionFlush12">
                  <div class="accordion-body">
                    <ul><li>
                        <a href="../instructor/key-points.html">Key Points</a>
                      </li>
                      <li>
                        <a href="../instructor/instructor-notes.html">Instructor Notes</a>
                      </li>
                      <li>
                        <a href="../instructor/images.html">Extract All Images</a>
                      </li>
                      <hr></ul></div>
                </div>
              </div>
            </div>
            <hr class="half-width lesson-resources"><a href="../instructor/aio.html">See all in one page</a>


            <hr class="d-none d-sm-block d-md-none"><div class="d-grid gap-1">

            </div>
          </div><!-- /div.accordion -->
        </div><!-- /div.sidebar-inner -->
      </nav></div><!-- /div.sidebar -->
  </div><!-- /div.sidebar-col -->
<!-- END:   inst/pkgdown/templates/navbar.html-->

        <!-- START: inst/pkgdown/templates/content-instructor.html -->
  <div class="col-xl-8 col-lg-12 primary-content">
    <nav class="lesson-content mx-md-4" aria-label="Previous and Next Chapter"><!-- content for small screens --><div class="d-block d-sm-block d-md-none">
        <a class="chapter-link" href="../instructor/02-transformers.html"><i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>Previous</a>
        <a class="chapter-link float-end" href="../instructor/index.html">Next<i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i></a>
      </div>
      <!-- content for large screens -->
      <div class="d-none d-sm-none d-md-block">
        <a class="chapter-link" href="../instructor/02-transformers.html" rel="prev">
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>
          Previous: Episode 2: BERT and
        </a>
        <a class="chapter-link float-end" href="../instructor/index.html" rel="next">
          Home
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i>
        </a>
      </div>
      <hr></nav><main id="main-content" class="main-content"><div class="container lesson-content">
        <h1>Episode 3: Using large language models</h1>
        <p>Last updated on 2025-08-19 |

        <a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/edit/main/episodes/03-LargeLanguageModels.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>





        <div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>

        

<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul><li>What is a Large Language Model (LLM)?</li>
<li>How do LLMs differ from traditional NLP models?</li>
<li>What is the Transformer architecture, and why is it important for
LLMs?</li>
<li>How does prompt engineering influence LLM outputs?</li>
<li>What are some real-world applications of LLMs?</li>
<li>How can LLMs generate, classify, and summarise text?</li>
</ul></div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<p>After following this lesson, learners will be able to:</p>
<ul><li>Understand what Large Language Models (LLMs) are and their role in
NLP</li>
<li>Explain the Transformer architecture and why it is foundational for
LLMs</li>
<li>Use prompt engineering to generate high-quality text responses</li>
<li>Apply LLMs to real-world tasks like news article analysis</li>
<li>Explore pre-trained LLMs and experiment with custom tasks</li>
<li>Set up a chatbot and a simple RAG</li>
<li>Understand the impact of LLMs in modern AI and language
processing</li>
</ul></div>
</div>
</div>
</div>
</div>
<p>Large Language Models (LLMs) are a hot and a big topic these days,
and are continuously in the news. Everybody heard of ChatGPT, many tried
it out for a variety of purposes, or even incorporated these tools in
their daily work. But what are these models exactly, how do they work
‘under the hood’, and how can you make use of them in the best possible
way?</p>
<p>In this episode, we will:</p>
<ul><li>Explore LLMs, which represent a significant advancement in natural
language processing (NLP). We will begin by defining what LLMs are and
touching on their foundational architecture, particularly the
Transformer model, which allows the LLM to understand and generate
human-like text.</li>
<li>Through practical examples, you will discover how to work with these
models for specific tasks with content generation.</li>
<li>We’ll also discuss real-world applications of LLMs, such as
analyzing news articles and generating insights from large amounts of
text data.</li>
</ul><p>This episode aims to equip you with both theoretical knowledge and
practical skills, preparing you to harness the power of LLMs in your own
projects and applications.</p>
<section><h2 class="section-heading" id="what-are-large-language-models">What are Large Language Models?<a class="anchor" aria-label="anchor" href="#what-are-large-language-models"></a></h2>
<hr class="half-width"><p>Large language models (LLMs) represent a significant advancement in
artificial intelligence, designed to process and interpret large-scale
natural language data to generate responses to user queries. By being
trained on extensive datasets through advanced machine learning
algorithms, these models learn the intricate patterns, structures and
nuances of human language. This enables them to produce coherent and
natural-sounding language outputs across various inputs. As a result,
large language models are becoming increasingly essential in a range of
tasks such as text generation, text summarisation, rewriting, question
answering, and language translation.</p>
<p>The emergence of <em>ChatGPT</em>, powered by OpenAI’s advanced LLMs,
has brought these capabilities into the mainstream. With ChatGPT, users
interact through natural language, enabling seamless conversations and
performing complex tasks across various sectors, like customer service,
education, and content creation. Models like GPT-4, BERT, and LLaMA are
also used across various applications; from chatbots, virtual
assistants, text analysis, translation, summarisation, and more.</p>
<p>Notably, the success of ChatGPT and other LLM-driven applications
highlights their versatility and potential to transform how humans
interact with digital systems. These models continue to push the
boundaries of what’s possible in human-computer interaction, offering a
glimpse into a future where machines and humans communicate more
naturally and effectively.</p>
<div class="section level3">
<h3 id="timeline">Timeline<a class="anchor" aria-label="anchor" href="#timeline"></a></h3>
<p>The journey of large language models over the past decade highlights
the rapid evolution and growing impact of AI in language processing:</p>
<ul><li><p><strong>2012</strong>: Significant advancements in deep learning
architectures and access to larger datasets lead to the creation of the
GPT (Generative Pre-trained Transformer) framework, marking a pivotal
point in AI development.</p></li>
<li><p><strong>2018</strong>: Google unveils BERT (Bidirectional Encoder
Representations from Transformers), introducing a groundbreaking
architecture that sets the stage for the next generation of large
language models.</p></li>
<li><p><strong>2020</strong>: OpenAI releases GPT-3, which, with its
remarkable 175 billion parameters, establishes a new standard for
performance in language-related tasks.</p></li>
<li><p><strong>2022</strong>: The launch of ChatGPT marks a turning
point by making GPT-3 and similar models easily accessible to the public
via a web interface, significantly raising public awareness and interest
in LLMs and generative AI.</p></li>
<li><p><strong>2023</strong>: Open-source LLMs begin to deliver
impressive outcomes with the introduction of models like Dolly 2.0,
LLaMA, Alpaca, and Vicuna. Meanwhile, the release of GPT-4 pushes the
envelope by setting new records for both parameter scale and overall
performance.</p></li>
</ul></div>
<div class="section level3">
<h3 id="starting-up-a-simple-chat">Starting up a simple chat<a class="anchor" aria-label="anchor" href="#starting-up-a-simple-chat"></a></h3>
<p>Let’s explore how these LLMs work by doing a simple chat
demonstration.</p>
<p>Imagine asking a computer a question and getting a human-like
response. An LLM, like GPT-4, can generate responses that feel natural,
informative, and tailored to your question by analyzing the context and
meaning of your input. In the demonstration below, you’ll see how an LLM
responds to a prompt and how it builds upon previous information to
maintain the conversation.</p>
</div>
<div class="section level3">
<h3 id="example-chat">Example Chat<a class="anchor" aria-label="anchor" href="#example-chat"></a></h3>
<ul><li><p><em>User</em>: “What’s the tallest mountain in the
world?”</p></li>
<li><p><em>LLM</em>: “The tallest mountain in the world is Mount
Everest, which stands at 8,848 meters (29,029 feet).”</p></li>
<li><p><em>User</em>: “How long does it take to climb it?”</p></li>
<li><p><em>LLM</em>: “Climbing Mount Everest typically takes around two
months, due to the acclimatization process needed to adjust to high
altitudes. Expeditions often start from Base Camp in Nepal, where
climbers spend several weeks preparing.”</p></li>
<li><p><em>User</em>: “Could someone with no climbing experience try
it?”</p></li>
<li><p><em>LLM</em>: “Climbing Mount Everest is extremely challenging
and requires significant training and experience due to the harsh
conditions, high altitudes, and technical aspects of the climb. Most
climbers spend years training and gaining experience on other mountains
before attempting Everest.”</p></li>
</ul><p>This example shows how LLMs can maintain context across multiple
questions. Notice how the model connects information across each
question, keeping the conversation relevant and informed.</p>
<p>This quick chat example introduces you to the conversational
abilities of LLMs, demonstrating their ability to respond contextually,
provide coherent answers, and follow up on earlier parts of the
conversation.</p>
</div>
</section><section><h2 class="section-heading" id="how-are-llms-different-from-other-nlp-techniques">How are LLMs different from other NLP techniques?<a class="anchor" aria-label="anchor" href="#how-are-llms-different-from-other-nlp-techniques"></a></h2>
<hr class="half-width"><p>LLMs stand apart from traditional NLP techniques due to their scale,
versatility, and reliance on deep learning models, especially
Transformers. Here’s a breakdown of how they differ:</p>
<div class="section level3">
<h3 id="scale-of-training-data-and-parameters">Scale of training data and parameters<a class="anchor" aria-label="anchor" href="#scale-of-training-data-and-parameters"></a></h3>
<ul><li>LLMs: Trained on massive datasets (trillions of words) and use
billions of parameters, allowing them to learn language patterns, facts,
and relationships at an unprecedented depth.</li>
<li>Traditional NLP: Techniques like rule-based systems or smaller
machine learning models rely on much smaller datasets, often requiring
domain-specific training for each task (e.g., sentiment analysis or
named entity recognition).</li>
</ul></div>
<div class="section level3">
<h3 id="model-architecture">Model architecture<a class="anchor" aria-label="anchor" href="#model-architecture"></a></h3>
<ul><li>LLMs: Use the Transformer architecture, particularly self-attention,
to analyze relationships between words regardless of position. This
allows them to capture long-range dependencies and context better than
traditional models.</li>
<li>Traditional NLP: Often use simpler models like bag-of-words, TF-IDF
(term frequency-inverse document frequency), RNNs (recurrent neural
networks), and LSTMs (long-short-term memory models), which treat words
independently or consider only local context, missing the complex,
global relationships.</li>
</ul></div>
<div class="section level3">
<h3 id="generalization-vs--task-specific-models">Generalization vs. task-specific models<a class="anchor" aria-label="anchor" href="#generalization-vs--task-specific-models"></a></h3>
<ul><li>LLMs: Can be applied across a wide range of tasks—summarization,
translation, question answering, etc.—without the need for separate
models for each. Fine-tuning makes them even more adaptable to specific
needs.</li>
<li>Traditional NLP: Typically requires developing or training a
separate model for each task. For example, separate models for sentiment
analysis, translation, and entity recognition.</li>
</ul></div>
<div class="section level3">
<h3 id="learning-from-unlabeled-data">Learning from unlabeled data<a class="anchor" aria-label="anchor" href="#learning-from-unlabeled-data"></a></h3>
<ul><li>LLMs: Leverage unsupervised or self-supervised learning during
pretraining, enabling them to learn language patterns from raw text
without human-labeled data.</li>
<li>Traditional NLP: are often supervised models, relying on labeled
data for training (e.g., labeled sentiment or part-of-speech tags),
which can be costly and time-consuming to create at scale.</li>
</ul></div>
<div class="section level3">
<h3 id="context-and-language-nuance">Context and language nuance<a class="anchor" aria-label="anchor" href="#context-and-language-nuance"></a></h3>
<ul><li>LLMs: Excel at understanding context, tone, and nuance, due to their
ability to weigh word relationships dynamically. This enables better
handling of idioms, sarcasm, and ambiguous phrases.</li>
<li>Traditional NLP: Struggles with complex language nuances, often
producing more rigid or literal interpretations. Contextual
understanding is limited, especially for longer sentences or
paragraphs.</li>
</ul></div>
<div class="section level3">
<h3 id="adaptability-and-fine-tuning">Adaptability and fine-tuning<a class="anchor" aria-label="anchor" href="#adaptability-and-fine-tuning"></a></h3>
<ul><li>LLMs: Easily adaptable to new tasks or domains with fine-tuning,
making them versatile across different applications.</li>
<li>Traditional NLP: Less flexible, often requiring retraining from
scratch or heavy feature engineering to adapt to new domains or
tasks.</li>
</ul><p>In short, LLMs represent a leap forward by combining scale,
flexibility, and deep learning power, allowing for more accurate,
nuanced, and versatile language processing compared to traditional NLP
techniques.</p>
</div>
</section><section><h2 class="section-heading" id="what-llms-are-good-at">What LLMs are good at<a class="anchor" aria-label="anchor" href="#what-llms-are-good-at"></a></h2>
<hr class="half-width"><ul><li>
<em>Language generation</em>: Creating coherent and contextually
appropriate text, making them ideal for creative writing, chatbots, and
automated responses.</li>
<li>
<em>Summarization and translation</em>: Quickly summarizing
articles, books, and translating text between languages with reasonable
accuracy.</li>
<li>
<em>Information retrieval and answering questions</em>: LLMs can
recall and apply general knowledge from their training data to answer
questions, though they don’t actually “know” facts.</li>
<li>
<em>Sentiment and text classification</em>: LLMs can classify text
for tasks like sentiment analysis, spam detection, and topic
categorization.</li>
</ul></section><section><h2 class="section-heading" id="what-llms-struggle-with">What LLMs struggle with<a class="anchor" aria-label="anchor" href="#what-llms-struggle-with"></a></h2>
<hr class="half-width"><ul><li>
<em>Fact-based accuracy</em>: Since LLMs don’t “know” facts, they
may generate incorrect or outdated information and are prone to
hallucinations (making up facts).</li>
<li>
<em>Understanding context over long passages</em>: LLMs can struggle
with context over very long texts and may lose track of earlier details,
affecting coherence.</li>
<li>
<em>Mathematical reasoning and logic</em>: Though improving, LLMs
often find complex problem-solving and detailed logical reasoning
challenging without direct guidance.</li>
<li>
<em>Ethical and sensitive issues</em>: LLMs may produce biased or
offensive text based on biases present in the training data, making
content moderation necessary in sensitive applications.</li>
</ul></section><section><h2 class="section-heading" id="how-do-llms-work">How do LLMs work?<a class="anchor" aria-label="anchor" href="#how-do-llms-work"></a></h2>
<hr class="half-width"><p>So, how is it that you can chat with a model and receive responses
that seem almost human? The answer lies in the architecture and training
of Large Language Models (LLMs), which are powered by advanced neural
networks that understand, generate, and even translate human language
with surprising accuracy.</p>
<p>At the core of LLMs lies a framework known as the
<strong>transformer</strong>; a concept already encountered in the
previous episode. Transformers allow these models to process vast
amounts of text and learn the structure and nuances of language. This
setup enables LLMs not only to answer questions but also to predict,
complete, and even generate coherent text based on the patterns they’ve
learned.</p>
<p>LLMs are trained on large text datasets and later fine-tuned on
specific tasks, which helps them adapt to a wide range of applications,
from conversation to text classification. The result? A model that can
chat, summarize, translate, and much more—all by leveraging these core
mechanisms. LLM’s rely on the following key concepts:</p>
<ul><li>
<em>Transformers and self-attention</em>: The transformer
architecture, especially the self-attention mechanism, is at the heart
of LLMs. Self-attention enables these models to understand the
importance of each word in relation to others in a sequence, regardless
of their position.</li>
<li>
<em>Pretraining and fine-tuning</em>: LLMs are first pre-trained on
large text datasets using tasks like predicting the next word in a
sentence, learning language patterns. They are then fine-tuned on
specific tasks (e.g., translation, summarization) to enhance performance
for targeted applications.</li>
<li>
<em>Generative vs. discriminative models</em>: LLMs can be applied
to both generative tasks (e.g., text generation) and discriminative
tasks (e.g., classification).</li>
</ul><p>In practice, this attention mechanism helps LLMs produce coherent
responses by establishing relationships between words as each new token
is generated. Here’s how it works:</p>
<ul><li><p><em>Understanding word relationships</em>. Self-attention enables
the model to weigh the importance of each word in a sentence, no matter
where it appears, to make sense of the sentence as a whole.</p></li>
<li><p><em>Predicting next words based on context</em>. With these
relationships mapped out, the model can predict the next word in a
sequence. For example, in “The fox,” self-attention allows the model to
anticipate that “jumps” or “runs” might come next rather than something
unrelated like “table.”</p></li>
<li><p><em>Structuring responses</em>. As each word is generated, the
model assesses how each new token impacts the entire sentence, ensuring
that responses are relevant, logically sound, and grammatically correct.
This ability to “structure” language is why LLMs can produce responses
that are contextually meaningful and well-organized.</p></li>
</ul></section><section><h2 class="section-heading" id="a-zoo-of-large-language-models">A zoo of Large Language Models<a class="anchor" aria-label="anchor" href="#a-zoo-of-large-language-models"></a></h2>
<hr class="half-width"><p>The era of Large Language Models gained momentum in 2018 with the
release of Google’s BERT. Since then, many companies have rapidly
developed newer and more powerful models. Among these are GPT (OpenAI),
Llama (Meta), Mistral (Mistral AI), Gemini (Google DeepMind), Claude
(Anthropic), and Grok (xAI). Some are open-source or more transparent
than others, revealing their architectures, parameter counts, or
training data and the collection thereof.</p>
<p>A list of LLMs: - GPT - OpenAI - Llama - Meta - Mistral / Mixtral -
Mistral AI (founded by former engineers from Google DeepMind and Meta) -
Gemini - Google DeepMind - Claude - Anthropic - (founded by former
OpenAI employees) - Grok - xAI (Elon Musk)</p>
<p>Training a large language model is extremely resource intensive. For
example, llama’s model Llama 3.1 405B is a model that has 405 billion
parameters. It was trained on 15 trillion tokens, uses 31 million GPU
hours (H100 gpus), and emitted almost 9000 tons of CO_2 (for the
training process only).</p>
<p>Inference also consumes considerable resources and has a significant
environmental impact. Large models require large memory for storing and
loading the model weights (storing weights alone can require
<em>hundreds</em> of gigabytes), and need high-performance GPUs to
achieve reasonable runtimes. As a result, many models operate on
cloud-based servers, increasing power consumption, especially when
scaled accomodate large numbers of users.</p>
</section><section><h2 class="section-heading" id="which-one-to-chose-when">Which one to chose when?<a class="anchor" aria-label="anchor" href="#which-one-to-chose-when"></a></h2>
<hr class="half-width"><p>With so many available models the question arises “which model you
should use when”? One thing to consider here is whether you want to use
an open source model or not. But another important aspect is that it
depends on the task at hand. There are various leaderboards (for
example: <a href="https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/,%20%5BHELM%5D(https://crfm.stanford.edu/helm/lite/latest/)" class="external-link">HuggingFace</a>
that track which tasks specific models are good at, based on widely used
benchmarks. Also, which language are you using? Most models are fully
trained on English, not many models are trained on Dutch text. So if you
are using Dutch texts, you may want to look for a model that is trained
on or finetuned for Dutch. Additionally, some LLMs are multimodal
models, meaning they can process various forms of input; text, images,
timeseries, audio, videos and so on.</p>
</section><section><h2 class="section-heading" id="building-a-chatbot">Building a chatbot<a class="anchor" aria-label="anchor" href="#building-a-chatbot"></a></h2>
<hr class="half-width"><p>It is time to start using an LLM! We are not going to train our own
LLM, but use Meta’s open source Llama model to set up a chatbot.</p>
<div class="section level4">
<h4 id="starting-ollama">Starting Ollama<a class="anchor" aria-label="anchor" href="#starting-ollama"></a></h4>
<p>Ollama is a platform that allows users to run various LLM locally on
your own computer. This is different from for example using chatgpt,
where you log in and use the online api. ChatGPT collects the input you
are providing and uses this to their own benefit. Running an LLM locally
using Ollama thus preserves your privacy. It also allows you to
customize a model, by setting certain parameters, or even by finetuning
a model.</p>
<p>To start Ollama:</p>
<pre><code>ollama serve</code></pre>
<p>Next, download the large language model to be used. In this case use
the smallest open source llama model, which is llama3.1:8b. Here 3.1 is
the version of the model and 8b stands for the number of parameters that
the model has.</p>
<pre><code>!ollama pull llama3.1:8b</code></pre>
<p>In general, a bigger version of the same model (such as Llama3.1:70b)
is better in accuracy, but since it is larger it takes more resources to
run and can hence be too much for a laptop.</p>
<p>Import the packages that will be used:</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="im">from</span> langchain_ollama <span class="im">import</span> ChatOllama</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a><span class="im">from</span> langchain_core.messages <span class="im">import</span> HumanMessage, SystemMessage</span></code></pre>
</div>
</div>
<div class="section level3">
<h3 id="create-a-model-instance">Create a model instance<a class="anchor" aria-label="anchor" href="#create-a-model-instance"></a></h3>
<p>Here, <code>model</code> defines the LLM to be used, which is set to
the model just downloaded, and <code>temperature</code> sets the
randomness of the mode, using the value zero ensures that repeating a
question will give the same model output (answer).</p>
<pre><code><span><span class="va">llm</span> <span class="op">=</span> <span class="fu">ChatOllama</span><span class="op">(</span>model<span class="op">=</span><span class="st">"llama3.1:8b"</span>, temperature<span class="op">=</span><span class="fl">0</span><span class="op">)</span></span></code></pre>
<p>Now that the model is set up, it can be invoked - ask it a
question.</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a>question <span class="op">=</span> <span class="st">"When was the moon landing?"</span></span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>chatresult <span class="op">=</span> llm.invoke([HumanMessage(content<span class="op">=</span>question)])</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a><span class="bu">print</span>(chatresult.content)</span></code></pre>
</div>
<div id="challenge1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div class="callout-inner">
<h3 class="callout-title">Challenge</h3>
<div class="callout-content">
<p>Play around with the chat bot by changing the questions. - How is the
quality of the answers? - Is it able to answer general questions, and
very specific questions? - Which limitations can you identify? - How
could you get better answers?</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">

</div>
</div>
</div>
</div>
<div id="discussion1" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<span class="callout-header">Discussion</span>
<div class="callout-inner">
<h3 class="callout-title">Challenge<em> (continued)</em>
</h3>
<div class="callout-content">
<p>This Llama chat bot, just like ChatGPT, is quite generic. It is good
at answering general questions; things that a lot of people know. Going
deeper and asking very specific questions often leads to vague or
inaccurate results.</p>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="use-context">Use context<a class="anchor" aria-label="anchor" href="#use-context"></a></h3>
<p>To improve on what to expect the LLM to return, it is also possible
to provide it with some context. For example, add:</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a>context <span class="op">=</span> <span class="st">"You are a highschool history teacher trying to explain societal impact of historic events."</span></span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>messages <span class="op">=</span> [</span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a>    SystemMessage(content<span class="op">=</span>context),</span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a>    HumanMessage(content<span class="op">=</span>question),</span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a>]</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a>chatresult <span class="op">=</span> llm.invoke(messages)</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a><span class="bu">print</span>(chatresult.content)</span></code></pre>
</div>
<p>The benefit here is that your answer will be phrased in a way that
fits your context, without having to specify this for every
question.</p>
</div>
<div class="section level3">
<h3 id="use-the-chat-history">Use the chat history<a class="anchor" aria-label="anchor" href="#use-the-chat-history"></a></h3>
<p>With this chatbot the LLM can be invoked to generate output based on
the provided input and context. However, what is not possible in this
state, is to ask followup questions. This can be useful to refine the
output that it generates. The next step is therefore to implement
message persistence in the workflow.</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="im">from</span> langgraph.checkpoint.memory <span class="im">import</span> MemorySaver</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a><span class="im">from</span> langgraph.graph <span class="im">import</span> START, MessagesState, StateGraph</span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> Image, display</span></code></pre>
</div>
<p>The package LangGraph is a library that is designed to build LLM
agents using workflows represented as graphs. The workflows you create
consist of connected components, which allows you to build multi-step
processes. The workflow graphs can be easily visualised which makes them
quite insightful. LangGraph also has a build-in persistence layer,
exactly what we want right now!</p>
<p>First, define an empty workflow graph with the StateGraph class with
the MessageState schema (a simple schema with messages as only key)</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a>workflow <span class="op">=</span> StateGraph(state_schema<span class="op">=</span>MessagesState)</span></code></pre>
</div>
<p>Then define a function to invoke the llm with a message</p>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a><span class="kw">def</span> call_llm(state: MessagesState):</span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a>    response <span class="op">=</span> llm.invoke(state[<span class="st">"messages"</span>])</span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a>    <span class="cf">return</span> {<span class="st">"messages"</span>: response}</span></code></pre>
</div>
<p>Then add the call_llm function as a node to the graph and connect it
with an edge to the start point of the graph. This start node sends the
user input to the graph, which in this case only contains the LLM
element.</p>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a>workflow.add_node(<span class="st">"LLM"</span>, call_llm)</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a>workflow.add_edge(START, <span class="st">"LLM"</span>)</span></code></pre>
</div>
<p>Initialise a memory that will preserve the messages state in a
dictionary while going though the graph multiple times asking followup
questions.</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a>memory <span class="op">=</span> MemorySaver()</span></code></pre>
</div>
<p>Then compile and visualise the graph with the memory as
checkpoint.</p>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a>graph <span class="op">=</span> workflow.<span class="bu">compile</span>(checkpointer<span class="op">=</span>memory)</span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a>display(Image(graph.get_graph().draw_mermaid_png()))</span></code></pre>
</div>
<p><img src=".././fig/workflow_llm.png" alt="workflow" class="figure"> Define a memory
id for the current conservation.</p>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a>config <span class="op">=</span> {<span class="st">"configurable"</span>: {<span class="st">"thread_id"</span>: <span class="st">"moonconversation"</span>}}</span></code></pre>
</div>
<p>Then call the workflow with memory we created with the original
question</p>
<div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a>question <span class="op">=</span> <span class="st">'Who landed on the Moon?'</span></span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a>messages <span class="op">=</span> [HumanMessage(question)]</span>
<span id="cb15-3"><a href="#cb15-3" tabindex="-1"></a>output <span class="op">=</span> graph.invoke({<span class="st">"messages"</span>: messages}, config)</span>
<span id="cb15-4"><a href="#cb15-4" tabindex="-1"></a></span>
<span id="cb15-5"><a href="#cb15-5" tabindex="-1"></a>output[<span class="st">"messages"</span>][<span class="op">-</span><span class="dv">1</span>].pretty_print()</span></code></pre>
</div>
<p>The question and answer are now saved in the graph state with this
config, and followup questions and answers with the same config will be
added to it.</p>
<p>Everything that is saved can be found in the config state</p>
<div class="codewrapper sourceCode" id="cb16">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a>graph.get_state(config)</span></code></pre>
</div>
<p>The workflow can now be used to ask followup questions without having
to repeat the original question, and based on the previous generated
answer.</p>
<div class="codewrapper sourceCode" id="cb17">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a><span class="co"># Followup</span></span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a>followup <span class="op">=</span> <span class="st">"Shorten the answer to 20 words"</span></span>
<span id="cb17-3"><a href="#cb17-3" tabindex="-1"></a>input_messages <span class="op">=</span> [HumanMessage(followup)]</span>
<span id="cb17-4"><a href="#cb17-4" tabindex="-1"></a>output <span class="op">=</span> graph.invoke({<span class="st">"messages"</span>: input_messages}, config)</span>
<span id="cb17-5"><a href="#cb17-5" tabindex="-1"></a></span>
<span id="cb17-6"><a href="#cb17-6" tabindex="-1"></a><span class="co"># print the last output</span></span>
<span id="cb17-7"><a href="#cb17-7" tabindex="-1"></a>output[<span class="st">"messages"</span>][<span class="op">-</span><span class="dv">1</span>].pretty_print()</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb18">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" tabindex="-1"></a><span class="co"># Followup instruction</span></span>
<span id="cb18-2"><a href="#cb18-2" tabindex="-1"></a>followup2 <span class="op">=</span> <span class="st">"Translate the answer to Dutch"</span></span>
<span id="cb18-3"><a href="#cb18-3" tabindex="-1"></a>input_messages <span class="op">=</span> [HumanMessage(followup2)]</span>
<span id="cb18-4"><a href="#cb18-4" tabindex="-1"></a>output <span class="op">=</span> graph.invoke({<span class="st">"messages"</span>: input_messages}, config)</span>
<span id="cb18-5"><a href="#cb18-5" tabindex="-1"></a></span>
<span id="cb18-6"><a href="#cb18-6" tabindex="-1"></a><span class="co"># print the last output</span></span>
<span id="cb18-7"><a href="#cb18-7" tabindex="-1"></a>output[<span class="st">"messages"</span>][<span class="op">-</span><span class="dv">1</span>].pretty_print()</span></code></pre>
</div>
</div>
</section><section><h2 class="section-heading" id="retrieval-augmented-generation---build-a-rag">Retrieval Augmented Generation - Build a RAG<a class="anchor" aria-label="anchor" href="#retrieval-augmented-generation---build-a-rag"></a></h2>
<hr class="half-width"><p>A chatbot tends to give quite generic answers. A more specific
chatbot can be made by building a Retrieval Augmented Generation agent.
This is an information that you yourself provide with a knowledge base:
a large number of documents. When prompted with a questions, the agent
first retrieves relevant sections of the data that is in the knowledge
base, and then generates and answer based on that data. In this way you
can build an agent with very specific knowledge.</p>
<p>The simplest form of a rag consists of two parts, a retriever and a
generator. The retriever part will collect data from the provided data,
so first a knowledge base has to be created for the retriever.</p>
<p>To generate text in the RAG the trained Llama model will be used,
which works well for English text. Because this model was not trained on
Dutch text, the RAG will work better for an English knowledge base.</p>
<p>Three newspaper pages will be used for the example RAG, these are
pages from a Curacao newspaper. This is a Dutch newspaper with an
additional page in English. The text versions of the newspapers can be
downloaded to only get these specific English pages. Save them in a
folder called “rag_data” for further processing: - <a href="https://www.delpher.nl/nl/kranten/view?query=the+moon&amp;coll=ddd&amp;identifier=ddd:010460545:mpeg21:p012&amp;resultsidentifier=ddd:010460545:mpeg21:a0134&amp;rowid=4" class="external-link">page1</a>
- <a href="https://www.delpher.nl/nl/kranten/view?query=moon+landing&amp;coll=ddd&amp;page=1&amp;facets%5Bspatial%5D%5B%5D=Nederlandse+Antillen&amp;identifier=ddd:010460616:mpeg21:a0146&amp;resultsidentifier=ddd:010460616:mpeg21:a0146&amp;rowid=1" class="external-link">page2</a>
- <a href="https://www.delpher.nl/nl/kranten/view?query=moon+landing&amp;coll=ddd&amp;page=1&amp;facets%5Bspatial%5D%5B%5D=Nederlandse+Antillen&amp;identifier=ddd:010460520:mpeg21:a0167&amp;resultsidentifier=ddd:010460520:mpeg21:a0167&amp;rowid=7" class="external-link">page3</a></p>
<div class="section level4">
<h4 id="the-knowledge-base---a-vector-store">The knowledge base - a vector store<a class="anchor" aria-label="anchor" href="#the-knowledge-base---a-vector-store"></a></h4>
<p>Language models all work with vectors - embedded text. Instead of
saving text, a the data has to be stored in embedded versions in a
vector store, where the retriever can shop around for the relevant
text.</p>
<p>There a number of packages to be used in this section to build the
RAG.</p>
<div class="codewrapper sourceCode" id="cb19">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb19-2"><a href="#cb19-2" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> Image, display</span>
<span id="cb19-3"><a href="#cb19-3" tabindex="-1"></a><span class="im">from</span> typing_extensions <span class="im">import</span> List, TypedDict</span>
<span id="cb19-4"><a href="#cb19-4" tabindex="-1"></a></span>
<span id="cb19-5"><a href="#cb19-5" tabindex="-1"></a><span class="im">from</span> langchain_ollama <span class="im">import</span> ChatOllama, OllamaEmbeddings</span>
<span id="cb19-6"><a href="#cb19-6" tabindex="-1"></a><span class="im">from</span> langchain_core.messages <span class="im">import</span> HumanMessage</span>
<span id="cb19-7"><a href="#cb19-7" tabindex="-1"></a><span class="im">from</span> langchain_core.documents <span class="im">import</span> Document</span>
<span id="cb19-8"><a href="#cb19-8" tabindex="-1"></a><span class="im">from</span> langchain_core.vectorstores <span class="im">import</span> InMemoryVectorStore</span>
<span id="cb19-9"><a href="#cb19-9" tabindex="-1"></a><span class="im">from</span> langchain_text_splitters <span class="im">import</span> RecursiveCharacterTextSplitter</span>
<span id="cb19-10"><a href="#cb19-10" tabindex="-1"></a><span class="im">from</span> langgraph.graph <span class="im">import</span> START, StateGraph</span>
<span id="cb19-11"><a href="#cb19-11" tabindex="-1"></a><span class="im">from</span> langchain_nomic.embeddings <span class="im">import</span> NomicEmbeddings</span></code></pre>
</div>
<p>Define the large language model to be used to generate an answer
based on provided context:</p>
<div class="codewrapper sourceCode" id="cb20">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" tabindex="-1"></a>llm <span class="op">=</span> ChatOllama(model<span class="op">=</span><span class="st">"llama3.1:8b"</span>, temperature<span class="op">=</span><span class="dv">0</span>)</span></code></pre>
</div>
<p>Define the embeddings model, this is the model to convert our
knowledge base texts into vector embeddings and will be used for the
retrieval part of the RAG:</p>
<div class="codewrapper sourceCode" id="cb21">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" tabindex="-1"></a>embeddings<span class="op">=</span>NomicEmbeddings(model<span class="op">=</span><span class="st">"nomic-embed-text-v1.5"</span>, inference_mode<span class="op">=</span><span class="st">"local"</span>)</span></code></pre>
</div>
<div id="accordionInstructor1" class="accordion instructor-note accordion-flush">
<div class="accordion-item">
<button class="accordion-button instructor-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseInstructor1" aria-expanded="false" aria-controls="collapseInstructor1">
  <h3 class="accordion-header" id="headingInstructor1">  <div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="edit-2"></i></div> Instructor Note </h3>
</button>
<div id="collapseInstructor1" class="accordion-collapse collapse" data-bs-parent="#accordionInstructor1" aria-labelledby="headingInstructor1">
<div class="accordion-body">
<p>For more on Nomic embeddings see: <a href="https://python.langchain.com/api_reference/nomic/embeddings/langchain_nomic.embeddings.NomicEmbeddings.html" class="external-link uri">https://python.langchain.com/api_reference/nomic/embeddings/langchain_nomic.embeddings.NomicEmbeddings.html</a></p>
<p>using inference_model=“local” uses (Embed4All)[<a href="https://docs.gpt4all.io/old/gpt4all_python_embedding.html" class="external-link uri">https://docs.gpt4all.io/old/gpt4all_python_embedding.html</a>]</p>
</div>
</div>
</div>
</div>
<p>In the text files, the articles are split by ‘—’. This information
can be used to store the individual articles into a list. Store the
filename of the articles in a list as well, so that one can find easily
in from which file a text snippet was taken.</p>
<div class="codewrapper sourceCode" id="cb22">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" tabindex="-1"></a><span class="bu">dir</span> <span class="op">=</span> <span class="st">"./rag_data"</span></span>
<span id="cb22-2"><a href="#cb22-2" tabindex="-1"></a>articles <span class="op">=</span> []</span>
<span id="cb22-3"><a href="#cb22-3" tabindex="-1"></a>metadata <span class="op">=</span> []</span>
<span id="cb22-4"><a href="#cb22-4" tabindex="-1"></a></span>
<span id="cb22-5"><a href="#cb22-5" tabindex="-1"></a><span class="co"># Iterate over files and add individual articles and corresponding filenames to lists</span></span>
<span id="cb22-6"><a href="#cb22-6" tabindex="-1"></a><span class="cf">for</span> <span class="bu">file</span> <span class="kw">in</span> os.listdir(<span class="bu">dir</span>):</span>
<span id="cb22-7"><a href="#cb22-7" tabindex="-1"></a>    file_path <span class="op">=</span> os.path.join(<span class="bu">dir</span>, <span class="bu">file</span>)</span>
<span id="cb22-8"><a href="#cb22-8" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(file_path, <span class="st">"r"</span>) <span class="im">as</span> f:</span>
<span id="cb22-9"><a href="#cb22-9" tabindex="-1"></a>        content <span class="op">=</span> f.read().split(<span class="st">'---'</span>)</span>
<span id="cb22-10"><a href="#cb22-10" tabindex="-1"></a>        articles.extend(content)</span>
<span id="cb22-11"><a href="#cb22-11" tabindex="-1"></a>        metadata.extend([file_path] <span class="op">*</span> <span class="bu">len</span>(content))</span></code></pre>
</div>
<p>The generator will in the end provide an answer based on the text
snippet that is retrieved from the knowledge base. If the fragment is
very long, it may contain a lot of irrelevant information, which will
blur the generated answer. Therefor it is better to split the data into
smaller parts, so that the retriever can collect very specific pieces of
text to generate an answer from. It is useful to keep some overlap
between the splits, so that information does not get lost because of for
example splits in the middle of a sentence.</p>
<div class="codewrapper sourceCode" id="cb23">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" tabindex="-1"></a>text_splitter <span class="op">=</span> RecursiveCharacterTextSplitter(</span>
<span id="cb23-2"><a href="#cb23-2" tabindex="-1"></a>    chunk_size<span class="op">=</span><span class="dv">500</span>, chunk_overlap<span class="op">=</span><span class="dv">50</span></span>
<span id="cb23-3"><a href="#cb23-3" tabindex="-1"></a>)</span>
<span id="cb23-4"><a href="#cb23-4" tabindex="-1"></a></span>
<span id="cb23-5"><a href="#cb23-5" tabindex="-1"></a>documents <span class="op">=</span> text_splitter.create_documents(articles, metadatas<span class="op">=</span>[{<span class="st">'filename'</span>: <span class="bu">file</span>} <span class="cf">for</span> <span class="bu">file</span> <span class="kw">in</span> files])</span>
<span id="cb23-6"><a href="#cb23-6" tabindex="-1"></a></span>
<span id="cb23-7"><a href="#cb23-7" tabindex="-1"></a><span class="bu">print</span>(documents)</span></code></pre>
</div>
<p>This text splitter splits text based on the defined character chunk
size, but also takes into account spaces and newlines to slit in “smart”
chunks, so the chunks will not be exactly of length 500.</p>
<p>Finally, convert each text split into a vector, and save all vectors
in a vector store. The text is converted into embeddings using the
earlier defined embeddings model.</p>
<div class="codewrapper sourceCode" id="cb24">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" tabindex="-1"></a>vectorstore <span class="op">=</span> InMemoryVectorStore.from_texts(</span>
<span id="cb24-2"><a href="#cb24-2" tabindex="-1"></a>    [doc.page_content <span class="cf">for</span> doc <span class="kw">in</span> documents],</span>
<span id="cb24-3"><a href="#cb24-3" tabindex="-1"></a>    embedding<span class="op">=</span>embeddings,</span>
<span id="cb24-4"><a href="#cb24-4" tabindex="-1"></a>)</span></code></pre>
</div>
<p>The contents of the vectorstore can be printed as</p>
<div class="codewrapper sourceCode" id="cb25">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" tabindex="-1"></a><span class="bu">print</span>(vectorstore.store)</span></code></pre>
</div>
<p>It shows that for each text fragment that was given, a vector is
created and it is saved in the vectorstore together with the original
text.</p>
</div>
<div class="section level4">
<h4 id="setting-up-the-retriever-and-generator">Setting up the retriever and generator<a class="anchor" aria-label="anchor" href="#setting-up-the-retriever-and-generator"></a></h4>
<p>Define the structure of a dictionary with the keys
<code>question</code>, <code>context</code>, and
<code>answer</code>.</p>
<div class="codewrapper sourceCode" id="cb26">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" tabindex="-1"></a><span class="kw">class</span> State(TypedDict):</span>
<span id="cb26-2"><a href="#cb26-2" tabindex="-1"></a>    question: <span class="bu">str</span></span>
<span id="cb26-3"><a href="#cb26-3" tabindex="-1"></a>    context: List[Document]</span>
<span id="cb26-4"><a href="#cb26-4" tabindex="-1"></a>    answer: <span class="bu">str</span></span></code></pre>
</div>
<p>Define the retriever function of the RAG. It takes in the question
and does a similarity search in the created vectorstore and returns the
text snippets that were found to be similar. The similarity search
converts the question into an embeddings vector and uses the cosine
similarity to determine the similarity between the question and
snippets. It then returns the top 4 snippets with the highest cosine
similarity score. The snippets are returned in the original text form,
i.e. the retrieved vectors are transformed back into text.</p>
<div class="codewrapper sourceCode" id="cb27">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" tabindex="-1"></a><span class="kw">def</span> retrieve(state: State):</span>
<span id="cb27-2"><a href="#cb27-2" tabindex="-1"></a>    <span class="co">"Retrieve documents that are similar to the question."</span></span>
<span id="cb27-3"><a href="#cb27-3" tabindex="-1"></a>    retrieved_docs <span class="op">=</span> vectorstore.similarity_search(state[<span class="st">"question"</span>], k<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb27-4"><a href="#cb27-4" tabindex="-1"></a>    <span class="cf">return</span> {<span class="st">"context"</span>: retrieved_docs}</span></code></pre>
</div>
<p>Define the generator function of the RAG. In this function a prompt
is defined for the RAG using the context and question. The large
language model (the Llama model, defined above) is then invoked with
this question and generates an answer for the provided prompt, which is
returned as the answer key of the dictionary.</p>
<div class="codewrapper sourceCode" id="cb28">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" tabindex="-1"></a><span class="kw">def</span> generate(state: State):</span>
<span id="cb28-2"><a href="#cb28-2" tabindex="-1"></a>    docs_content <span class="op">=</span> <span class="st">"</span><span class="ch">\n\n</span><span class="st">"</span>.join(doc.page_content <span class="cf">for</span> doc <span class="kw">in</span> state[<span class="st">"context"</span>])</span>
<span id="cb28-3"><a href="#cb28-3" tabindex="-1"></a>    </span>
<span id="cb28-4"><a href="#cb28-4" tabindex="-1"></a>    rag_prompt <span class="op">=</span> <span class="st">"""You are an assistant for question-answering tasks.</span></span>
<span id="cb28-5"><a href="#cb28-5" tabindex="-1"></a><span class="st">    Here is the context to use to answer the question:</span></span>
<span id="cb28-6"><a href="#cb28-6" tabindex="-1"></a><span class="st">    </span><span class="sc">{context}</span></span>
<span id="cb28-7"><a href="#cb28-7" tabindex="-1"></a><span class="st">    Think carefully about the above context.</span></span>
<span id="cb28-8"><a href="#cb28-8" tabindex="-1"></a><span class="st">    Now, review the user question:</span></span>
<span id="cb28-9"><a href="#cb28-9" tabindex="-1"></a><span class="st">    </span><span class="sc">{question}</span></span>
<span id="cb28-10"><a href="#cb28-10" tabindex="-1"></a><span class="st">    Provide an answer to this questions using only the above context.</span></span>
<span id="cb28-11"><a href="#cb28-11" tabindex="-1"></a><span class="st">    Use 10 sentences maximum and keep the answer concise.</span></span>
<span id="cb28-12"><a href="#cb28-12" tabindex="-1"></a><span class="st">    Answer:"""</span></span>
<span id="cb28-13"><a href="#cb28-13" tabindex="-1"></a></span>
<span id="cb28-14"><a href="#cb28-14" tabindex="-1"></a>    rag_prompt_formatted <span class="op">=</span> rag_prompt.<span class="bu">format</span>(context<span class="op">=</span>docs_content, question<span class="op">=</span>State[<span class="st">"question"</span>])</span>
<span id="cb28-15"><a href="#cb28-15" tabindex="-1"></a>    </span>
<span id="cb28-16"><a href="#cb28-16" tabindex="-1"></a>    generate <span class="op">=</span> llm.invoke([HumanMessage(content<span class="op">=</span>rag_prompt_formatted)])</span>
<span id="cb28-17"><a href="#cb28-17" tabindex="-1"></a>    <span class="cf">return</span> {<span class="st">"answer"</span>: generate.content}</span></code></pre>
</div>
</div>
<div class="section level4">
<h4 id="build-the-workflow">Build the workflow<a class="anchor" aria-label="anchor" href="#build-the-workflow"></a></h4>
<p>The retriever and generator are combined into a workflow graph. The
workflow is defined as a StateGraph that uses the dictionary structure
(with the keys <code>question</code>, <code>context</code>, and
<code>answer</code>) defined above. The retriever and generator are
added as nodes, and the two are connected via the edge. The retrieve is
set as the start point of the workflow, and finally the graph is
compiled into an executable.</p>
<pre><code><span><span class="va">workflow</span> <span class="op">=</span> <span class="fu">StateGraph</span><span class="op">(</span><span class="va">State</span><span class="op">)</span></span>
<span><span class="fu">workflow.add_node</span><span class="op">(</span><span class="st">"retrieve"</span>, <span class="va">retrieve</span><span class="op">)</span></span>
<span><span class="fu">workflow.add_node</span><span class="op">(</span><span class="st">"generate"</span>, <span class="va">generate</span><span class="op">)</span></span>
<span><span class="fu">workflow.add_edge</span><span class="op">(</span><span class="st">"retrieve"</span>, <span class="st">"generate"</span><span class="op">)</span></span>
<span><span class="fu">workflow.set_entry_point</span><span class="op">(</span><span class="st">"retrieve"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">graph</span> <span class="op">=</span> <span class="fu">workflow.compile</span><span class="op">(</span><span class="op">)</span></span></code></pre>
<div class="codewrapper sourceCode" id="cb30">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" tabindex="-1"></a>display(Image(graph.get_graph().draw_mermaid_png()))https:<span class="op">//</span>scikit<span class="op">-</span>learn.org<span class="op">/</span><span class="fl">1.5</span><span class="op">/</span>modules<span class="op">/</span>grid_search.html</span></code></pre>
</div>
<figure><img src=".././fig/workflow_rag.png" alt="workflow" class="figure mx-auto d-block"><div class="figcaption">workflow</div>
</figure><p>That’s it! The RAG can now be asked questions. Let’s see what it can
tell about the moon landing:</p>
<div class="codewrapper sourceCode" id="cb31">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" tabindex="-1"></a>response <span class="op">=</span> graph.invoke({<span class="st">"question"</span>: <span class="st">"Who landed on the Moon?"</span>})</span>
<span id="cb31-2"><a href="#cb31-2" tabindex="-1"></a><span class="bu">print</span>(response[<span class="st">"answer"</span>])</span></code></pre>
</div>
<p>This is quite a specific answer. It can be seen why by looking at the
text snippets that were used:</p>
<pre><code><span><span class="fu">print</span><span class="op">(</span><span class="va">response</span><span class="op">[</span><span class="st">"context"</span><span class="op">]</span><span class="op">)</span></span></code></pre>
<p>While a general chatbot uses all the information in the material that
it was trained on, the RAG only uses the information that was stored in
the vectorstore to generate the answer.</p>
<div id="challenge2" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div class="callout-inner">
<h3 class="callout-title">Challenge</h3>
<div class="callout-content">
<p>Try generating more answers with the RAG based on other questions,
perhaps also looking at the newspaper texts that are used. What stands
out?</p>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2"> Show me the solution </h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" data-bs-parent="#accordionSolution2" aria-labelledby="headingSolution2">
<div class="accordion-body">
<p>For example: - The RAG returns in some cases that no answer can be
generated on the context it was provided with - For some questions, the
LLM returns that it cannot provide an answer because of safety
precautions that are inherent to the LLM used, such as information about
violent acts.</p>
</div>
</div>
</div>
</div>
<p>This is the simplest form of a RAG, with a retriever and a generator.
However, one can make the RAG more complex by adding more components and
options to the workflow, for example one to check the relevance of the
retrieved documents, removing those that turn out to be irrelevant to be
used for the answer generation, or having a component that can
reformulate the question. Another example is to add a hallucination
checker step after the generator that checks if the generated answer can
actually be found in the provided context.</p>
</div>
</section><div class="section level1">
<h1 id="pitfalls-limitations-privacy">Pitfalls, limitations, privacy<a class="anchor" aria-label="anchor" href="#pitfalls-limitations-privacy"></a></h1>
<p>While LLMs are very powerful and provide us with many great
possibilities and opportunities, they also have limitations.</p>
<ul><li>Training data: LLMs are trained on large data sets. These are often
collected from the internet and books, but these come with downsides
<ul><li>They have a date cutoff; LLMs are trained on a static data set,
meaning they are trained on information up to a certain date. They
therefor do not have the latest information. They are definitely not
useful for recent news events (often they mention this), but also lack
behind in for example technological advancements. Always consider how
old the model is that you are using.s</li>
<li>There is no fact checking involved in the training data. If the
training data contains a lot of incorrect information or fake news, this
will affect the answers generated.</li>
<li>The training data is often biased, including social and cultural
biases. This can lead to harmful responses that are stereotyping or
racist. Training LLMs does involve a human review for fine-tuning the
models, such that they are prevented from answering questions on illegal
activities, political advice, advice on harming yourself or others, or
generating violent or sexual content. When for example prompted with
questions about politics it does provide generic factual information,
but will also say that it will not give advice or opinionated
answers.</li>
<li>For GPT-4, there is no exact information provided as to which data
it is trained on, meaning that the data might be breaking privacy laws
or copyright infringement.</li>
<li>The data an LLM is trained on is generic, resulting in that it is
not good at generating answers for specialised questions. There are
however already a lot of models that are finetuned for specific
fields.</li>
<li>Language: LLMs are primarily trained on data collected from the
internet, resulting in that they are ‘best’ in the most spoken
languages. ChatGPT is trained on many languages, but languages that are
less widely spoken will automatically have smaller data to train on,
which makes the LLM less accurate in these languages.</li>
</ul></li>
<li>Multi-step thinking: LLMs are generally not good at multi-step
thinking. They are very good at providing bullet point lists of
information, but reasoning like humans do, drawing a conclusion from
combined logic is something they are not good at (yet).</li>
<li>Hallucinations: LLMs tend to hallucinate. When it ‘does not know the
answer’, it will often still try to provide an answer. You should
therefore not blindly use the answers from an LLM, but still check the
given information yourself.</li>
<li>Privacy: when using a language model locally, such as done above
with Llama, your privacy is preserved. The model is only on your laptop,
and the data you provide is not uploaded to any server. But when you for
example use ChatGPT via the web interface, there is no privacy. Any
information you provide, questions, provided context and so on will be
used by ChatGPT. It will be used (ao) for improving the model, which may
be considered a good thing, but other things it is used for are not
necessarily known. This means that you should be careful in what you
provide to the LLM. Never provide sensitive information or private data
to an LLM that you do not run fully locally.</li>
</ul><div id="key-points-to-remember" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<span class="callout-header">Key Points</span>
<div id="key-points-to-remember" class="callout-inner">
<h3 class="callout-title">Key points to remember</h3>
<div class="callout-content">
<ul><li>
<em>Transformer models power LLMs</em>: The Transformer architecture
and its self-attention mechanism allow LLMs to handle long-range
dependencies in text effectively.</li>
<li>
<em>LLMs excel at multiple tasks</em>: From text generation to
classification and summarisation, LLMs like Llama are versatile and
perform well on various NLP tasks.</li>
<li>
<em>Prompt engineering is crucial</em>: Designing effective prompts
can significantly improve the quality of outputs from LLMs, especially
for creative tasks like text generation and translation.</li>
<li>
<em>Real-world use cases</em>: LLMs can be applied to real-world
problems like news classification, summarisation, and headline
generation, improving efficiency in content management and
delivery.</li>
</ul></div>
</div>
</div>
</div>



      </div> <!-- / div.lesson-content -->
    </main><!-- / main#main-content.main-content --><nav class="bottom-pagination mx-md-4" aria-label="Previous and Next Chapter"><div class="d-block d-sm-block d-md-none">
        <a class="chapter-link" href="../instructor/02-transformers.html"><i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>Previous</a>
        <a class="chapter-link float-end" href="../instructor/index.html">Next<i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i></a>
      </div>
      <!-- content for large screens -->
      <div class="d-none d-sm-none d-md-block">
        <a class="chapter-link" href="../instructor/02-transformers.html" rel="prev">
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>
          Previous: Episode 2: BERT and
        </a>
        <a class="chapter-link float-end" href="../instructor/index.html" rel="next">
          Home
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i>
        </a>
      </div>
    </nav></div> <!-- / div.primary-content.col-xs-12 -->
<!-- END:   inst/pkgdown/templates/content-instructor.html-->

      </div><!--/div.row-->
      		<footer class="row footer mx-md-3"><hr><div class="col-md-6">
        <p>This lesson is subject to the <a href="CODE_OF_CONDUCT.html">Code of Conduct</a></p>
        <p>

        <a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/edit/main/episodes/03-LargeLanguageModels.md" class="external-link">Edit on GitHub</a>

	
        | <a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/blob/main/CONTRIBUTING.md" class="external-link">Contributing</a>
        | <a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/" class="external-link">Source</a></p>
				<p><a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/blob/main/CITATION.cff" class="external-link">Cite</a> | <a href="mailto:team@carpentries.org">Contact</a> | <a href="https://carpentries.org/about/" class="external-link">About</a></p>
			</div>
			<div class="col-md-6">

        <p>Materials licensed under <a href="LICENSE.html">CC-BY 4.0</a> by the authors</p>

        <p>Template licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/" class="external-link">CC-BY 4.0</a> by <a href="https://carpentries.org/" class="external-link">The Carpentries</a></p>
        <p>Built with <a href="https://github.com/carpentries/sandpaper/tree/0.17.1" class="external-link">sandpaper (0.17.1)</a>, <a href="https://github.com/carpentries/pegboard/tree/0.7.9" class="external-link">pegboard (0.7.9)</a>, and <a href="https://github.com/carpentries/varnish/tree/1.0.7" class="external-link">varnish (1.0.7)</a></p>
			</div>
		</footer></div> <!-- / div.container -->
	<div id="to-top">
		<a href="#top">
      <i class="search-icon" data-feather="arrow-up" role="img" aria-label="Back To Top"></i><br><!-- <span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top --><span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top
		</a>
	</div>
  <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "LearningResource",
  "@id": "https://esciencecenter-digital-skills.github.io/Natural-language-processing/instructor/03-LargeLanguageModels.html",
  "inLanguage": "en",
  "dct:conformsTo": "https://bioschemas.org/profiles/LearningResource/1.0-RELEASE",
  "description": "A Carpentries Lesson teaching foundational data and coding skills to researchers worldwide",
  "keywords": "software, data, lesson, The Carpentries, NLP, English, social sciences, pre-alpha",
  "name": "Episode 3: Using large language models",
  "creativeWorkStatus": "active",
  "url": "https://esciencecenter-digital-skills.github.io/Natural-language-processing/instructor/03-LargeLanguageModels.html",
  "identifier": "https://esciencecenter-digital-skills.github.io/Natural-language-processing/instructor/03-LargeLanguageModels.html",
  "dateCreated": "2024-04-24",
  "dateModified": "2025-08-19",
  "datePublished": "2025-08-19"
}

  </script><script>
		feather.replace();
	</script></body></html><!-- END:   inst/pkgdown/templates/layout.html-->

