<!DOCTYPE html>
<!-- START: inst/pkgdown/templates/layout.html --><!-- Generated by pkgdown: do not edit by hand --><html lang="en" data-bs-theme="auto"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><title>Fundamentals of Natural Language Processing (NLP) in Python: Transformers: BERT and Beyond</title><meta name="viewport" content="width=device-width, initial-scale=1"><script src="../assets/themetoggle.js"></script><link rel="stylesheet" type="text/css" href="../assets/styles.css"><script src="../assets/scripts.js" type="text/javascript"></script><!-- mathjax --><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      config: ["MMLorHTML.js"],
      jax: ["input/TeX","input/MathML","output/HTML-CSS","output/NativeMML", "output/PreviewHTML"],
      extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
      TeX: {
        extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
      },
      tex2jax: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [ ['$$','$$'], ['\\[', '\\]'] ],
        processEscapes: true
      }
    });
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><!-- Responsive Favicon for The Carpentries --><link rel="apple-touch-icon" sizes="180x180" href="../favicons/incubator/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="../favicons/incubator/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="../favicons/incubator/favicon-16x16.png"><link rel="manifest" href="../favicons/incubator/site.webmanifest"><link rel="mask-icon" href="../favicons/incubator/safari-pinned-tab.svg" color="#5bbad5"><meta name="msapplication-TileColor" content="#da532c"><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"><meta name="theme-color" media="(prefers-color-scheme: dark)" content="black"></head><body>
    <header id="top" class="navbar navbar-expand-md top-nav incubator"><svg xmlns="http://www.w3.org/2000/svg" class="d-none"><symbol id="check2" viewbox="0 0 16 16"><path d="M13.854 3.646a.5.5 0 0 1 0 .708l-7 7a.5.5 0 0 1-.708 0l-3.5-3.5a.5.5 0 1 1 .708-.708L6.5 10.293l6.646-6.647a.5.5 0 0 1 .708 0z"></path></symbol><symbol id="circle-half" viewbox="0 0 16 16"><path d="M8 15A7 7 0 1 0 8 1v14zm0 1A8 8 0 1 1 8 0a8 8 0 0 1 0 16z"></path></symbol><symbol id="moon-stars-fill" viewbox="0 0 16 16"><path d="M6 .278a.768.768 0 0 1 .08.858 7.208 7.208 0 0 0-.878 3.46c0 4.021 3.278 7.277 7.318 7.277.527 0 1.04-.055 1.533-.16a.787.787 0 0 1 .81.316.733.733 0 0 1-.031.893A8.349 8.349 0 0 1 8.344 16C3.734 16 0 12.286 0 7.71 0 4.266 2.114 1.312 5.124.06A.752.752 0 0 1 6 .278z"></path><path d="M10.794 3.148a.217.217 0 0 1 .412 0l.387 1.162c.173.518.579.924 1.097 1.097l1.162.387a.217.217 0 0 1 0 .412l-1.162.387a1.734 1.734 0 0 0-1.097 1.097l-.387 1.162a.217.217 0 0 1-.412 0l-.387-1.162A1.734 1.734 0 0 0 9.31 6.593l-1.162-.387a.217.217 0 0 1 0-.412l1.162-.387a1.734 1.734 0 0 0 1.097-1.097l.387-1.162zM13.863.099a.145.145 0 0 1 .274 0l.258.774c.115.346.386.617.732.732l.774.258a.145.145 0 0 1 0 .274l-.774.258a1.156 1.156 0 0 0-.732.732l-.258.774a.145.145 0 0 1-.274 0l-.258-.774a1.156 1.156 0 0 0-.732-.732l-.774-.258a.145.145 0 0 1 0-.274l.774-.258c.346-.115.617-.386.732-.732L13.863.1z"></path></symbol><symbol id="sun-fill" viewbox="0 0 16 16"><path d="M8 12a4 4 0 1 0 0-8 4 4 0 0 0 0 8zM8 0a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 0zm0 13a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 13zm8-5a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2a.5.5 0 0 1 .5.5zM3 8a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2A.5.5 0 0 1 3 8zm10.657-5.657a.5.5 0 0 1 0 .707l-1.414 1.415a.5.5 0 1 1-.707-.708l1.414-1.414a.5.5 0 0 1 .707 0zm-9.193 9.193a.5.5 0 0 1 0 .707L3.05 13.657a.5.5 0 0 1-.707-.707l1.414-1.414a.5.5 0 0 1 .707 0zm9.193 2.121a.5.5 0 0 1-.707 0l-1.414-1.414a.5.5 0 0 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .707zM4.464 4.465a.5.5 0 0 1-.707 0L2.343 3.05a.5.5 0 1 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .708z"></path></symbol></svg><a class="visually-hidden-focusable skip-link" href="#main-content">Skip to main content</a>
  <div class="container-fluid top-nav-container">
    <div class="col-md-8">
      <div class="large-logo">
        <img id="incubator-logo" alt="Carpentries Incubator" src="../assets/images/incubator-logo.svg"><span class="badge text-bg-danger">
          <abbr title="This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught.">
            <a href="https://docs.carpentries.org/resources/curriculum/lesson-life-cycle.html" class="external-link alert-link">
              <i aria-hidden="true" class="icon" data-feather="alert-octagon" style="border-radius: 5px"></i>
              Pre-Alpha
            </a>
            <span class="visually-hidden">This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught.</span>
          </abbr>
        </span>

      </div>
    </div>
    <div class="selector-container">
      <div id="theme-selector">
        <li class="nav-item dropdown" id="theme-button-list">
          <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
            <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="bd-theme-text"><li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                Light
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                Dark
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                Auto
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
          </ul></li>
      </div>

      <div class="dropdown" id="instructor-dropdown">
        <button class="btn btn-secondary dropdown-toggle bordered-button" type="button" id="dropdownMenu1" data-bs-toggle="dropdown" aria-expanded="false">
          <i aria-hidden="true" class="icon" data-feather="eye"></i> Instructor View <i data-feather="chevron-down"></i>
        </button>
        <ul class="dropdown-menu" aria-labelledby="dropdownMenu1"><li><button class="dropdown-item" type="button" onclick="window.location.href='../03-transformers.html';">Learner View</button></li>
        </ul></div>
    </div>
  </div>
  <hr></header><nav class="navbar navbar-expand-xl bottom-nav incubator" aria-label="Main Navigation"><div class="container-fluid nav-container">
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle Navigation">
      <span class="navbar-toggler-icon"></span>
      <span class="menu-title">Menu</span>
    </button>
    <div class="nav-logo">
      <img class="small-logo" alt="Carpentries Incubator" src="../assets/images/incubator-logo-sm.svg"></div>
    <div class="lesson-title-md">
      Fundamentals of Natural Language Processing (NLP) in Python
    </div>
    <div class="search-icon-sm">
      <!-- TODO: do not show until we have search
        <i role="img" aria-label="Search the All In One page" data-feather="search"></i>
      -->
    </div>
    <div class="desktop-nav">
      <ul class="navbar-nav me-auto mb-2 mb-lg-0"><li class="nav-item">
          <span class="lesson-title">
            Fundamentals of Natural Language Processing (NLP) in Python
          </span>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/key-points.html">Key Points</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/instructor-notes.html">Instructor Notes</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/images.html">Extract All Images</a>
        </li>
        <li class="nav-item dropdown">
          <button class="nav-link dropdown-toggle" id="navbarDropdown" data-bs-toggle="dropdown" aria-expanded="false">
            More <i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu" aria-labelledby="navbarDropdown"><hr></ul></li>
      </ul></div>
    <!--
    <form class="d-flex col-md-2 search-form">
      <fieldset disabled>
      <input class="form-control me-2 searchbox" type="search" placeholder="" aria-label="">
        <button class="btn btn-outline-success tablet-search-button"  type="submit">
          <i class="search-icon" data-feather="search" role="img" aria-label="Search the All In One page"></i>
        </button>
      </fieldset>
    </form>
    -->
    <a id="search-button" class="btn btn-primary" href="../instructor/aio.html" role="button" aria-label="Search the All In One page">Search the All In One page</a>
  </div><!--/div.container-fluid -->
</nav><div class="col-md-12 mobile-title">
  Fundamentals of Natural Language Processing (NLP) in Python
</div>

<aside class="col-md-12 lesson-progress"><div style="width: 50%" class="percentage">
    50%
  </div>
  <div class="progress incubator">
    <div class="progress-bar incubator" role="progressbar" style="width: 50%" aria-valuenow="50" aria-label="Lesson Progress" aria-valuemin="0" aria-valuemax="100">
    </div>
  </div>
</aside><div class="container">
      <div class="row">
        <!-- START: inst/pkgdown/templates/navbar.html -->
<div id="sidebar-col" class="col-lg-4">
  <div id="sidebar" class="sidebar">
      <nav aria-labelledby="flush-headingEleven"><button role="button" aria-label="close menu" alt="close menu" aria-expanded="true" aria-controls="sidebar" class="collapse-toggle" data-collapse="Collapse " data-episodes="Episodes ">
          <i class="search-icon" data-feather="x" role="img"></i>
        </button>
        <div class="sidebar-inner">
          <div class="row mobile-row" id="theme-row-mobile">
            <div class="col" id="theme-selector">
              <li class="nav-item dropdown" id="theme-button-list">
                <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
                  <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><span class="d-lg-none ms-1" id="bd-theme-text">Toggle Theme</span>
                </button>
                <ul class="dropdown-menu dropdown-menu-right" aria-labelledby="bd-theme-text"><li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                      Light
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                      Dark
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                      <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                      Auto
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                </ul></li>
            </div>
          </div>
          <div class="row mobile-row">
            <div class="col">
              <div class="sidenav-view-selector">
                <div class="accordion accordion-flush" id="accordionFlush9">
                  <div class="accordion-item">
                    <h2 class="accordion-header" id="flush-headingNine">
                      <button class="accordion-button collapsed" id="instructor" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseNine" aria-expanded="false" aria-controls="flush-collapseNine">
                        <i id="eye" aria-hidden="true" class="icon" data-feather="eye"></i> Instructor View
                      </button>
                    </h2>
                    <div id="flush-collapseNine" class="accordion-collapse collapse" aria-labelledby="flush-headingNine" data-bs-parent="#accordionFlush2">
                      <div class="accordion-body">
                        <a href="../03-transformers.html">Learner View</a>
                      </div>
                    </div>
                  </div><!--/div.accordion-item-->
                </div><!--/div.accordion-flush-->
              </div><!--div.sidenav-view-selector -->
            </div><!--/div.col -->

            <hr></div><!--/div.mobile-row -->

          <div class="accordion accordion-flush" id="accordionFlush11">
            <div class="accordion-item">

              <button id="chapters" class="accordion-button show" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseEleven" aria-expanded="false" aria-controls="flush-collapseEleven">
                <h2 class="accordion-header chapters" id="flush-headingEleven">
                  EPISODES
                </h2>
              </button>
              <div id="flush-collapseEleven" class="accordion-collapse show collapse" aria-labelledby="flush-headingEleven" data-bs-parent="#accordionFlush11">

                <div class="accordion-body">
                  <div class="accordion accordion-flush" id="accordionFlush1">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading1">
        <a href="index.html">Summary and Schedule</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush2">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading2">
        <a href="01-introduction.html">1. Introduction</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush3">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading3">
        <a href="02-word_representations.html">2. From words to vectors</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlushcurrent">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-headingcurrent">
      <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapsecurrent" aria-expanded="true" aria-controls="flush-collapsecurrent">
        <span class="visually-hidden">Current Chapter</span>
        <span class="current-chapter">
        3. Transformers: BERT and Beyond
        </span>
      </button>
    </div><!--/div.accordion-header-->

    <div id="flush-collapsecurrent" class="accordion-collapse collapse show" aria-labelledby="flush-headingcurrent" data-bs-parent="#accordionFlushcurrent">
      <div class="accordion-body">
        <ul><li><a href="#transformers">Transformers</a></li>
<li><a href="#bert">BERT</a></li>
<li><a href="#bert-architecture">BERT Architecture</a></li>
<li><a href="#bert-for-word-based-analysis">BERT for Word-Based Analysis</a></li>
<li><a href="#bert-as-a-language-model">BERT as a Language Model</a></li>
<li><a href="#bert-for-text-classification">BERT for Text Classification</a></li>
<li><a href="#model-evaluation">Model Evaluation</a></li>
<li><a href="#bert-for-token-classification">BERT for Token Classification</a></li>
        </ul></div><!--/div.accordion-body-->
    </div><!--/div.accordion-collapse-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush5">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading5">
        <a href="04-LargeLanguageModels.html">4. Using large language models</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

                </div>
              </div>
            </div>

            <hr class="half-width"><div class="accordion accordion-flush lesson-resources" id="accordionFlush12">
              <div class="accordion-item">
                <h2 class="accordion-header" id="flush-headingTwelve">
                  <button class="accordion-button collapsed" id="lesson-resources" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseTwelve" aria-expanded="false" aria-controls="flush-collapseTwelve">
                    RESOURCES
                  </button>
                </h2>
                <div id="flush-collapseTwelve" class="accordion-collapse collapse" aria-labelledby="flush-headingTwelve" data-bs-parent="#accordionFlush12">
                  <div class="accordion-body">
                    <ul><li>
                        <a href="../instructor/key-points.html">Key Points</a>
                      </li>
                      <li>
                        <a href="../instructor/instructor-notes.html">Instructor Notes</a>
                      </li>
                      <li>
                        <a href="../instructor/images.html">Extract All Images</a>
                      </li>
                      <hr></ul></div>
                </div>
              </div>
            </div>
            <hr class="half-width lesson-resources"><a href="../instructor/aio.html">See all in one page</a>


            <hr class="d-none d-sm-block d-md-none"><div class="d-grid gap-1">

            </div>
          </div><!-- /div.accordion -->
        </div><!-- /div.sidebar-inner -->
      </nav></div><!-- /div.sidebar -->
  </div><!-- /div.sidebar-col -->
<!-- END:   inst/pkgdown/templates/navbar.html-->

        <!-- START: inst/pkgdown/templates/content-instructor.html -->
  <div class="col-xl-8 col-lg-12 primary-content">
    <nav class="lesson-content mx-md-4" aria-label="Previous and Next Chapter"><!-- content for small screens --><div class="d-block d-sm-block d-md-none">
        <a class="chapter-link" href="../instructor/02-word_representations.html"><i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>Previous</a>
        <a class="chapter-link float-end" href="../instructor/04-LargeLanguageModels.html">Next<i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i></a>
      </div>
      <!-- content for large screens -->
      <div class="d-none d-sm-none d-md-block">
        <a class="chapter-link" href="../instructor/02-word_representations.html" rel="prev">
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>
          Previous: From words to
        </a>
        <a class="chapter-link float-end" href="../instructor/04-LargeLanguageModels.html" rel="next">
          Next: Using large language...
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i>
        </a>
      </div>
      <hr></nav><main id="main-content" class="main-content"><div class="container lesson-content">
        <h1>Transformers: BERT and Beyond</h1>
        <p>Last updated on 2025-12-01 |

        <a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/edit/main/episodes/03-transformers.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>



        <p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 120 minutes</p>

        <div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>

        

<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul><li>What are some drawbacks of static word embeddings?</li>
<li>What are Transformers?</li>
<li>What is BERT and how does it work?</li>
<li>How can I use BERT to solve supervised NLP tasks?</li>
<li>How should I evaluate my classifiers?</li>
</ul></div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul><li>Understand how a Transformer works and recognize their main
components.</li>
<li>Understand how to use pre-trained transformers (Use Case: BERT)</li>
<li>Use BERT to predict words in context.</li>
<li>Use BERT to classify texts.</li>
<li>Learn to evaluate your NLP Classifier.</li>
<li>Understand assumptions and basic evaluation for NLP outputs.</li>
</ul></div>
</div>
</div>
</div>
</div>
<p>Word embeddings such as Word2Vec can be used to represent words as
unique vectors instead of Python strings. These vector representations
give us numerical “proxy” representations for words. And these allow us
to provide mathematical definitions for attributing semantics or meaning
to words. They also enable metrics and measures for studying linguistic
relationships with words. For example, one can devise metrics for
similarity and semantic closeness of words, by defining a measure of
distance between their corresponding vectors. This has proven to be
quite useful for downstream lexical-related tasks.</p>
<p>However, a big drawback of Word2Vec is that <strong>each word is
represented in isolation</strong>, which means that once we finished
training a model, each word has exactly one vector associated with it,
regardless of the different contexts in which it appears in the corpus
text. This is what is called <em>static word embedding</em>, and
unfortunately that is a serious limitation in expressing finer-grained
complexities in language. Words derive their meaning
<em>dynamically</em>, based on the specific context in which they are
used. Think of syntactic information, which is relevant to understand
the difference between “the dog bit the man” and “the man bit the dog”.
Another case is polysemy, where the same word can have very different
meanings depending on the context, for example, “bit” in “the dog bit
the man” and “in this bit of the book”. Therefore, we would like to have
richer vectors of words that are themselves sensitive to their context
in order to obtain finer-grained representations of word meaning.</p>
<div id="polysemy-in-language" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="polysemy-in-language" class="callout-inner">
<h3 class="callout-title">Polysemy in Language</h3>
<div class="callout-content">
<p>Think of words (at least 2) that can have more than one meaning
depending on the context. Come up with one simple sentence per meaning
and explain what they mean in each context. Discuss: How do you know
which of the possible meanings does the word have when you see it?</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<p>Two possible examples can be the words ‘fine’ and ‘run’</p>
<p>Sentences for ‘fine’: - She has a fine watch (fine == high-quality) -
He had to pay a fine (fine == penalty) - I am feeling fine (fine == not
bad)</p>
<p>Sentences for ‘run’: - I had to run to catch the bus (run == moving
fast) - Stop talking, before you run out of ideas (run (out) ==
exhaust)</p>
<p>Note how in the “run out” example we even have to understand that the
meaning of run is not literal but goes accompanied with a preposition
that changes its meaning.</p>
</div>
</div>
</div>
</div>
<div id="bridging-word-embeddings-and-contextualized-models" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="bridging-word-embeddings-and-contextualized-models" class="callout-inner">
<h3 class="callout-title">Bridging Word Embeddings and Contextualized
Models</h3>
<div class="callout-content">
<p>The limitations of Word2Vec became apparent as researchers tackled
more complex natural language understanding tasks. BERT, the model we
will describe below, was not the first attempt to improve upon the
drawbacks of Word2Vec. Several intermediate models emerged to address
the shortcomings. Some prominent models were:</p>
<ul><li><p><strong>FastText</strong> <a href="https://arxiv.org/pdf/1607.01759" class="external-link">(Joulin, et. al., 2016)</a>,
developed by Facebook, extended Word2Vec by representing words as bags
of character n-grams rather than atomic units. This subword tokenization
approach enabled the model to generate embeddings for previously unseen
words by combining learned subword representations—for instance,
understanding “unhappiness” through its components “un-,” “happiness,”
and “-ness.” FastText proved particularly valuable for morphologically
rich languages and handling misspellings or rare word forms.</p></li>
<li><p><strong>ELMo</strong> <a href="https://aclanthology.org/N18-1202.pdf" class="external-link">(Peters, et. al., 2018)</a>
marked a paradigm shift by integrating context into the word
representations. Unlike Word2Vec’s static embeddings, ELMo generated
different representations for the same word based on its surrounding
context using bidirectional LSTM networks. The model was pretrained on
large text corpora using a language modeling objective—predicting words
from both left and right contexts simultaneously—and crucially
introduced effective transfer learning to NLP. BERT would basically
replicate this concept but using a more powerful neural network
architecture: the Transformer, which allowed for the scaling of training
material.</p></li>
<li><p><strong>ULMFiT</strong> <a href="https://aclanthology.org/P18-1031.pdf" class="external-link">(Howard &amp; Ruder,
2018)</a>. Universal Language Model Fine-tuning, also tackled the
problem via transfer learning; that is, <em>re-using the same model</em>
for learning several different tasks and hence enriching word
representations after each task was learned. This idea also enriched
BERT post-training methodologies.</p></li>
</ul><p>These intermediate models established several crucial concepts: that
subword tokenization could handle vocabulary limitations, that
context-dependent representations were superior to static embeddings,
that deep bidirectional architectures captured richer linguistic
information, and most importantly, that large-scale pretraining followed
by task-specific fine-tuning could dramatically improve performance
across diverse NLP applications.</p>
</div>
</div>
</div>
<p>In late 2018, the <a href="https://aclanthology.org/N19-1423.pdf" class="external-link">BERT</a> language model was
introduced. Using a novel architecture called <a href="https://arxiv.org/pdf/1706.03762" class="external-link">Transformer</a> (2017), BERT was
specifically designed to scale the amount of training data and integrate
context into word representations. To understand BERT, we will first
look at what a transformer is and we will then directly use some code to
make use of BERT.</p>
<section><h2 class="section-heading" id="transformers">Transformers<a class="anchor" aria-label="anchor" href="#transformers"></a></h2>
<hr class="half-width"><p>The Transformer is a deep neural network architecture proposed by
Google researchers in a paper called <em>Attention is all you Need</em>.
They tackled specifically the NLP task of Machine Translation (MT),
which is stated as: how to generate a sentence (sequence of words) in
target language B given a sentence in source language A? We all know
that translation cannot be done word by word in isolation, therefore
integrating the context from both the source language and the target
language is necessary. In order to translate, first one neural network
needs to <em>encode</em> the whole meaning of the sentence in language A
into a single vector representation, then a second neural network needs
to <em>decode</em> that representation into tokens that are both
coherent with the meaning of language A and understandable in language
B. Therefore we say that translation is modeling language B
<em>conditioned</em> on what language A originally said.</p>
<figure><img src="../fig/trans1.png" alt="Transformer Architecture" class="figure mx-auto d-block"><div class="figcaption">Transformer Architecture</div>
</figure><p>As seen in the picture, the original Transformer is an
Encoder-Decoder network that tackles translation. We first need a token
embedder which converts the string of words into a sequence of vectors
that the Transformer network can process. The first component, the
<strong>Encoder</strong>, is optimized for creating <strong>rich
representations</strong> of the source sequence (in this case an English
sentence) while the second one, the <strong>Decoder</strong> is a
<strong>generative network</strong> that is conditioned on the encoded
representation. The third component we see is the infamous attention
mechanism, a third neural network what computes the correlation between
source and target tokens (<em>which word in Dutch should I pay attention
to, to decide a better next English word?</em>) to generate the most
likely token in the target sequence (in this case Dutch words).</p>
<div id="emulate-the-attention-mechanism" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="emulate-the-attention-mechanism" class="callout-inner">
<h3 class="callout-title">Emulate the Attention Mechanism</h3>
<div class="callout-content">
<p>Pair with a person who speaks a language different from English (we
will call it language B). Think of 1 or 2 simple sentences in English
and come up with their translations in the second language. In a piece
of paper write down both sentences (one on top of the other with some
distance in between) and try to:</p>
<ol style="list-style-type: decimal"><li>Draw a mapping of words or phrases from language B to English. Is it
always possible to do this one-to-one for words?</li>
<li>Think of how this might relate to attention in transformers?</li>
</ol></div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2"> Show me the solution </h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" data-bs-parent="#accordionSolution2" aria-labelledby="headingSolution2">
<div class="accordion-body">
<p>Here is an example of a sentence in English and its translation into
Spanish. We can look at the final mapping and observe that:</p>
<ol style="list-style-type: decimal"><li>Even though they are closely related languages, the translation is
not linear</li>
<li>There is also not a direct word-to-word mapping between the
sentences</li>
<li>Some words present in the source are not present in the target (and
vice versa)</li>
<li>Some words are not translations of each other but they are still
very relevant to understand the context</li>
</ol><figure><img src="../fig/trans_attention_mapping.png" class="figure mx-auto d-block"></figure></div>
</div>
</div>
</div>
<p>Next, we will see how BERT exploits the idea of a <strong>Transformer
Encoder</strong> to perform the NLP Task we are interested in:
generating powerful word representations.</p>
</section><section><h2 class="section-heading" id="bert">BERT<a class="anchor" aria-label="anchor" href="#bert"></a></h2>
<hr class="half-width"><p><a href="https://aclanthology.org/N19-1423.pdf" class="external-link">BERT</a> is an
acronym that stands for <strong>B</strong>idirectional
<strong>E</strong>ncoder <strong>R</strong>epresentations from
<strong>T</strong>ransformers. The name describes it all: the idea is to
use the power of the Encoder component of the Transformer architecture
to create powerful token representations that preserve the contextual
meaning of the whole input segment, instead of each word in isolation.
The BERT vector representations of each token take into account both the
left context (what comes before the word) and the right context (what
comes after the word). Another advantage of the transformer Encoder is
that it is parallelizable, which made it possible for the first time to
train these networks on millions of datapoints, dramatically improving
model generalization.</p>
<div id="pretraining-bert" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="pretraining-bert" class="callout-inner">
<h3 class="callout-title">Pretraining BERT</h3>
<div class="callout-content">
<p>To obtain the BERT vector representations the Encoder is pre-trained
with two different tasks:</p>
<ul><li><p><strong>Masked Language Model:</strong> for each sentence, mask
one token at a time and predict which token is missing based on the
context from both sides. A training input example would be “Maria [MASK]
Groningen” and the model should predict the word “loves”.</p></li>
<li><p><strong>Next Sentence Prediction:</strong> the Encoder gets a
linear binary classifier on top, which is trained to decide for each
pair of sequences A and B, if sequence A precedes sequence B in a text.
For the sentence pair: “Maria loves Groningen.” and “This is a city in
the Netherlands.” the output of the classifier is “True” and for the
pair “Maria loves Groningen.” and “It was a tasty cake.” the output
should be “false” as there is no obvious continuation between the two
sentences.</p></li>
</ul><p>Already the second pre-training task gives us an idea of the power of
BERT: after it has been pretrained on hundreds of thousands of texts,
one can plug-in a classifier on top and re-use the <em>linguistic</em>
knowledge previously acquired to fine-tune it for a specific task,
without needing to learn the weights of the whole network from scratch
all over again. In the next sections we will describe the components of
BERT and show how to use it. This model and hundreds of related
transformer-based pre-trained encoders can also be found on <a href="https://huggingface.co/google-bert/bert-base-cased" class="external-link">Hugging
Face</a>.</p>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="bert-architecture">BERT Architecture<a class="anchor" aria-label="anchor" href="#bert-architecture"></a></h2>
<hr class="half-width"><p>The BERT Architecture can be seen as a basic NLP pipeline on its
own:</p>
<ol style="list-style-type: decimal"><li>
<strong>Tokenizer:</strong> splits text into tokens that the model
recognizes</li>
<li>
<strong>Embedder:</strong> converts each token into a fixed-sized
vector that represents it. These vectors are the actual input for the
Encoder.</li>
<li>
<strong>Encoder</strong> several neural layers that model the
token-level interactions of the input sequence to enhance meaning
representation. The output of the encoder is a set of
<strong>H</strong>idden layers, the vector representation of the
ingested sequence.</li>
<li>
<strong>Output Layer:</strong> the final encoder layer (which we
depict as a sequence <strong>H</strong>’s in the figure) contains
arguably the best token-level representations that encode syntactic and
semantic properties of each token, but this time each vector is already
contextualized with the specific sequence.</li>
<li>
<em>OPTIONAL</em> <strong>Classifier Layer:</strong> an additional
classifier can be connected on top of the BERT token vectors which are
used as features for performing a downstream task. This can be used to
classify at the text level, for example sentiment analysis of a
sentence, or at the token-level, for example Named Entity
Recognition.</li>
</ol><figure><img src="../fig/bert3.png" alt="BERT Architecture" class="figure mx-auto d-block"><div class="figcaption">BERT Architecture</div>
</figure><p>BERT uses (self-) attention, which is very useful to capture
longer-range word dependencies such as co-reference, where, for example,
a pronoun can be linked to the noun it refers to previously in the same
sentence. See the following example:</p>
<figure><img src="../fig/trans5.png" alt="The Encoder Self-Attention Mechanism" class="figure mx-auto d-block"><div class="figcaption">The Encoder Self-Attention Mechanism</div>
</figure></section><section><h2 class="section-heading" id="bert-for-word-based-analysis">BERT for Word-Based Analysis<a class="anchor" aria-label="anchor" href="#bert-for-word-based-analysis"></a></h2>
<hr class="half-width"><p>Let’s see how these components can be manipulated with code. For this
we will be using the HuggingFace’s <em>transformers</em> Python library.
The first two main components we need to initialize are the model and
tokenizer. The HuggingFace hub contains thousands of models based on a
Transformer architecture for dozens of tasks, data domains and also
hundreds of languages. Here we will explore the vanilla English BERT
which was how everything started. We can initialize this model with the
next lines:</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> BertTokenizer, BertModel</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a>tokenizer <span class="op">=</span> BertTokenizer.from_pretrained(<span class="st">'bert-base-cased'</span>)</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a>model <span class="op">=</span> BertModel.from_pretrained(<span class="st">"bert-base-cased"</span>)</span></code></pre>
</div>
<div class="section level3">
<h3 id="bert-tokenizer">BERT Tokenizer<a class="anchor" aria-label="anchor" href="#bert-tokenizer"></a></h3>
<p>We start with a string of text as written in any blog, book,
newspaper etcetera. The <code>tokenizer</code> object is responsible of
splitting the string into recognizable tokens for the model and
embedding the tokens into their vector representations</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"Maria loves Groningen"</span></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>encoded_input <span class="op">=</span> tokenizer(text, return_tensors<span class="op">=</span><span class="st">'pt'</span>)</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a><span class="bu">print</span>(encoded_input)</span></code></pre>
</div>
<p>The print shows the <code>encoded_input</code> object returned by the
tokenizer, with its attributes and values. The <code>input_ids</code>
are the most important output for now, as these are the token IDs
recognized by BERT</p>
<pre><code>{
    'input_ids': tensor([[  101,  3406,  7871,   144,  3484, 15016,   102]]),
    'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]),
    'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])
}
</code></pre>
<p>NOTE: the printing function shows transformers objects as
dictionaries; however, to access the attributes, you must use the Python
object syntax, such as in the following example:</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="bu">print</span>(encoded_input.input_ids.shape)</span></code></pre>
</div>
<p>Output:</p>
<p><code>torch.Size([1, 7])</code></p>
<p>The output is a 2-dimensional tensor where the first dimension
contains 1 element (this dimension represents the batch size), and the
second dimension contains 7 elements which are equivalent to the 7
tokens that BERT generated from our string input.</p>
<p>In order to see what these Token IDs represent, we can
<em>translate</em> them into human readable strings. This includes
converting the tensors into <code>numpy</code> arrays and converting
each ID into its string representation:</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a>token_ids <span class="op">=</span> <span class="bu">list</span>(encoded_input.input_ids[<span class="dv">0</span>].detach().numpy())</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>string_tokens <span class="op">=</span> tokenizer.convert_ids_to_tokens(token_ids)</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"IDs:"</span>, token_ids)</span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"TOKENS:"</span>, string_tokens)</span></code></pre>
</div>
<pre><code>IDs: [101, 3406, 7871, 144, 3484, 15016, 102]

TOKENS: ['[CLS]', 'Maria', 'loves', 'G', '##ron', '##ingen', '[SEP]']
</code></pre>
<p>First we feed the tokenized input into the model:</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a>output <span class="op">=</span> model(<span class="op">**</span>encoded_input)</span></code></pre>
</div>
<p>Now we can explore the representations in the model.</p>
<div id="callout3" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div class="callout-inner">
<div class="callout-content">
<p>In the case of wanting to obtain a single vector for
<em>Groningen</em>, you can average the three vectors that belong to the
token pieces that ultimately form that word. For example:</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a>tok_G <span class="op">=</span> output.last_hidden_state[<span class="dv">0</span>][<span class="dv">3</span>].detach().numpy()</span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a>tok_ron <span class="op">=</span> output.last_hidden_state[<span class="dv">0</span>][<span class="dv">4</span>].detach().numpy()</span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a>tok_ingen <span class="op">=</span> output.last_hidden_state[<span class="dv">0</span>][<span class="dv">5</span>].detach().numpy()</span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a></span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a>tok_groningen <span class="op">=</span> np.mean([tok_G, tok_ron, tok_ingen], axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb8-7"><a href="#cb8-7" tabindex="-1"></a><span class="bu">print</span>(tok_groningen.shape)</span>
<span id="cb8-8"><a href="#cb8-8" tabindex="-1"></a><span class="bu">print</span>(tok_groningen[:<span class="dv">10</span>])</span></code></pre>
</div>
<p>We obtain the output:</p>
<pre><code>(768,)
[ 0.00990007 -0.44266668  0.05274092  0.08865168  0.71115637 -0.4092603
  0.18841815  0.19883917  0.24680579 -0.07899686]</code></pre>
<p>We use the functions <code>detach().numpy()</code> to bring the
values from the Pytorch execution environment (for example a GPU) into
the main Python thread and treat it as a <code>Numpy</code> vector for
convenience. Then, since we are dealing with three <code>Numpy</code>
vectors we can average the three of them and end up with a single
<code>Groningen</code> vector of 768-dimensions representing the average
of <code>'G', '##ron', '##ingen'</code>.</p>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="analyzing-polysemy-with-bert">Analyzing polysemy with BERT<a class="anchor" aria-label="anchor" href="#analyzing-polysemy-with-bert"></a></h3>
<p>We can encode two sentences containing the word <em>note</em> to see
how BERT actually handles polysemy (<em>note</em> means something very
different in each sentence) thanks to the representation of each word
now being contextualized instead of isolated as was the case with
Word2vec.</p>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a><span class="co"># We tokenize the sentence and identify the two different 'note' tokens inside the sentence</span></span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a>text_note <span class="op">=</span> <span class="st">"Please note that this bank note is fake!"</span></span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a>tokenized_text <span class="op">=</span> tokenizer(text_note, return_tensors<span class="op">=</span><span class="st">'pt'</span>)</span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a>token_ids <span class="op">=</span> <span class="bu">list</span>(tokenized_text.input_ids[<span class="dv">0</span>].detach().numpy())</span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a>string_tokens <span class="op">=</span> tokenizer.convert_ids_to_tokens(token_ids)</span>
<span id="cb10-6"><a href="#cb10-6" tabindex="-1"></a><span class="bu">print</span>(string_tokens)</span></code></pre>
</div>
<p>We are printing the BERT tokens extracted from the sentence, and
displaying them:</p>
<pre><code>['[CLS]', 'Please', 'note', 'that', 'this', 'bank', 'note', 'is', 'fake', '!', '[SEP]']</code></pre>
<p>We then manually count which token indices in the list belong to
<code>note</code>, so we can extract their vectors later</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a>note_index_1 <span class="op">=</span> <span class="dv">2</span> <span class="co"># first occurrence of `note`</span></span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a>note_index_2 <span class="op">=</span> <span class="dv">6</span> <span class="co"># second occurrence of `note`</span></span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a><span class="bu">print</span>(string_tokens[note_index_1], string_tokens[note_index_2])</span></code></pre>
</div>
<p>We now pass the sentence through the BERT encoder and extract the
encoded vectors belonging to each <code>note</code> token:</p>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a><span class="co"># Encode the sentence and extract the vector belonging to each 'note' token</span></span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb13-4"><a href="#cb13-4" tabindex="-1"></a>    bert_output <span class="op">=</span> model(<span class="op">**</span>tokenized_text)</span>
<span id="cb13-5"><a href="#cb13-5" tabindex="-1"></a>note_vector_1 <span class="op">=</span> bert_output.last_hidden_state[<span class="dv">0</span>][note_index_1].detach().numpy()</span>
<span id="cb13-6"><a href="#cb13-6" tabindex="-1"></a>note_vector_2 <span class="op">=</span> bert_output.last_hidden_state[<span class="dv">0</span>][note_index_2].detach().numpy()</span>
<span id="cb13-7"><a href="#cb13-7" tabindex="-1"></a></span>
<span id="cb13-8"><a href="#cb13-8" tabindex="-1"></a><span class="bu">print</span>(note_vector_1[:<span class="dv">5</span>])</span>
<span id="cb13-9"><a href="#cb13-9" tabindex="-1"></a><span class="bu">print</span>(note_vector_2[:<span class="dv">5</span>])</span></code></pre>
</div>
<pre><code>[1.0170387  0.93691176 0.30571502 0.33091038 0.73093796]
[0.17840035 0.65847856 0.22412607 0.21162085 0.5393072 ]</code></pre>
<p>By printing the first 5 dimensions of the vectors we can see that,
even though both vectors technically belong to the same string input
<code>note</code>, they have different numeric representations. This is
the case because the BERT encoder takes into account the position of the
token as well as all the previous and following tokens when computing
the representation for each token.</p>
<p>To be sure, we can compute the cosine similarity of the word
<code>note</code> in the first sentence and the word <em>note</em> in
the second sentence confirming that they are indeed two different
representations, even when both cases have the same token-id and they
are the 12th token of the sentence:</p>
<div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a><span class="im">from</span> sklearn.metrics.pairwise <span class="im">import</span> cosine_similarity</span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb15-3"><a href="#cb15-3" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" tabindex="-1"></a>vector1 <span class="op">=</span> np.array(note_vector_1).reshape(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>)  <span class="co"># Reshape to 2D array</span></span>
<span id="cb15-5"><a href="#cb15-5" tabindex="-1"></a>vector2 <span class="op">=</span> np.array(note_vector_2).reshape(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>)  <span class="co"># Reshape to 2D array</span></span>
<span id="cb15-6"><a href="#cb15-6" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" tabindex="-1"></a>similarity <span class="op">=</span> cosine_similarity(vector1, vector2)</span>
<span id="cb15-8"><a href="#cb15-8" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Cosine Similarity 'note' vs 'note': </span><span class="sc">{</span>similarity[<span class="dv">0</span>][<span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code></pre>
</div>
<p>With this small experiment, we have confirmed that the Encoder
produces context-dependent word representations, as opposed to Word2Vec,
where <em>note</em> would always have the same vector no matter where it
appeared.</p>
<div id="callout4" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div class="callout-inner">
<div class="callout-content">
<p>When running examples in a BERT pre-trained model, it is advisable to
wrap your code inside a <code>torch.no_grad():</code> context. This is
linked to the fact that BERT is a Neural Network that has been trained
(and can be further fine-tuned) with the Backpropagation algorithm.
Essentially, this wrapper tells the model that we are not in training
mode, and we are not interested in <em>updating</em> the weights (as it
would happen when training any neural network), because the weights are
already optimal enough. By using this wrapper, we make the model more
efficient as it does not need to calculate the gradients for an eventual
backpropagation step, since we are only interested in what <em>comes
out</em> of the Encoder. So the previous code can be made more efficient
like this:</p>
<div class="codewrapper sourceCode" id="cb16">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a><span class="im">import</span> torch </span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a></span>
<span id="cb16-3"><a href="#cb16-3" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb16-4"><a href="#cb16-4" tabindex="-1"></a>    output <span class="op">=</span> model(<span class="op">**</span>encoded_input)</span>
<span id="cb16-5"><a href="#cb16-5" tabindex="-1"></a>    <span class="bu">print</span>(output)</span>
<span id="cb16-6"><a href="#cb16-6" tabindex="-1"></a>    <span class="bu">print</span>(output.last_hidden_state.shape)</span></code></pre>
</div>
</div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="bert-as-a-language-model">BERT as a Language Model<a class="anchor" aria-label="anchor" href="#bert-as-a-language-model"></a></h2>
<hr class="half-width"><p>As mentioned before, the main pre-training task of BERT is Language
Modelling (LM): calculating the probability of a word based on the known
neighboring words (yes, Word2Vec was also a kind of LM!). Obtaining
training data for this task is very cheap, as all we need is millions of
sentences from existing texts, without any labels. In this setting, BERT
encodes a sequence of words, and predicts from a set of English tokens,
what is the most likely token that could be inserted in the
<code>[MASK]</code> position</p>
<figure><img src="../fig/bert1b.png" alt="BERT Language Modeling" class="figure mx-auto d-block"><div class="figcaption">BERT Language Modeling</div>
</figure><p>We can therefore start using BERT as a predictor for word completion.
From now own, we will learn how to use the <code>pipeline</code> object,
this is very useful when we only want to use a pre-trained model for
predictions (no need to fine-tune or do word-specific analysis). The
<code>pipeline</code> will internally initialize both model and
tokenizer for us and also merge word pieces back into complete
words.</p>
<div id="callout5" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div class="callout-inner">
<div class="callout-content">
<p>The <code>pipeline</code> object is very convenient to test all the
out-of-the-box models that you can find in the HuggingFace platform, and
if you are lucky and you like what you see, this is the fastest way to
get model predictions.</p>
<p>But consider that for some tasks you will want more control over the
tokenization and how the model merges the predictions. If that is the
case, initializing the tokenizer, encoding the texts into word-vectors
and running the model inference yourself might be the way to go (like it
is shown in the <em>Polysemy in BERT</em> section).</p>
<p>Even if you <em>like what you see</em>, it is important that you
evaluate the performance of the models in <strong>your data and use
case</strong> regardless of what the model performance claims to be. We
will show an example with a couple of evaluation metrics in the <em>BERT
for Text Classification</em> section.</p>
</div>
</div>
</div>
<p>In this case again we use <code>bert-base-cased</code>, which refers
to the vanilla BERT English model. Once we declared a pipeline, we can
feed it with sentences that contain one masked token at a time (beware
that BERT can only predict one word at a time, since that was its
training scheme). For example:</p>
<div class="codewrapper sourceCode" id="cb17">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" tabindex="-1"></a><span class="kw">def</span> pretty_print_outputs(sentences, model_outputs):</span>
<span id="cb17-4"><a href="#cb17-4" tabindex="-1"></a>    <span class="cf">for</span> i, model_out <span class="kw">in</span> <span class="bu">enumerate</span>(model_outputs):</span>
<span id="cb17-5"><a href="#cb17-5" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">=====</span><span class="ch">\t</span><span class="st">"</span>,sentences[i])</span>
<span id="cb17-6"><a href="#cb17-6" tabindex="-1"></a>        <span class="cf">for</span> label_scores <span class="kw">in</span> model_out:</span>
<span id="cb17-7"><a href="#cb17-7" tabindex="-1"></a>            <span class="bu">print</span>(label_scores)</span>
<span id="cb17-8"><a href="#cb17-8" tabindex="-1"></a></span>
<span id="cb17-9"><a href="#cb17-9" tabindex="-1"></a></span>
<span id="cb17-10"><a href="#cb17-10" tabindex="-1"></a>nlp <span class="op">=</span> pipeline(task<span class="op">=</span><span class="st">"fill-mask"</span>, model<span class="op">=</span><span class="st">"bert-base-cased"</span>, tokenizer<span class="op">=</span><span class="st">"bert-base-cased"</span>)</span>
<span id="cb17-11"><a href="#cb17-11" tabindex="-1"></a>sentences <span class="op">=</span> [<span class="st">"Paris is the [MASK] of France"</span>, <span class="st">"I want to eat a cold [MASK] this afternoon"</span>, <span class="st">"Maria [MASK] Groningen"</span>]</span>
<span id="cb17-12"><a href="#cb17-12" tabindex="-1"></a>model_outputs <span class="op">=</span> nlp(sentences, top_k<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb17-13"><a href="#cb17-13" tabindex="-1"></a>pretty_print_outputs(sentences, model_outputs)</span></code></pre>
</div>
<pre><code>=====	 Paris is the [MASK] of France
{'score': 0.9807965755462646, 'token': 2364, 'token_str': 'capital', 'sequence': 'Paris is the capital of France'}
{'score': 0.004513159394264221, 'token': 6299, 'token_str': 'Capital', 'sequence': 'Paris is the Capital of France'}
{'score': 0.004281804896891117, 'token': 2057, 'token_str': 'center', 'sequence': 'Paris is the center of France'}
{'score': 0.002848200500011444, 'token': 2642, 'token_str': 'centre', 'sequence': 'Paris is the centre of France'}
{'score': 0.0022805952467024326, 'token': 1331, 'token_str': 'city', 'sequence': 'Paris is the city of France'}

=====	 I want to eat a cold [MASK] this afternoon
{'score': 0.19168031215667725, 'token': 13473, 'token_str': 'pizza', 'sequence': 'I want to eat a cold pizza this afternoon'}
{'score': 0.14800849556922913, 'token': 25138, 'token_str': 'turkey', 'sequence': 'I want to eat a cold turkey this afternoon'}
{'score': 0.14620967209339142, 'token': 14327, 'token_str': 'sandwich', 'sequence': 'I want to eat a cold sandwich this afternoon'}
{'score': 0.09997560828924179, 'token': 5953, 'token_str': 'lunch', 'sequence': 'I want to eat a cold lunch this afternoon'}
{'score': 0.06001955270767212, 'token': 4014, 'token_str': 'dinner', 'sequence': 'I want to eat a cold dinner this afternoon'}

=====	 Maria [MASK] Groningen
{'score': 0.24399833381175995, 'token': 117, 'token_str': ',', 'sequence': 'Maria, Groningen'}
{'score': 0.12300779670476913, 'token': 1104, 'token_str': 'of', 'sequence': 'Maria of Groningen'}
{'score': 0.11991506069898605, 'token': 1107, 'token_str': 'in', 'sequence': 'Maria in Groningen'}
{'score': 0.07722211629152298, 'token': 1306, 'token_str': '##m', 'sequence': 'Mariam Groningen'}
{'score': 0.0632941722869873, 'token': 118, 'token_str': '-', 'sequence': 'Maria - Groningen'}
</code></pre>
<p>When we call the <code>nlp</code> pipeline, requesting to return the
<code>top_k</code> most likely suggestions to complete the provided
sentences (in this case <code>k=5</code>). The pipeline returns a list
of outputs as Python dictionaries. Depending on the task, the fields of
the dictionary will differ. In this case, the <code>fill-mask</code>
task returns a score (between 0 and 1, the higher the score the more
likely the token is), a tokenId, and its corresponding string, as well
as the full “unmasked” sequence.</p>
<p>In the list of outputs we can observe: the first example shows
correctly that the missing token in the first sentence is
<em>capital</em>, the second example is a bit more ambiguous, but the
model at least uses the context to correctly predict a series of items
that can be eaten (unfortunately, none of its suggestions sound very
tasty); finally, the third example gives almost no useful context so the
model plays it safe and only suggests prepositions or punctuation. This
already shows some of the weaknesses of the approach.</p>
<div id="challenge3" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div class="callout-inner">
<h3 class="callout-title">Challenge</h3>
<div class="callout-content">
<p>Play with the <code>fill-mask</code> pipeline and try to find
examples where the model gives bad predictions and examples where the
predictions are very good. You can try:</p>
<ul><li>Changing the <code>top_k</code> parameter</li>
<li>Test the multilingual BERT model to compare. To do this, you should
change the <code>model</code> and <code>tokenizer</code> parameter name
to <code>bert-base-multilingual-cased</code>
</li>
<li>Search for bias in completions. For example, compare predictions for
“This man works as a [MASK].” vs. “This woman works as a [MASK].”.</li>
</ul></div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3"> Show me the solution </h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" data-bs-parent="#accordionSolution3" aria-labelledby="headingSolution3">
<div class="accordion-body">
<p>This is a free exercise, so anything works. But even by running the
same cases with the multilingual models we see some interesting aspects.
For example, the predictions are of less quality in English. This is due
to the “spread” of information across other languages, including a worse
tokenization, since this model tries do predict for around 200
languages.</p>
<p>Another interesting example is searching for <strong>bias in the
completions</strong>, these can be bias in many areas. In this case,
comparing the outputs you get for the sentences “This man works as a
[MASK].” and “This woman works as a [MASK].” exposes the huge gender
biases inside BERT word representations: for ‘man’ BERT predicts
<code>['lawyer', 'carpenter', 'doctor', 'waiter', 'mechanic']</code> and
for woman it predicts
<code>['nurse', 'waitress', 'teacher', 'main', 'prostitute']</code>.</p>
</div>
</div>
</div>
</div>
<p>We will next see the case of combining BERT with a classifier on
top.</p>
</section><section><h2 class="section-heading" id="bert-for-text-classification">BERT for Text Classification<a class="anchor" aria-label="anchor" href="#bert-for-text-classification"></a></h2>
<hr class="half-width"><p>The task of text classification is assigning a label to a whole
sequence of tokens, for example a sentence. With the parameter
<code>task="text_classification"</code> the <code>pipeline()</code>
function will load the base model and automatically add a linear layer
with a softmax on top. This layer can be fine-tuned with our own labeled
data or we can also directly load the fully pre-trained text
classification models that are already available in HuggingFace.</p>
<figure><img src="../fig/bert4.png" alt="BERT as an Emotion Classifier" class="figure mx-auto d-block"><div class="figcaption">BERT as an Emotion Classifier</div>
</figure><p>Let’s see the example of a ready pre-trained emotion classifier based
on <code>RoBERTa</code> model. This model was fine-tuned in the Go
emotions <a href="https://huggingface.co/datasets/google-research-datasets/go_emotions" class="external-link">dataset</a>,
taken from English Reddit and labeled for 28 different emotions at the
sentence level. The fine-tuned model is called <a href="https://huggingface.co/SamLowe/roberta-base-go_emotions" class="external-link">roberta-base-go_emotions</a>.
This model takes a sentence as input and outputs a probability
distribution over the 28 possible emotions that might be conveyed in the
text. For example:</p>
<div class="codewrapper sourceCode" id="cb19">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" tabindex="-1"></a></span>
<span id="cb19-2"><a href="#cb19-2" tabindex="-1"></a>classifier <span class="op">=</span> pipeline(task<span class="op">=</span><span class="st">"text-classification"</span>, model<span class="op">=</span><span class="st">"SamLowe/roberta-base-go_emotions"</span>, top_k<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb19-3"><a href="#cb19-3" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" tabindex="-1"></a>sentences <span class="op">=</span> [<span class="st">"I am not having a great day"</span>, <span class="st">"This is a lovely and innocent sentence"</span>, <span class="st">"Maria loves Groningen"</span>]</span>
<span id="cb19-5"><a href="#cb19-5" tabindex="-1"></a>model_outputs <span class="op">=</span> classifier(sentences)</span>
<span id="cb19-6"><a href="#cb19-6" tabindex="-1"></a></span>
<span id="cb19-7"><a href="#cb19-7" tabindex="-1"></a>pretty_print_outputs(sentences, model_outputs)</span></code></pre>
</div>
<pre><code>=====	 I am not having a great day
{'label': 'disappointment', 'score': 0.46669483184814453}
{'label': 'sadness', 'score': 0.39849498867988586}
{'label': 'annoyance', 'score': 0.06806594133377075}

=====	 This is a lovely and innocent sentence
{'label': 'admiration', 'score': 0.6457845568656921}
{'label': 'approval', 'score': 0.5112180113792419}
{'label': 'love', 'score': 0.09214121848344803}

=====	 Maria loves Groningen
{'label': 'love', 'score': 0.8922032117843628}
{'label': 'neutral', 'score': 0.10132959485054016}
{'label': 'approval', 'score': 0.02525361441075802}</code></pre>
<p>This code outputs again a list of dictionaries with the
<code>top-k</code> (<code>k=3</code>) emotions that each of the two
sentences convey. In this case, the first sentence evokes (in order of
likelihood) <em>disappointment</em>, <em>sadness</em> and
<em>annoyance</em>; whereas the second sentence evokes <em>love</em>,
<em>neutral</em> and <em>approval</em>. Note however that the likelihood
of each prediction decreases dramatically below the top choice, so
perhaps this specific classifier is only useful for the top emotion.</p>
<div id="callout6" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div class="callout-inner">
<div class="callout-content">
<p>Fine-tuning BERT is very cheap, because we only need to train the
<em>classifier</em> layer, a very small neural network, that can learn
to choose between the classes (labels) for your custom classification
problem, without needing a big amount of annotated data. This classifier
is a one-layer neural layer that assigns a score that can be translated
to the probability over a set of labels, given the input features
provided by BERT, which already <em>encodes</em> the meaning of the
entire sequence in its hidden states. Unfortunately fine-tuning is out
of the scope of this course but you can learn more about fine-tuning
BERT-like models in <a href="https://huggingface.co/docs/transformers/v4.57.1/en/training#fine-tuning" class="external-link">this
HuggingFace tutorial</a></p>
</div>
</div>
</div>
<figure><img src="../fig/bert4b.png" alt="BERT as an Emotion Classifier" class="figure mx-auto d-block"><div class="figcaption">BERT as an Emotion Classifier</div>
</figure></section><section><h2 class="section-heading" id="model-evaluation">Model Evaluation<a class="anchor" aria-label="anchor" href="#model-evaluation"></a></h2>
<hr class="half-width"><p>Model evaluation is a critical step in any machine learning project,
and it is also the case for NLP. While it may be tempting to rely on the
accuracy scores observed during training, or the model descriptions
provided on the web, this approach can be misleading and often results
in models that fail in real-world applications, and that includes your
data.</p>
<div id="callout7" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div class="callout-inner">
<div class="callout-content">
<p>The fundamental principle of model evaluation is to test your model
on data it has never seen before. <strong>If you are training your own
models, this is typically achieved by splitting your dataset into
training, validation, and test sets</strong>. The training set is used
to teach the model, the validation set helps tune hyperparameters or
other aspects of model development, and the test set provides a less
biased assessment of the final model’s performance. Without this
separation, you risk overfitting, where your model memorizes the
training data rather than learning generalizable patterns.</p>
<p>If you are using out-of-the-box models, evaluation is also mandatory,
as you must be sure that the outputs you obtain behave in the manner
that you would expect. In this case you only need to design the test set
(with human annotators) and measure the performance. <strong>It doesn’t
matter how broadly a model was tested, you must always confirm that it
is suitable for your intended usage</strong>. It is also not enough to
use semi-automated evaluation, designing human-labeled data will also
provide insightful information for shaping your own task and judging the
predictions you get.</p>
</div>
</div>
</div>
<div class="section level3">
<h3 id="evaluation-metrics">Evaluation Metrics<a class="anchor" aria-label="anchor" href="#evaluation-metrics"></a></h3>
<p>Several metrics are commonly used to evaluate NLP models, each
offering different insights into performance. Here we will describe the
4 most used in suervised learning classifiers</p>
<p>Let’s give the toy example of classifying photographs of cats and
dogs. In this task, there are 2 classes: <code>cat</code> and
<code>dog</code>, and we have a model that return the label
<code>cat</code> or <code>dog</code> given an input photograph. If we
want to evaluate how good is our model at recognizing <code>dog</code>s
(our <em>positive class</em> in this example), there are 4 possibilities
when comparing with the ground truth (your labeled data):</p>
<ol style="list-style-type: decimal"><li>
<strong>True Positives (TP):</strong> the number of <code>dog</code>
photographs the model <em>correctly</em> labeled as
<code>dog</code>.</li>
<li>
<strong>True Negatives (TN):</strong> the number of
non-<code>dog</code> photographs <em>correctly</em> labeled as something
else.</li>
<li>
<strong>False Positives (FP):</strong> the number of
non-<code>dog</code> photographs the model <em>mistakenly</em> labeled
as <code>dog</code>.</li>
<li>
<strong>False Negatives (FN):</strong> the number of
<code>dog</code> photographs <em>mistakenly</em> labeled as something
else.</li>
</ol><p>Based on this simple counts, we can derive four metrics that inform
us at scale about the performance of our classifiers:</p>
<ul><li>
<strong>Accuracy:</strong> measures the global proportion of correct
predictions, regardless of the class they hold. This is al true cases
(TP + TN) divided by all tested instances (TP+TN+FP+FN).</li>
<li>
<strong>Precision:</strong> This answers the following question: “Of
all the predictions the model made for a particular class, how many were
actually correct?”. This is TP divided by (TP+FP).</li>
<li>
<strong>Recall:</strong> This answers the following question: “Of
all the actual instances of a class, how many did the model successfully
identify?”. This is TP divided by (TP+FN).</li>
<li>
<strong>F1-score:</strong> provides a harmonic mean of precision and
recall, offering a single metric that balances both concerns.</li>
</ul><p>Deciding which metric is the most relevant to your case depends on
your specific task, but having a view at all of the metrics is always
insightful.</p>
<figure><img src="../fig/bert_precisionRecall.png" alt="An example for a classifier of Cats and Dogs. Source: Wikipedia" class="figure mx-auto d-block"><div class="figcaption">An example for a classifier of Cats and Dogs.
Source: Wikipedia</div>
</figure><p><strong>It’s important to remember that a high accuracy score doesn’t
always indicate a good model</strong>. For example, if you’re
classifying rare events that occur only 5% of the time, a naive model
that always predicts “no event” would achieve 95% accuracy while being
completely useless. This is why examining multiple metrics and
understanding your data’s characteristics is essential for proper model
evaluation.</p>
<p>In Python, the <code>scikit-learn</code> package already provides us
with these (and many other) evaluation metrics. All we need to do is
prepare an ordered list with the <code>true_labels</code> and a list
with the corresponding <code>predicted_labels</code> for each example in
our data.</p>
<p>To illustrate the usage of evaluation, we will use a simpler
sentiment model that predicts 5 classes: <code>Very positive</code>,
<code>positive</code>, <code>neutral</code>, <code>negative</code> and
<code>very negative</code>. Here is an example of the model predictions
for four toy examples:</p>
<div class="codewrapper sourceCode" id="cb21">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb21-2"><a href="#cb21-2" tabindex="-1"></a></span>
<span id="cb21-3"><a href="#cb21-3" tabindex="-1"></a><span class="co"># Load the classification pipeline with the specified model</span></span>
<span id="cb21-4"><a href="#cb21-4" tabindex="-1"></a>pipe <span class="op">=</span> pipeline(<span class="st">"text-classification"</span>, model<span class="op">=</span><span class="st">"tabularisai/multilingual-sentiment-analysis"</span>)</span>
<span id="cb21-5"><a href="#cb21-5" tabindex="-1"></a></span>
<span id="cb21-6"><a href="#cb21-6" tabindex="-1"></a><span class="co"># Classify a new sentence</span></span>
<span id="cb21-7"><a href="#cb21-7" tabindex="-1"></a>sentences <span class="op">=</span> [</span>
<span id="cb21-8"><a href="#cb21-8" tabindex="-1"></a>    <span class="st">"I love this product! It's amazing and works perfectly."</span>,</span>
<span id="cb21-9"><a href="#cb21-9" tabindex="-1"></a>    <span class="st">"The movie was a bit boring, I could predict the ending since minute 1."</span>,</span>
<span id="cb21-10"><a href="#cb21-10" tabindex="-1"></a>    <span class="st">"Mary Shelley wrote this book around 1816."</span>,</span>
<span id="cb21-11"><a href="#cb21-11" tabindex="-1"></a>    <span class="st">"Everything suuuucks!"</span></span>
<span id="cb21-12"><a href="#cb21-12" tabindex="-1"></a>]</span>
<span id="cb21-13"><a href="#cb21-13" tabindex="-1"></a>gold_labels <span class="op">=</span> [</span>
<span id="cb21-14"><a href="#cb21-14" tabindex="-1"></a>    <span class="st">"Very Positive"</span>,</span>
<span id="cb21-15"><a href="#cb21-15" tabindex="-1"></a>    <span class="st">"Negative"</span>,</span>
<span id="cb21-16"><a href="#cb21-16" tabindex="-1"></a>    <span class="st">"Neutral"</span>,</span>
<span id="cb21-17"><a href="#cb21-17" tabindex="-1"></a>    <span class="st">"Very Negative"</span></span>
<span id="cb21-18"><a href="#cb21-18" tabindex="-1"></a>]</span>
<span id="cb21-19"><a href="#cb21-19" tabindex="-1"></a></span>
<span id="cb21-20"><a href="#cb21-20" tabindex="-1"></a><span class="co"># The pipeline can also run on a batch of examples</span></span>
<span id="cb21-21"><a href="#cb21-21" tabindex="-1"></a>result <span class="op">=</span> pipe(sentences)</span>
<span id="cb21-22"><a href="#cb21-22" tabindex="-1"></a></span>
<span id="cb21-23"><a href="#cb21-23" tabindex="-1"></a><span class="co"># Print the result</span></span>
<span id="cb21-24"><a href="#cb21-24" tabindex="-1"></a>predicted_labels <span class="op">=</span> []</span>
<span id="cb21-25"><a href="#cb21-25" tabindex="-1"></a><span class="cf">for</span> res <span class="kw">in</span> result:</span>
<span id="cb21-26"><a href="#cb21-26" tabindex="-1"></a>    <span class="bu">print</span>(res)</span>
<span id="cb21-27"><a href="#cb21-27" tabindex="-1"></a>    predicted_labels.append(res[<span class="st">'label'</span>])</span></code></pre>
</div>
<pre><code>{'label': 'Very Positive', 'score': 0.5586304068565369}
{'label': 'Negative', 'score': 0.9448591470718384}
{'label': 'Neutral', 'score': 0.9229359030723572}
{'label': 'Very Negative', 'score': 0.36225152015686035}</code></pre>
<p>We can see that the model predicts correctly the 4 examples we gave.
This is unsurprising as they are incredibly obvious examples. We can
also print the results and inspect them because they are only 4
instances, but it is clearly not a scalable approach.</p>
<div id="callout8" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div class="callout-inner">
<div class="callout-content">
<p>Note that many models will provide a confidence <code>score</code>,
with their predictions. It is very tempting to interpret these scores as
a proxy to “how certain is the model of prediction X”. However, you
should be very careful, this score is only a relative confidence measure
with respect to the training data, and it does not always translate well
to unseen data. Most of the times it is better to just ignore it,
especially if it is a model that you didn’t train yourself.</p>
</div>
</div>
</div>
<p>We can obtain an automated evaluation report, including the basic
evaluation metrics, from <code>scikit-learn</code> by calling:</p>
<div class="codewrapper sourceCode" id="cb23">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> classification_report</span>
<span id="cb23-2"><a href="#cb23-2" tabindex="-1"></a></span>
<span id="cb23-3"><a href="#cb23-3" tabindex="-1"></a><span class="bu">print</span>(classification_report(y_true<span class="op">=</span>gold_labels, y_pred<span class="op">=</span>predicted_labels))</span></code></pre>
</div>
<p>The output is a table showing a breakdown of metrics by class. The
column <code>support</code> shows how many examples per class were
present in the gold data.</p>
<pre><code>               precision    recall  f1-score   support

     Negative       1.00      1.00      1.00         1
      Neutral       1.00      1.00      1.00         1
Very Negative       1.00      1.00      1.00         1
Very Positive       1.00      1.00      1.00         1

     accuracy                           1.00         4
    macro avg       1.00      1.00      1.00         4
 weighted avg       1.00      1.00      1.00         4</code></pre>
<p>These 4 metrics range from 0 to 1 (note that sometimes people
multiply the scores by 100 to gain more granularity with decimal
places). In this case, because we had a <em>perfect</em> score
everything amounts to 1. In the most catastrophic scenario all scores
would be zero.</p>
<div id="evaluate-sentiment-classifier" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="evaluate-sentiment-classifier" class="callout-inner">
<h3 class="callout-title">Evaluate Sentiment Classifier</h3>
<div class="callout-content">
<p>Now it is time to scale things a little bit more… Use the same
pipeline from the given toy example to run predictions over 100 examples
of short book reviews. Then print the classification report for the
given <em>test set</em>. These examples are given in the
<code>data/sentiment_film_data.tsv</code> file.</p>
<p>You can use the following helper functions, the first one helps you
read the file and the second one normalizes the 5-class predictions into
the 3-class annotations given in the test set:</p>
<div class="codewrapper sourceCode" id="cb25">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" tabindex="-1"></a><span class="kw">def</span> load_data(filename):</span>
<span id="cb25-2"><a href="#cb25-2" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(filename, <span class="st">'r'</span>) <span class="im">as</span> f:</span>
<span id="cb25-3"><a href="#cb25-3" tabindex="-1"></a>        lines <span class="op">=</span> f.readlines()[<span class="dv">1</span>:] <span class="co"># skip header</span></span>
<span id="cb25-4"><a href="#cb25-4" tabindex="-1"></a>    sentences, labels <span class="op">=</span> <span class="bu">zip</span>(<span class="op">*</span>(line.strip().split(<span class="st">'</span><span class="ch">\t</span><span class="st">'</span>) <span class="cf">for</span> line <span class="kw">in</span> lines))</span>
<span id="cb25-5"><a href="#cb25-5" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">list</span>(sentences), <span class="bu">list</span>(labels)</span>
<span id="cb25-6"><a href="#cb25-6" tabindex="-1"></a></span>
<span id="cb25-7"><a href="#cb25-7" tabindex="-1"></a><span class="kw">def</span> get_normalized_labels(predictions):</span>
<span id="cb25-8"><a href="#cb25-8" tabindex="-1"></a>    <span class="co"># predicitons is a list with dicts such as {'label': 'positive', 'score': 0.95}</span></span>
<span id="cb25-9"><a href="#cb25-9" tabindex="-1"></a>    <span class="co"># We also need to normalize the labels to match the true labels (which are only 'positive' and 'negative')</span></span>
<span id="cb25-10"><a href="#cb25-10" tabindex="-1"></a>    normalized <span class="op">=</span> []</span>
<span id="cb25-11"><a href="#cb25-11" tabindex="-1"></a>    <span class="cf">for</span> pred <span class="kw">in</span> predictions:</span>
<span id="cb25-12"><a href="#cb25-12" tabindex="-1"></a>        label <span class="op">=</span> pred[<span class="st">'label'</span>].lower()</span>
<span id="cb25-13"><a href="#cb25-13" tabindex="-1"></a>        <span class="cf">if</span> <span class="st">'positive'</span> <span class="kw">in</span> label:</span>
<span id="cb25-14"><a href="#cb25-14" tabindex="-1"></a>            normalized.append(<span class="st">'positive'</span>)</span>
<span id="cb25-15"><a href="#cb25-15" tabindex="-1"></a>        <span class="cf">elif</span> <span class="st">'negative'</span> <span class="kw">in</span> label:</span>
<span id="cb25-16"><a href="#cb25-16" tabindex="-1"></a>            normalized.append(<span class="st">'negative'</span>)</span>
<span id="cb25-17"><a href="#cb25-17" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb25-18"><a href="#cb25-18" tabindex="-1"></a>            normalized.append(<span class="st">'neutral'</span>)</span>
<span id="cb25-19"><a href="#cb25-19" tabindex="-1"></a>    <span class="cf">return</span> normalized</span></code></pre>
</div>
</div>
</div>
</div>
<div id="accordionSolution4" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution4" aria-expanded="false" aria-controls="collapseSolution4">
  <h4 class="accordion-header" id="headingSolution4"> Show me the solution </h4>
</button>
<div id="collapseSolution4" class="accordion-collapse collapse" data-bs-parent="#accordionSolution4" aria-labelledby="headingSolution4">
<div class="accordion-body">
<div class="codewrapper sourceCode" id="cb26">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> classification_report, precision_recall_fscore_support</span>
<span id="cb26-2"><a href="#cb26-2" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb26-3"><a href="#cb26-3" tabindex="-1"></a></span>
<span id="cb26-4"><a href="#cb26-4" tabindex="-1"></a>sentences, labels <span class="op">=</span> load_data(<span class="st">'data/sentiment_film_data.tsv'</span>)</span>
<span id="cb26-5"><a href="#cb26-5" tabindex="-1"></a><span class="co"># The labels from our dataset</span></span>
<span id="cb26-6"><a href="#cb26-6" tabindex="-1"></a>y_true <span class="op">=</span> labels</span>
<span id="cb26-7"><a href="#cb26-7" tabindex="-1"></a><span class="co"># Run the model to get predictions per sentence</span></span>
<span id="cb26-8"><a href="#cb26-8" tabindex="-1"></a>y_pred <span class="op">=</span> pipe(sentences)</span>
<span id="cb26-9"><a href="#cb26-9" tabindex="-1"></a><span class="co"># Normalize the labels to match the gold standard</span></span>
<span id="cb26-10"><a href="#cb26-10" tabindex="-1"></a>y_pred <span class="op">=</span> get_normalized_labels(y_pred)</span>
<span id="cb26-11"><a href="#cb26-11" tabindex="-1"></a></span>
<span id="cb26-12"><a href="#cb26-12" tabindex="-1"></a><span class="co"># Detailed report with all metrics</span></span>
<span id="cb26-13"><a href="#cb26-13" tabindex="-1"></a><span class="bu">print</span>(classification_report(y_true, y_pred))</span></code></pre>
</div>
<p>Here is the classification report:</p>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>              precision    recall  f1-score   support

    negative       0.57      1.00      0.73        23
     neutral       0.53      0.22      0.31        37
    positive       0.69      0.78      0.73        40

    accuracy                           0.62       100
   macro avg       0.60      0.66      0.59       100
weighted avg       0.61      0.62      0.57       100</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="confusion-matrix">Confusion Matrix<a class="anchor" aria-label="anchor" href="#confusion-matrix"></a></h3>
<p>The confusion matrix is another direct and informative tool for
understanding your model’s performance, by offering an intuitive
visualization of your model’s behavior in detail. It is a table that
compares your model’s predictions against the true labels. The
<strong>rows typically represent the actual classes, while the columns
show the predicted classes</strong>. Each cell contains the count of
instances that fall into that particular combination of true and
predicted labels. Perfect predictions would result in all counts
appearing along the diagonal of the matrix, with zeros everywhere
else.</p>
<div class="codewrapper sourceCode" id="cb28">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> ConfusionMatrixDisplay</span>
<span id="cb28-2"><a href="#cb28-2" tabindex="-1"></a></span>
<span id="cb28-3"><a href="#cb28-3" tabindex="-1"></a><span class="kw">def</span> show_confusion_matrix(y_true, y_pred, labels<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb28-4"><a href="#cb28-4" tabindex="-1"></a>    ConfusionMatrixDisplay.from_predictions(y_true, y_pred, display_labels<span class="op">=</span>labels, cmap<span class="op">=</span><span class="st">'Blues'</span>)</span>
<span id="cb28-5"><a href="#cb28-5" tabindex="-1"></a>    plt.show()</span>
<span id="cb28-6"><a href="#cb28-6" tabindex="-1"></a></span>
<span id="cb28-7"><a href="#cb28-7" tabindex="-1"></a>show_confusion_matrix(y_true, y_pred)</span></code></pre>
</div>
<p>This code shows the following matrix:</p>
<figure><img src="../fig/bert_sentiment_matrix.png" class="figure mx-auto d-block"></figure><p>Some of the insights from a confusion matrix include:</p>
<ul><li>Observe which classes your model handles well and which ones it
struggles with.</li>
<li>Observe confusion patterns between classes. By examining the
off-diagonal cells, you can identify systematic errors your model makes.
For example, perhaps your sentiment classifier consistently confuses
neutral statements with positive ones, but rarely mistakes neutral for
negative.</li>
<li>Detect Bias, for example by exposing a tendency to over-predict
certain classes while ignoring others.</li>
<li>Detect class imbalance. Even if your overall accuracy seems high,
the confusion matrix might reveal that your model achieves this by
simply predicting the majority class most of the time.</li>
</ul></div>
</section><section><h2 class="section-heading" id="bert-for-token-classification">BERT for Token Classification<a class="anchor" aria-label="anchor" href="#bert-for-token-classification"></a></h2>
<hr class="half-width"><p>Just as we plugged in a trainable text classifier layer, we can add a
token-level classifier that assigns a class to each of the tokens
encoded by a transformer (as opposed to one label for the whole
sequence). A specific example of this task is Named Entity Recognition,
but you can basically define any task that requires to
<em>highlight</em> sub-strings of text and classify them using this
technique.</p>
<div class="section level3">
<h3 id="named-entity-recognition">Named Entity Recognition<a class="anchor" aria-label="anchor" href="#named-entity-recognition"></a></h3>
<p>Named Entity Recognition (NER) is the task of recognizing mentions of
real-world entities inside a text. The concept of
<strong>Entity</strong> includes proper names that unequivocally
identify a unique individual (PER), place (LOC), organization (ORG), or
other object/name (MISC). Depending on the domain, the concept can
expanded to recognize other unique (and more conceptual) entities such
as DATE, MONEY, WORK_OF_ART, DISEASE, PROTEIN_TYPE, etcetera…</p>
<p>In terms of NLP, this boils down to classifying each token into a
series of labels (<code>PER</code>, <code>LOC</code>, <code>ORG</code>,
<code>O</code>[no-entity] ). Since a single entity can be expressed with
multiple words (e.g. New York) the usual notation used for labeling the
text is IOB (<strong>I</strong>nner <strong>O</strong>ut
<strong>B</strong>eginnig of entity) notations which identifies the
limits of each entity tokens. For example:</p>
<figure><img src="../fig/bert5.png" alt="BERT as an NER Classifier" class="figure mx-auto d-block"><div class="figcaption">BERT as an NER Classifier</div>
</figure><p>This is a typical sequence classification problem where an imput
sequence must be fully mapped into an output sequence of labels with
global constraints (for example, there can’t be an inner I-LOC label
before a beginning B-LOC label). Since the labels of the tokens are
context dependent, a language model with attention mechanism such as
BERT is very beneficial for a task like NER.</p>
<p>Because this is one of the core tasks in NLP, there are dozens of
pre-trained NER classifiers available in HuggingFace that you can use
right away. We use once again the <code>pipeline()</code> wrapper to
directly run the model for predictions , in this case with
<code>task="ner"</code>. For example:</p>
<div class="codewrapper sourceCode" id="cb29">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer, AutoModelForTokenClassification</span>
<span id="cb29-2"><a href="#cb29-2" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb29-3"><a href="#cb29-3" tabindex="-1"></a></span>
<span id="cb29-4"><a href="#cb29-4" tabindex="-1"></a><span class="co"># We can also pass to the pipeline the initialized model &amp; tokenizer. </span></span>
<span id="cb29-5"><a href="#cb29-5" tabindex="-1"></a><span class="co"># This way we have access to both separately if we need them later</span></span>
<span id="cb29-6"><a href="#cb29-6" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(<span class="st">"dslim/bert-base-NER"</span>)</span>
<span id="cb29-7"><a href="#cb29-7" tabindex="-1"></a>model <span class="op">=</span> AutoModelForTokenClassification.from_pretrained(<span class="st">"dslim/bert-base-NER"</span>)</span>
<span id="cb29-8"><a href="#cb29-8" tabindex="-1"></a></span>
<span id="cb29-9"><a href="#cb29-9" tabindex="-1"></a>ner_classifier <span class="op">=</span> pipeline(<span class="st">"token-classification"</span>, model<span class="op">=</span>model, tokenizer<span class="op">=</span>tokenizer, aggregation_strategy<span class="op">=</span><span class="st">"first"</span>)</span>
<span id="cb29-10"><a href="#cb29-10" tabindex="-1"></a>example <span class="op">=</span> <span class="st">"My name is Wolfgang Schmid and I live in Berlin"</span></span>
<span id="cb29-11"><a href="#cb29-11" tabindex="-1"></a></span>
<span id="cb29-12"><a href="#cb29-12" tabindex="-1"></a>ner_results <span class="op">=</span> ner_classifier(example)</span>
<span id="cb29-13"><a href="#cb29-13" tabindex="-1"></a><span class="cf">for</span> nr <span class="kw">in</span> ner_results:</span>
<span id="cb29-14"><a href="#cb29-14" tabindex="-1"></a>    <span class="bu">print</span>(nr)</span></code></pre>
</div>
<p>The code now prints the following:</p>
<pre><code>{'entity_group': 'PER', 'score': 0.9995944, 'word': 'Wolfgang Schmid', 'start': 11, 'end': 26}
{'entity_group': 'LOC', 'score': 0.99956733, 'word': 'Berlin', 'start': 41, 'end': 47}</code></pre>
<p>As you can see, the outputs contain already the entities aggregated
at the Span Leven (instead of the Token Level). Word pieces are merged
back into <em>human words</em> and also multiword entities are assigned
a single entity unified label. Depending on your use case you can
request the pipeline to give different
<code>aggregation_strateg[ies]</code>. More info about the pipeline can
be found <a href="https://huggingface.co/docs/transformers/main_classes/pipelines" class="external-link">here</a>.</p>
<p>The same evaluation metrics you learned for the text classifier can
also be applied to this classification task. A common Python library to
deal with token classification is <a href="https://github.com/chakki-works/seqeval" class="external-link">seqeval</a> or <a href="https://github.com/huggingface/evaluate?tab=readme-ov-file" class="external-link">evaluate</a>.
Remember to always test on a significant human-labeled dataset to assess
if the predictions you are getting make sense. If they don’t, more
advances use of the models, including fine-tuning should be used.</p>
<p>What did we learn in this lesson?</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<span class="callout-header">Key Points</span>
<div class="callout-inner">
<div class="callout-content">
<ul><li><p>Static word representations, such as word2vec, still lack of
enough context to do more advanced tasks, we made this weakness evident
by studying polysemy.</p></li>
<li><p>The transformer architecture consists of three main components:
an <strong>Encoder</strong> to create powerful text representations
(embeddings), an <strong>Attention Mechanism</strong> to learn more from
the full sequence context, and a <strong>Decoder</strong>, a generative
model to predicts the next token based on the context it has so
far.</p></li>
<li><p>BERT is a deep encoder that creates rich contextualized
representations of words and sentences. These representations are very
powerful features that can be re-used by other machine Learning and deep
learning models.</p></li>
<li><p>Several of the core NLP tasks can be solved using
Transformer-based models. In this episode we covered language modeling
(fill-in the mask), text classification (sentiment analysis) and token
classification (named entity recognition).</p></li>
<li><p>Evaluating the model performance using your own data <strong>for
your own use case</strong> is crucial to understand possible drawbacks
when using this model for unknown predictions</p></li>
</ul></div>
</div>
</div>
</div>
</section></div> <!-- / div.lesson-content -->
    </main><!-- / main#main-content.main-content --><nav class="bottom-pagination mx-md-4" aria-label="Previous and Next Chapter"><div class="d-block d-sm-block d-md-none">
        <a class="chapter-link" href="../instructor/02-word_representations.html"><i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>Previous</a>
        <a class="chapter-link float-end" href="../instructor/04-LargeLanguageModels.html">Next<i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i></a>
      </div>
      <!-- content for large screens -->
      <div class="d-none d-sm-none d-md-block">
        <a class="chapter-link" href="../instructor/02-word_representations.html" rel="prev">
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>
          Previous: From words to
        </a>
        <a class="chapter-link float-end" href="../instructor/04-LargeLanguageModels.html" rel="next">
          Next: Using large language...
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i>
        </a>
      </div>
    </nav></div> <!-- / div.primary-content.col-xs-12 -->
<!-- END:   inst/pkgdown/templates/content-instructor.html-->

      </div><!--/div.row-->
      		<footer class="row footer mx-md-3"><hr><div class="col-md-6">
        <p>This lesson is subject to the <a href="CODE_OF_CONDUCT.html">Code of Conduct</a></p>
        <p>

        <a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/edit/main/episodes/03-transformers.md" class="external-link">Edit on GitHub</a>

	
        | <a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/blob/main/CONTRIBUTING.md" class="external-link">Contributing</a>
        | <a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/" class="external-link">Source</a></p>
				<p><a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/blob/main/CITATION.cff" class="external-link">Cite</a> | <a href="mailto:team@carpentries.org">Contact</a> | <a href="https://carpentries.org/about/" class="external-link">About</a></p>
			</div>
			<div class="col-md-6">

        <p>Materials licensed under <a href="LICENSE.html">CC-BY 4.0</a> by the authors</p>

        <p>Template licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/" class="external-link">CC-BY 4.0</a> by <a href="https://carpentries.org/" class="external-link">The Carpentries</a></p>
        <p>Built with <a href="https://github.com/carpentries/sandpaper/tree/0.17.1" class="external-link">sandpaper (0.17.1)</a>, <a href="https://github.com/carpentries/pegboard/tree/0.7.9" class="external-link">pegboard (0.7.9)</a>, and <a href="https://github.com/carpentries/varnish/tree/1.0.7" class="external-link">varnish (1.0.7)</a></p>
			</div>
		</footer></div> <!-- / div.container -->
	<div id="to-top">
		<a href="#top">
      <i class="search-icon" data-feather="arrow-up" role="img" aria-label="Back To Top"></i><br><!-- <span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top --><span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top
		</a>
	</div>
  <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "LearningResource",
  "@id": "https://esciencecenter-digital-skills.github.io/Natural-language-processing/instructor/03-transformers.html",
  "inLanguage": "en",
  "dct:conformsTo": "https://bioschemas.org/profiles/LearningResource/1.0-RELEASE",
  "description": "A Carpentries Lesson teaching foundational data and coding skills to researchers worldwide",
  "keywords": "software, data, lesson, The Carpentries, NLP, English, social sciences, pre-alpha",
  "name": "Transformers: BERT and Beyond",
  "creativeWorkStatus": "active",
  "url": "https://esciencecenter-digital-skills.github.io/Natural-language-processing/instructor/03-transformers.html",
  "identifier": "https://esciencecenter-digital-skills.github.io/Natural-language-processing/instructor/03-transformers.html",
  "dateCreated": "2024-04-24",
  "dateModified": "2025-12-01",
  "datePublished": "2025-12-02"
}

  </script><script>
		feather.replace();
	</script></body></html><!-- END:   inst/pkgdown/templates/layout.html-->

