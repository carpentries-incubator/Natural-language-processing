<!DOCTYPE html>
<!-- START: inst/pkgdown/templates/layout.html --><!-- Generated by pkgdown: do not edit by hand --><html lang="en" data-bs-theme="auto"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><title>Fundamentals of Natural Language Processing (NLP) in Python: Introduction</title><meta name="viewport" content="width=device-width, initial-scale=1"><script src="../assets/themetoggle.js"></script><link rel="stylesheet" type="text/css" href="../assets/styles.css"><script src="../assets/scripts.js" type="text/javascript"></script><!-- mathjax --><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      config: ["MMLorHTML.js"],
      jax: ["input/TeX","input/MathML","output/HTML-CSS","output/NativeMML", "output/PreviewHTML"],
      extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
      TeX: {
        extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
      },
      tex2jax: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [ ['$$','$$'], ['\\[', '\\]'] ],
        processEscapes: true
      }
    });
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><!-- Responsive Favicon for The Carpentries --><link rel="apple-touch-icon" sizes="180x180" href="../favicons/incubator/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="../favicons/incubator/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="../favicons/incubator/favicon-16x16.png"><link rel="manifest" href="../favicons/incubator/site.webmanifest"><link rel="mask-icon" href="../favicons/incubator/safari-pinned-tab.svg" color="#5bbad5"><meta name="msapplication-TileColor" content="#da532c"><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"><meta name="theme-color" media="(prefers-color-scheme: dark)" content="black"></head><body>
    <header id="top" class="navbar navbar-expand-md top-nav incubator"><svg xmlns="http://www.w3.org/2000/svg" class="d-none"><symbol id="check2" viewbox="0 0 16 16"><path d="M13.854 3.646a.5.5 0 0 1 0 .708l-7 7a.5.5 0 0 1-.708 0l-3.5-3.5a.5.5 0 1 1 .708-.708L6.5 10.293l6.646-6.647a.5.5 0 0 1 .708 0z"></path></symbol><symbol id="circle-half" viewbox="0 0 16 16"><path d="M8 15A7 7 0 1 0 8 1v14zm0 1A8 8 0 1 1 8 0a8 8 0 0 1 0 16z"></path></symbol><symbol id="moon-stars-fill" viewbox="0 0 16 16"><path d="M6 .278a.768.768 0 0 1 .08.858 7.208 7.208 0 0 0-.878 3.46c0 4.021 3.278 7.277 7.318 7.277.527 0 1.04-.055 1.533-.16a.787.787 0 0 1 .81.316.733.733 0 0 1-.031.893A8.349 8.349 0 0 1 8.344 16C3.734 16 0 12.286 0 7.71 0 4.266 2.114 1.312 5.124.06A.752.752 0 0 1 6 .278z"></path><path d="M10.794 3.148a.217.217 0 0 1 .412 0l.387 1.162c.173.518.579.924 1.097 1.097l1.162.387a.217.217 0 0 1 0 .412l-1.162.387a1.734 1.734 0 0 0-1.097 1.097l-.387 1.162a.217.217 0 0 1-.412 0l-.387-1.162A1.734 1.734 0 0 0 9.31 6.593l-1.162-.387a.217.217 0 0 1 0-.412l1.162-.387a1.734 1.734 0 0 0 1.097-1.097l.387-1.162zM13.863.099a.145.145 0 0 1 .274 0l.258.774c.115.346.386.617.732.732l.774.258a.145.145 0 0 1 0 .274l-.774.258a1.156 1.156 0 0 0-.732.732l-.258.774a.145.145 0 0 1-.274 0l-.258-.774a1.156 1.156 0 0 0-.732-.732l-.774-.258a.145.145 0 0 1 0-.274l.774-.258c.346-.115.617-.386.732-.732L13.863.1z"></path></symbol><symbol id="sun-fill" viewbox="0 0 16 16"><path d="M8 12a4 4 0 1 0 0-8 4 4 0 0 0 0 8zM8 0a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 0zm0 13a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 13zm8-5a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2a.5.5 0 0 1 .5.5zM3 8a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2A.5.5 0 0 1 3 8zm10.657-5.657a.5.5 0 0 1 0 .707l-1.414 1.415a.5.5 0 1 1-.707-.708l1.414-1.414a.5.5 0 0 1 .707 0zm-9.193 9.193a.5.5 0 0 1 0 .707L3.05 13.657a.5.5 0 0 1-.707-.707l1.414-1.414a.5.5 0 0 1 .707 0zm9.193 2.121a.5.5 0 0 1-.707 0l-1.414-1.414a.5.5 0 0 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .707zM4.464 4.465a.5.5 0 0 1-.707 0L2.343 3.05a.5.5 0 1 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .708z"></path></symbol></svg><a class="visually-hidden-focusable skip-link" href="#main-content">Skip to main content</a>
  <div class="container-fluid top-nav-container">
    <div class="col-md-8">
      <div class="large-logo">
        <img id="incubator-logo" alt="Carpentries Incubator" src="../assets/images/incubator-logo.svg"><span class="badge text-bg-danger">
          <abbr title="This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught.">
            <a href="https://docs.carpentries.org/resources/curriculum/lesson-life-cycle.html" class="external-link alert-link">
              <i aria-hidden="true" class="icon" data-feather="alert-octagon" style="border-radius: 5px"></i>
              Pre-Alpha
            </a>
            <span class="visually-hidden">This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught.</span>
          </abbr>
        </span>

      </div>
    </div>
    <div class="selector-container">
      <div id="theme-selector">
        <li class="nav-item dropdown" id="theme-button-list">
          <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
            <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="bd-theme-text"><li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                Light
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                Dark
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                Auto
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
          </ul></li>
      </div>

      <div class="dropdown" id="instructor-dropdown">
        <button class="btn btn-secondary dropdown-toggle bordered-button" type="button" id="dropdownMenu1" data-bs-toggle="dropdown" aria-expanded="false">
          <i aria-hidden="true" class="icon" data-feather="eye"></i> Instructor View <i data-feather="chevron-down"></i>
        </button>
        <ul class="dropdown-menu" aria-labelledby="dropdownMenu1"><li><button class="dropdown-item" type="button" onclick="window.location.href='../01-introduction.html';">Learner View</button></li>
        </ul></div>
    </div>
  </div>
  <hr></header><nav class="navbar navbar-expand-xl bottom-nav incubator" aria-label="Main Navigation"><div class="container-fluid nav-container">
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle Navigation">
      <span class="navbar-toggler-icon"></span>
      <span class="menu-title">Menu</span>
    </button>
    <div class="nav-logo">
      <img class="small-logo" alt="Carpentries Incubator" src="../assets/images/incubator-logo-sm.svg"></div>
    <div class="lesson-title-md">
      Fundamentals of Natural Language Processing (NLP) in Python
    </div>
    <div class="search-icon-sm">
      <!-- TODO: do not show until we have search
        <i role="img" aria-label="Search the All In One page" data-feather="search"></i>
      -->
    </div>
    <div class="desktop-nav">
      <ul class="navbar-nav me-auto mb-2 mb-lg-0"><li class="nav-item">
          <span class="lesson-title">
            Fundamentals of Natural Language Processing (NLP) in Python
          </span>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/key-points.html">Key Points</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/instructor-notes.html">Instructor Notes</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/images.html">Extract All Images</a>
        </li>
        <li class="nav-item dropdown">
          <button class="nav-link dropdown-toggle" id="navbarDropdown" data-bs-toggle="dropdown" aria-expanded="false">
            More <i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu" aria-labelledby="navbarDropdown"><hr></ul></li>
      </ul></div>
    <!--
    <form class="d-flex col-md-2 search-form">
      <fieldset disabled>
      <input class="form-control me-2 searchbox" type="search" placeholder="" aria-label="">
        <button class="btn btn-outline-success tablet-search-button"  type="submit">
          <i class="search-icon" data-feather="search" role="img" aria-label="Search the All In One page"></i>
        </button>
      </fieldset>
    </form>
    -->
    <a id="search-button" class="btn btn-primary" href="../instructor/aio.html" role="button" aria-label="Search the All In One page">Search the All In One page</a>
  </div><!--/div.container-fluid -->
</nav><div class="col-md-12 mobile-title">
  Fundamentals of Natural Language Processing (NLP) in Python
</div>

<aside class="col-md-12 lesson-progress"><div style="width: 0%" class="percentage">
    0%
  </div>
  <div class="progress incubator">
    <div class="progress-bar incubator" role="progressbar" style="width: 0%" aria-valuenow="0" aria-label="Lesson Progress" aria-valuemin="0" aria-valuemax="100">
    </div>
  </div>
</aside><div class="container">
      <div class="row">
        <!-- START: inst/pkgdown/templates/navbar.html -->
<div id="sidebar-col" class="col-lg-4">
  <div id="sidebar" class="sidebar">
      <nav aria-labelledby="flush-headingEleven"><button role="button" aria-label="close menu" alt="close menu" aria-expanded="true" aria-controls="sidebar" class="collapse-toggle" data-collapse="Collapse " data-episodes="Episodes ">
          <i class="search-icon" data-feather="x" role="img"></i>
        </button>
        <div class="sidebar-inner">
          <div class="row mobile-row" id="theme-row-mobile">
            <div class="col" id="theme-selector">
              <li class="nav-item dropdown" id="theme-button-list">
                <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
                  <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><span class="d-lg-none ms-1" id="bd-theme-text">Toggle Theme</span>
                </button>
                <ul class="dropdown-menu dropdown-menu-right" aria-labelledby="bd-theme-text"><li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                      Light
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                      Dark
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                      <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                      Auto
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                </ul></li>
            </div>
          </div>
          <div class="row mobile-row">
            <div class="col">
              <div class="sidenav-view-selector">
                <div class="accordion accordion-flush" id="accordionFlush9">
                  <div class="accordion-item">
                    <h2 class="accordion-header" id="flush-headingNine">
                      <button class="accordion-button collapsed" id="instructor" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseNine" aria-expanded="false" aria-controls="flush-collapseNine">
                        <i id="eye" aria-hidden="true" class="icon" data-feather="eye"></i> Instructor View
                      </button>
                    </h2>
                    <div id="flush-collapseNine" class="accordion-collapse collapse" aria-labelledby="flush-headingNine" data-bs-parent="#accordionFlush2">
                      <div class="accordion-body">
                        <a href="../01-introduction.html">Learner View</a>
                      </div>
                    </div>
                  </div><!--/div.accordion-item-->
                </div><!--/div.accordion-flush-->
              </div><!--div.sidenav-view-selector -->
            </div><!--/div.col -->

            <hr></div><!--/div.mobile-row -->

          <div class="accordion accordion-flush" id="accordionFlush11">
            <div class="accordion-item">

              <button id="chapters" class="accordion-button show" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseEleven" aria-expanded="false" aria-controls="flush-collapseEleven">
                <h2 class="accordion-header chapters" id="flush-headingEleven">
                  EPISODES
                </h2>
              </button>
              <div id="flush-collapseEleven" class="accordion-collapse show collapse" aria-labelledby="flush-headingEleven" data-bs-parent="#accordionFlush11">

                <div class="accordion-body">
                  <div class="accordion accordion-flush" id="accordionFlush1">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading1">
        <a href="index.html">Summary and Schedule</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlushcurrent">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-headingcurrent">
      <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapsecurrent" aria-expanded="true" aria-controls="flush-collapsecurrent">
        <span class="visually-hidden">Current Chapter</span>
        <span class="current-chapter">
        1. Introduction
        </span>
      </button>
    </div><!--/div.accordion-header-->

    <div id="flush-collapsecurrent" class="accordion-collapse collapse show" aria-labelledby="flush-headingcurrent" data-bs-parent="#accordionFlushcurrent">
      <div class="accordion-body">
        <ul><li><a href="#what-is-nlp">What is NLP?</a></li>
<li><a href="#why-should-we-learn-nlp-fundamentals">Why should we learn NLP Fundamentals?</a></li>
<li><a href="#language-as-data">Language as Data</a></li>
<li><a href="#a-primer-on-linguistics">A Primer on Linguistics</a></li>
        </ul></div><!--/div.accordion-body-->
    </div><!--/div.accordion-collapse-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush3">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading3">
        <a href="02-word_representations.html">2. From words to vectors</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush4">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading4">
        <a href="03-transformers.html">3. Transformers: BERT and Beyond</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush5">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading5">
        <a href="04-LargeLanguageModels.html">4. Episode 3: Using large language models</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

                </div>
              </div>
            </div>

            <hr class="half-width"><div class="accordion accordion-flush lesson-resources" id="accordionFlush12">
              <div class="accordion-item">
                <h2 class="accordion-header" id="flush-headingTwelve">
                  <button class="accordion-button collapsed" id="lesson-resources" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseTwelve" aria-expanded="false" aria-controls="flush-collapseTwelve">
                    RESOURCES
                  </button>
                </h2>
                <div id="flush-collapseTwelve" class="accordion-collapse collapse" aria-labelledby="flush-headingTwelve" data-bs-parent="#accordionFlush12">
                  <div class="accordion-body">
                    <ul><li>
                        <a href="../instructor/key-points.html">Key Points</a>
                      </li>
                      <li>
                        <a href="../instructor/instructor-notes.html">Instructor Notes</a>
                      </li>
                      <li>
                        <a href="../instructor/images.html">Extract All Images</a>
                      </li>
                      <hr></ul></div>
                </div>
              </div>
            </div>
            <hr class="half-width lesson-resources"><a href="../instructor/aio.html">See all in one page</a>


            <hr class="d-none d-sm-block d-md-none"><div class="d-grid gap-1">

            </div>
          </div><!-- /div.accordion -->
        </div><!-- /div.sidebar-inner -->
      </nav></div><!-- /div.sidebar -->
  </div><!-- /div.sidebar-col -->
<!-- END:   inst/pkgdown/templates/navbar.html-->

        <!-- START: inst/pkgdown/templates/content-instructor.html -->
  <div class="col-xl-8 col-lg-12 primary-content">
    <nav class="lesson-content mx-md-4" aria-label="Previous and Next Chapter"><!-- content for small screens --><div class="d-block d-sm-block d-md-none">
        <a class="chapter-link" href="../instructor/index.html"><i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>Previous</a>
        <a class="chapter-link float-end" href="../instructor/02-word_representations.html">Next<i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i></a>
      </div>
      <!-- content for large screens -->
      <div class="d-none d-sm-none d-md-block">
        <a class="chapter-link" href="../instructor/index.html" rel="prev">
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>
          Home
        </a>
        <a class="chapter-link float-end" href="../instructor/02-word_representations.html" rel="next">
          Next: From words to...
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i>
        </a>
      </div>
      <hr></nav><main id="main-content" class="main-content"><div class="container lesson-content">
        <h1>Introduction</h1>
        <p>Last updated on 2025-10-24 |

        <a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/edit/main/episodes/01-introduction.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>



        <p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 120 minutes</p>

        <div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>

        

<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul><li>What is Natural Language Processing?</li>
<li>What are some common applications of NLP?</li>
<li>What makes text different from other data?</li>
<li>Why not just learn Large Language Models?</li>
<li>What linguistic properties should we consider when dealing with
texts?</li>
</ul></div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul><li>Define Natural Language Processing</li>
<li>Show the most relevant NLP tasks and applications in practice</li>
<li>Learn how to handle Linguistic Data and how is Linguistics relevant
to NLP</li>
</ul></div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="what-is-nlp">What is NLP?<a class="anchor" aria-label="anchor" href="#what-is-nlp"></a></h2>
<hr class="half-width"><p>Natural language processing (NLP) is an area of research and
application that focuses on making human languages processable for
computers, so that they can perform useful tasks. It is therefore not a
single method, but a collection of techniques that help us deal with
linguistic inputs. The range of techniques spans simple word counts, to
Machine Learning (ML) methods, all the way up to complex Deep Learning
(DL) architectures.</p>
<p>We use the term “natural language”, as opposed to “artificial
language” such as programming languages, which are by design constructed
to be easily formalized into machine-readable instructions. In contrast
to programming languages, natural languages are complex, ambiguous, and
heavily context-dependent, making them challenging for computers to
process. To complicate matters, there is not only a single <em>human
language</em>. More than 7000 languages are spoken around the world,
each with its own grammar, vocabulary, and cultural context.</p>
<p>In this course we will mainly focus on written language, specifically
written English, we leave out audio and speech, as they require a
different kind of input processing. But consider that we use English
only as a convenience so we can address the technical aspects of
processing textual data. While ideally most of the concepts from NLP
apply to most languages, one should always be aware that certain
languages require different approaches to solve seemingly similar
problems. We would like to encourage the usage of NLP in other less
widely known languages, especially if it is a minority language. You can
read more about this topic in this <a href="https://www.ruder.io/nlp-beyond-english/" class="external-link">blogpost</a>.</p>
<p>We can already find differences between languages in the most basic
step for processing text. Take the problem of segmenting text into
meaningful units, most of the times these units are words, in NLP we
call this task <strong>tokenization</strong>. A naive approach is to
obtain individual words by splitting text by spaces, as it seems obvious
that we always separate words with spaces. Just as human beings break up
sentences into words, phrases and other units in order to learn about
grammar and other structures of a language, NLP techniques achieve a
similar goal through tokenization. Let’s see how can we segment or
<strong>tokenize</strong> a sentence in English:</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a>english_sentence <span class="op">=</span> <span class="st">"Tokenization isn't always trivial."</span></span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a>english_words <span class="op">=</span> english_sentence.split(<span class="st">" "</span>)</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="bu">print</span>(english_words)</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(english_words))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>['Tokenization', "isn't", 'always', 'trivial.']
4</code></pre>
</div>
<p>The words are mostly well separated, however we do not get fully
formed words (we have punctuation with the period after “trivial” and
also special cases such as the abbreviation of “is not” into “isn’t”).
But at least we get a rough count of the number of words present in the
sentence. Let’s now look at the same example in Chinese:</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="co"># Chinese Translation of "Tokenization is not always trivial"</span></span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>chinese_sentence <span class="op">=</span> <span class="st">"标记化并不总是那么简单"</span> </span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a>chinese_words <span class="op">=</span> chinese_sentence.split(<span class="st">" "</span>)</span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a><span class="bu">print</span>(chinese_words)</span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(chinese_words))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>['标记化并不总是那么简单']
1</code></pre>
</div>
<p>The same example however did not work in Chinese, because Chinese
does not use spaces to separate words. This is an example of how the
idiosyncrasies of human language affects how we can process them with
computers. We therefore need to use a tokenizer specifically designed
for Chinese to obtain the list of well-formed words in the text. Here we
use a “pre-trained” tokenizer called <strong>jieba</strong>, which uses
a dictionary-based approach to correctly identify the distinct
words:</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="im">import</span> jieba  <span class="co"># A popular Chinese text segmentation library</span></span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>chinese_sentence <span class="op">=</span> <span class="st">"标记化并不总是那么简单"</span></span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a>chinese_words <span class="op">=</span> jieba.lcut(chinese_sentence)</span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a><span class="bu">print</span>(chinese_words)</span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(chinese_words))  <span class="co"># Output: 7</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>['标记', '化', '并', '不', '总是', '那么', '简单']
7</code></pre>
</div>
<p>We can trust that the output is valid because we are using a verified
library - jieba, even though we don’t speak Chinese. Another interesting
aspect is that the Chinese sentence has more words than the English one,
even though they convey the same meaning. This shows the complexity of
dealing with more than one language at a time, as is the case in task
such as <strong>Machine Translation</strong> (using computers to
translate speech or text from one human language to another).</p>
<div id="pre-trained-models-and-fine-tuning" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="pre-trained-models-and-fine-tuning" class="callout-inner">
<h3 class="callout-title">Pre-trained Models and Fine-tuning</h3>
<div class="callout-content">
<p>These two terms frequently arise in discussions of NLP. The notion of
pre-trained comes from Machine Learning and describes a model that has
already been optimized on relevant data for a given task. Such a model
can typically be loaded and applied directly to new datasets, often
working “out of the box.” without need of further refinement. Ideally,
publicly released pre-trained models have undergone rigorous testing for
both generalization and output quality on different textual data that it
was intended to be used on. Nevertheless, it remains essential to
carefully review the evaluation methods used before relying on them in
practice. It is also recommended that you perform your own evaluation of
the model on text that you intend to use it on.</p>
<p>Sometimes a pre-trained model is of good quality, but it does not fit
the nuances of our specific dataset. For example, the model was trained
on newspaper articles but you are interested in poetry. In this case, it
is common to perform <em>fine-tuning</em>, this means that instead of
training your own model from scratch, you start with the knowledge
obtained in the pre-trained model and adjust it (fine-tune it) to work
optimally with your specific data. If this is done well it leads to
increased performance in the specific task you are trying to solve. The
advantage of fine-tuning is that you often do not need a large amount of
data to improve the results, hence the popularity of the technique.</p>
</div>
</div>
</div>
<p>Natural Language Processing deals with the challenges of correctly
processing and generating text in any language. This can be as simple as
counting word frequencies to detect different writing styles, using
statistical methods to classify texts into different categories, or
using <strong>deep neural networks</strong> to generate human-like text
by exploiting word co-occurrences in large amounts of texts.</p>
</section><section><h2 class="section-heading" id="why-should-we-learn-nlp-fundamentals">Why should we learn NLP Fundamentals?<a class="anchor" aria-label="anchor" href="#why-should-we-learn-nlp-fundamentals"></a></h2>
<hr class="half-width"><p>In the past decade, NLP has evolved significantly, especially in the
field of deep learning, to the point that it has become embedded in our
daily lives, one just needs to look at the term Large Language Models
(LLMs), the latest generation of NLP models, which is now ubiquitous in
news media and tech products we use on a daily basis.</p>
<p>The term LLM now is often (and wrongly) used as a synonym of
Artificial Intelligence. We could therefore think that today we just
need to learn how to manipulate LLMs in order to fulfill our research
goals involving textual data. The truth is that Language Modeling has
always been part of the core tasks of NLP, therefore, by learning NLP
you will understand better where are the main ideas behind LLMs coming
from.</p>
<figure><img src="../fig/intro0_cs_nlp.png" alt="NLP is an interdisciplinary field, and LLMs are just a subset of it" class="figure mx-auto d-block"><div class="figcaption">NLP is an interdisciplinary field, and LLMs are
just a subset of it</div>
</figure><p>LLM is a blanket term for an assembly of large neural networks that
are trained on vast amounts of text data with the objective of
optimizing for language modeling. Once they are trained, they are used
to generate human-like text or fine-tunned to perform much more advanced
tasks. Indeed, the surprising and fascinating properties that emerge
from training models at this scale allows us to solve different complex
tasks such as answer elaborate questions, translate languages, solve
complex problems, generate narratives that emulate reasoning, and many
more, all of this with a single tool.</p>
<p>It is important, however, to pay attention to what is happening
behind the scenes in order to be able <strong>trace sources of errors
and biases</strong> that get hidden in the complexity of these models.
The purpose of this course is precisely to take a step back, and
understand that:</p>
<ul><li>There are a wide variety of tools available, beyond LLMs, that do
not require so much computing power</li>
<li>Sometimes a much simpler method than an LLM is available that can
solve our problem at hand</li>
<li>If we learn how previous approaches to solve linguistic problems
were designed, we can better understand the limitations of LLMs and how
to use them effectively</li>
<li>LLMs excel at confidently delivering information, without any
regards for correctness. This calls for a careful design of
<strong>evaluation metrics</strong> that give us a better understanding
of the quality of the generated content.</li>
</ul><p>Let’s go back to our problem of segmenting text and see what ChatGPT
has to say about tokenizing Chinese text:</p>
<figure><img src="../fig/intro1.png" alt="ChatGPT Just Works! Does it…?" class="figure mx-auto d-block"><div class="figcaption">ChatGPT Just Works! Does it…?</div>
</figure><p>We got what sounds like a straightforward confident answer. However,
it is not clear how the model arrived at this solution. Second, we do
not know whether the solution is correct or not. In this case ChatGPT
made some assumptions for us, such as choosing a specific kind of
tokenizer to give the answer, and since we do not speak the language, we
do not know if this is indeed the best approach to tokenize Chinese
text. If we understand the concept of Token (which we will today!), then
we can be more informed about the quality of the answer, whether it is
useful to us, and therefore make a better use of the model.</p>
<p>And by the way, ChatGPT was <strong>almost</strong> correct, in the
specific case of the gpt-4 tokenizer, the model will return 12 tokens
(not 11!) for the given Chinese sentence.</p>
<figure><img src="../fig/intro1b.png" alt="GPT-4 Tokenization Example" class="figure mx-auto d-block"><div class="figcaption">GPT-4 Tokenization Example</div>
</figure><p>We can also argue if the statement “Chinese is generally tokenized
character by character” is an overstatement or not. In any case, the
real question here is: Are we ok with <em>almost correct answers</em>?
Please note that this is not a call to avoid using LLM’s but a call for
a careful consideration of usage and more importantly, an attempt to
explain the mechanisms behind via NLP concepts.</p>
</section><section><h2 class="section-heading" id="language-as-data">Language as Data<a class="anchor" aria-label="anchor" href="#language-as-data"></a></h2>
<hr class="half-width"><p>From a more technical perspective, NLP focuses on applying advanced
statistical techniques to linguistic data. This is a key factor, since
we need a structured dataset with a well defined set of features in
order to manipulate it numerically. Your first task as an NLP
practitioner is to <strong>understand what aspects of textual data are
relevant for your application</strong> and apply techniques to
systematically extract meaningful features from unstructured data (if
using statistics or Machine Learning) or choose an appropriate neural
architecture (if using Deep Learning) that can help solve our problem at
hand.</p>
<div class="section level3">
<h3 id="what-is-a-word">What is a word?<a class="anchor" aria-label="anchor" href="#what-is-a-word"></a></h3>
<p>When dealing with language our basic data unit is usually a word. We
deal with sequences of words and with how they relate to each other to
generate meaning in text pieces. Thus, our first step will be to load a
text file and provide it with structure by splitting it into valid words
(tokenization)!</p>
<div id="token-vs-word" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="token-vs-word" class="callout-inner">
<h3 class="callout-title">Token vs Word</h3>
<div class="callout-content">
<p>For simplicity, in the rest of the course we will use the terms
“word” and “token” interchangeably, but as we just saw they do not
always have the same granularity. Originally the concept of token
comprised dictionary words, numeric symbols and punctuation. Nowadays,
tokenization has also evolved and became an optimization task on its own
(How can we segment text in a way that neural networks learn optimally
from text?). Tokenizers allow one to reconstruct or revert back to the
original pre-tokenized form of tokens or words, hence we can afford to
use <em>token</em> and <em>word</em> as synonyms. If you are curious,
you can visualize how different state-of-the-art tokenizers split text
<a href="https://tiktokenizer.vercel.app/" class="external-link">in this WebApp</a></p>
</div>
</div>
</div>
<p>Let’s open a file, read it into a string and split it by spaces. We
will print the original text and the list of “words” to see how they
look:</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">"84_frankenstein_clean.txt"</span>) <span class="im">as</span> f:</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>  text <span class="op">=</span> f.read()</span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a><span class="bu">print</span>(text[:<span class="dv">100</span>])</span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Length:"</span>, <span class="bu">len</span>(text))</span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a>proto_tokens <span class="op">=</span> text.split()</span>
<span id="cb7-8"><a href="#cb7-8" tabindex="-1"></a><span class="bu">print</span>(proto_tokens[:<span class="dv">40</span>])</span>
<span id="cb7-9"><a href="#cb7-9" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(proto_tokens))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Letter 1 St. Petersburgh, Dec. 11th, 17-- TO Mrs. Saville, England You will rejoice to hear that no disaster has accompanied the commencement of an en
Length: 417931

Proto-Tokens:
['Letter', '1', 'St.', 'Petersburgh,', 'Dec.', '11th,', '17--', 'TO', 'Mrs.', 'Saville,', 'England', 'You', 'will', 'rejoice', 'to', 'hear', 'that', 'no', 'disaster', 'has', 'accompanied', 'the', 'commencement', 'of', 'an', 'enterprise', 'which', 'you', 'have', 'regarded', 'with', 'such', 'evil', 'forebodings.', 'I', 'arrived', 'here', 'yesterday,', 'and', 'my']
74942</code></pre>
</div>
<p>Splitting by white space is possible but needs several extra steps to
separate out punctuation appropriately. A more sophisticated approach is
to use the <a href="https://github.com/explosion/spaCy" class="external-link">spaCy</a>
library to segment the text into human-readable tokens. First we will
download the pre-trained model, in this case we only need the small
English version:</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="op">!</span> python <span class="op">-</span>m spacy download en_core_web_sm</span></code></pre>
</div>
<p>This is a model that spaCy already trained for us on a subset of web
English data. Hence, the model already “knows” how to tokenize into
English words. When the model processes a string, it does not only do
the splitting for us but already provides more advanced linguistic
properties of the tokens (such as part-of-speech tags, or named
entities). Let’s now import the model and use it to parse our
document:</p>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a><span class="im">import</span> spacy</span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a>nlp <span class="op">=</span> spacy.load(<span class="st">"en_core_web_sm"</span>) <span class="co"># we load the small English model for efficiency</span></span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a>doc <span class="op">=</span> nlp(text) <span class="co"># Doc is a python object with several methods to retrieve linguistic properties</span></span>
<span id="cb10-6"><a href="#cb10-6" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" tabindex="-1"></a><span class="co"># SpaCy-Tokens</span></span>
<span id="cb10-8"><a href="#cb10-8" tabindex="-1"></a>tokens <span class="op">=</span> [token.text <span class="cf">for</span> token <span class="kw">in</span> doc] <span class="co"># Note that spacy tokens are also python objects </span></span>
<span id="cb10-9"><a href="#cb10-9" tabindex="-1"></a><span class="bu">print</span>(tokens[:<span class="dv">40</span>])</span>
<span id="cb10-10"><a href="#cb10-10" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(tokens))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>['Letter', '1', 'St.', 'Petersburgh', ',', 'Dec.', '11th', ',', '17', '-', '-', 'TO', 'Mrs.', 'Saville', ',', 'England', 'You', 'will', 'rejoice', 'to', 'hear', 'that', 'no', 'disaster', 'has', 'accompanied', 'the', 'commencement', 'of', 'an', 'enterprise', 'which', 'you', 'have', 'regarded', 'with', 'such', 'evil', 'forebodings', '.']
85713</code></pre>
</div>
<p>The differences look subtle at the beginning, but if we carefully
inspect the way spaCy splits the text, we can see the advantage of using
a specialized tokenizer. There are also several useful features that
spaCy provides us with. For example, we can choose to extract only
symbols, or only alphanumerical tokens, and more advanced linguistic
properties, for example we can remove punctuation and only keep
alphanumerical tokens:</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a>only_words <span class="op">=</span> [token <span class="cf">for</span> token <span class="kw">in</span> doc <span class="cf">if</span> token.is_alpha]  <span class="co"># Only alphanumerical tokens</span></span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a><span class="bu">print</span>(only_words[:<span class="dv">50</span>])</span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(only_words))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[Letter, Petersburgh, TO, Saville, England, You, will, rejoice, to, hear, that, no, disaster, has, accompanied, the, commencement, of, an, enterprise, which, you, have, regarded, with, such, evil, forebodings, I, arrived, here, yesterday, and, my, first, task, is, to, assure, my, dear, sister, of, my, welfare, and, increasing, confidence, in, the]
75062</code></pre>
</div>
<p>or keep only the verbs from our text:</p>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a>only_verbs <span class="op">=</span> [token <span class="cf">for</span> token <span class="kw">in</span> doc <span class="cf">if</span> token.pos_ <span class="op">==</span> <span class="st">"VERB"</span>]  <span class="co"># Only verbs</span></span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a><span class="bu">print</span>(only_verbs[:<span class="dv">10</span>])</span>
<span id="cb14-3"><a href="#cb14-3" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(only_verbs))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[rejoice, hear, accompanied, regarded, arrived, assure, increasing, walk, feel, braces]
10148</code></pre>
</div>
<p>SpaCy also predicts the sentences under the hood for us. It might
seem trivial to you as a human reader to recognize where a sentence
begins and ends but for a machine, just like finding words, finding
sentences is a task on its own, for which sentence-segmentation models
exist. In the case of Spacy, we can access the sentences like this:</p>
<div class="codewrapper sourceCode" id="cb16">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a>sentences <span class="op">=</span> [sent.text <span class="cf">for</span> sent <span class="kw">in</span> doc.sents] <span class="co"># Sentences are also python objects</span></span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a><span class="bu">print</span>(sentences[:<span class="dv">5</span>])</span>
<span id="cb16-3"><a href="#cb16-3" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(sentences))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>['Letter 1 St. Petersburgh, Dec. 11th, 17-- TO Mrs. Saville, England You will rejoice to hear that no disaster has accompanied the commencement of an enterprise which you have regarded with such evil forebodings.', 'I arrived here yesterday, and my first task is to assure my dear sister of my welfare and increasing confidence in the success of my undertaking.', 'I am already far north of London, and as I walk in the streets of Petersburgh, I feel a cold northern breeze play upon my cheeks, which braces my nerves and fills me with delight.', 'Do you understand this feeling?', 'This breeze, which has traveled from the regions towards which I am advancing, gives me a foretaste of those icy climes.']
3317</code></pre>
</div>
<p>We can also see what named entities the model predicted:</p>
<div class="codewrapper sourceCode" id="cb18">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(doc.ents))</span>
<span id="cb18-2"><a href="#cb18-2" tabindex="-1"></a><span class="cf">for</span> ent <span class="kw">in</span> doc.ents[:<span class="dv">5</span>]:</span>
<span id="cb18-3"><a href="#cb18-3" tabindex="-1"></a>    <span class="bu">print</span>(ent.label_, ent.text)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>1713
DATE Dec. 11th
CARDINAL 17
PERSON Saville
GPE England
DATE yesterday</code></pre>
</div>
<p>These are just basic tests to demonstrate how you can immediately
process the structure of text using existing NLP libraries. The spaCy
models we used are simpler relative to state of the art approaches. So
the more complex the input text and task, the more errors are likely to
appear when using such models. The biggest advantage of using these
existing libraries is that they help you transform unstructured plain
text files into structured data that you can manipulate later for your
own goals such as training language models.</p>
<div id="nlp-in-the-real-world" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="nlp-in-the-real-world" class="callout-inner">
<h3 class="callout-title">NLP in the real world</h3>
<div class="callout-content">
<p>Name three to five tools/products that you use on a daily basis and
that you think leverage NLP techniques. To do this exercise you may make
use of the Web.</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<p>These are some of the most popular NLP-based products that we use on
a daily basis:</p>
<ul><li>Agentic Chatbots (ChatGPT, Perplexity)</li>
<li>Voice-based assistants (e.g., Alexa, Siri, Cortana)</li>
<li>Machine translation (e.g., Google translate, DeepL, Amazon
translate)</li>
<li>Search engines (e.g., Google, Bing, DuckDuckGo)</li>
<li>Keyboard autocompletion on smartphones</li>
<li>Spam filtering</li>
<li>Spell and grammar checking apps</li>
<li>Customer care chatbots</li>
<li>Text summarization tools (e.g., news aggregators)</li>
<li>Sentiment analysis tools (e.g., social media monitoring)</li>
</ul></div>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="nlp-tasks">NLP tasks<a class="anchor" aria-label="anchor" href="#nlp-tasks"></a></h3>
<p>The previous exercise shows that a great deal of NLP techniques are
embedded in our daily life. Indeed NLP is an important component in a
wide range of software applications that we use in our day to day
activities.</p>
<p>There are several ways to describe the tasks that NLP solves. From
the Machine Learning perspective, we have:</p>
<ul><li>Unsupervised tasks: exploiting existing patterns from large amounts
of text.</li>
</ul><figure><img src="../fig/intro_unsupervised.png" width="582" alt="Unsupervised Learning" class="figure mx-auto d-block"><div class="figcaption">Unsupervised Learning</div>
</figure><ul><li>Supervised tasks: learning to classify texts given a labeled set of
examples</li>
</ul><figure><img src="../fig/intro_supervised.png" width="605" alt="Supervised Learning" class="figure mx-auto d-block"><div class="figcaption">Supervised Learning</div>
</figure><p>The Deep Learning perspective usually involves the selection of the
right model among different neural network architectures to tackle an
NLP task, such as:</p>
<ul><li><p>Multi-layer Perceptron</p></li>
<li><p>Recurrent Neural Network</p></li>
<li><p>Convolutional Neural Network</p></li>
<li><p>Long-Short Term Memory Networks (LSTMs)</p></li>
<li><p>Transformer (including LLMs!)</p></li>
</ul><p>Regardless of the chosen method, below we show one possible taxonomy
of NLP tasks. The tasks are grouped together with some of their most
prominent applications. This is definitely a non-exhaustive list, as in
reality there are hundreds of them, but it is a good start:</p>
<figure><img src="../fig/intro_taxonomy.png" width="630" alt="A taxonomy of NLP Tasks" class="figure mx-auto d-block"><div class="figcaption">A taxonomy of NLP Tasks</div>
</figure><ul><li>
<p><strong>Text Classification</strong>: Assign one or more labels
to a given piece of text. This text is usually referred to as a
<em>document</em> and in our context this can be a sentence, a
paragraph, a book chapter, etc…</p>
<ul><li>
<strong>Language Identification</strong>: determining the language
in which a particular input text is written.</li>
<li>
<strong>Spam Filtering</strong>: classifying emails into spam or not
spam based on their content.</li>
<li>
<strong>Authorship Attribution</strong>: determining the author of a
text based on its style and content (based on the assumption that each
author has a unique writing style).</li>
<li>
<strong>Sentiment Analysis</strong>: classifying text into positive,
negative or neutral sentiment. For example, in the sentence “I love this
product!”, the model would classify it as positive sentiment.</li>
</ul></li>
<li>
<p><strong>Token Classification</strong>: The task of individually
assigning one label to each word in a document. This is a one-to-one
mapping; however, because words do not occur in isolation and their
meaning depend on the sequence of words to the left or the right of
them, this is also called Word-In-Context Classification or Sequence
Labeling and usually involves syntactic and semantic analysis.</p>
<ul><li>
<strong>Part-Of-Speech Tagging</strong>: is the task of assigning a
part-of-speech label (e.g., noun, verb, adjective) to each word in a
sentence.</li>
<li>
<strong>Chunking</strong>: splitting a running text into “chunks” of
words that together represent a meaningful unit: phrases, sentences,
paragraphs, etc.</li>
<li>
<strong>Word Sense Disambiguation</strong>: based on the context
what does a word mean (think of “book” in “I read a book.” vs “I want to
book a flight.”)</li>
<li>
<strong>Named Entity Recognition</strong>: recognize world entities
in text, e.g. Persons, Locations, Book Titles, or many others. For
example “Mary Shelley” is a person, “Frankenstein or the Modern
Prometheus” is a book, etc.</li>
<li>
<strong>Semantic Role Labeling</strong>: the task of finding out
“Who did what to whom?” in a sentence: information from events such as
agents, participants, circumstances, subject-verb-object triples
etc.</li>
<li>
<strong>Relation Extraction</strong>: the task of identifying named
relationships between entities in a text, e.g. “Apple is based in
California” has the relation (Apple, based_in, California).</li>
<li>
<strong>Co-reference Resolution</strong>: the task of determining
which words refer to the same entity in a text, e.g. “Mary is a doctor.
She works at the hospital.” Here “She” refers to “Mary”.</li>
<li>
<strong>Entity Linking</strong>: the task of disambiguation of named
entities in a text, linking them to their corresponding entries in a
knowledge base, e.g. Mary Shelley’s biography in Wikipedia.</li>
</ul></li>
<li>
<p><strong>Language Modeling</strong>: Given a sequence of words,
the model predicts the next word. For example, in the sentence “The
capital of France is _____”, the model should predict “Paris” based on
the context. This task was initially useful for building solutions that
require speech and optical character recognition (even handwriting),
language translation and spelling correction. Nowadays this has scaled
up to the LLMs that we know. A byproduct of pre-trained Language
Modeling is the <strong>vectorized representation</strong> of texts
which allows to perform specific tasks such as:</p>
<ul><li>
<strong>Text Similarity</strong>: The task of determining how
similar two pieces of text are.</li>
<li>
<strong>Plagiarism detection</strong>: determining whether a piece
of text, B, is close enough to another known piece of text, A, which
increases the likelihood that it was plagiarized.</li>
<li>
<strong>Document clustering</strong>: grouping similar texts
together based on their content.</li>
<li>
<strong>Topic modelling</strong>: a specific instance of clustering,
here we automatically identify abstract “topics” that occur in a set of
documents, where each topic is represented as a cluster of words that
frequently appear together.</li>
<li>
<strong>Information Retrieval</strong>: this is the task of finding
relevant information or documents from a large collection of
unstructured data based on user’s query, e.g., “What’s the best
restaurant near me?”.</li>
</ul></li>
<li>
<p><strong>Text Generation</strong>: the task of generating text
based on a given input. This is usually done by generating the output
word by word, conditioned on both the input and the output so far. The
difference with Language Modeling is that for generation there are
higher-level generation objectives such as:</p>
<ul><li>
<strong>Machine Translation</strong>: translating text from one
language to another, e.g., “Hello” in English to “Que tal” in
Spanish.</li>
<li>
<strong>Summarization</strong>: generating a concise summary of a
longer text. It can be abstractive (generating new sentences that
capture the main ideas of the original text) but also extractive
(selecting important sentences from the original text).</li>
<li>
<strong>Paraphrasing</strong>: generating a new sentence that
conveys the same meaning as the original sentence, e.g., “The cat is on
the mat.” to “The mat has a cat on it.”.</li>
<li>
<strong>Question Answering</strong>: given a question and a context,
the model generates an answer. For example, given the question “What is
the capital of France?” and the Wikipedia article about France as the
context, the model should answer “Paris”. This task can be approached as
a text classification problem (where the answer is one of the predefined
options) or as a generative task (where the model generates the answer
from scratch).</li>
<li>
<strong>Conversational Agent (ChatBot)</strong>: Building a system
that interacts with a user via natural language, e.g., “What’s the
weather today, Siri?”. These agents are widely used to improve user
experience in customer service, personal assistance and many other
domains.</li>
</ul></li>
</ul><p>For the purposes of this episode, we will focus on <strong>supervised
learning</strong> tasks and we will emphasize how the
<strong>Transformer architecture</strong> is used to tackle some of
them.</p>
<div id="inputs-and-outputs" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="inputs-and-outputs" class="callout-inner">
<h3 class="callout-title">Inputs and Outputs</h3>
<div class="callout-content">
<p>Look at the NLP Task taxonomy described above and write down a couple
of examples of (Input, Output) instance pairs that you would need in
order to train a supervised model for your chosen task.</p>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2"> Show me the solution </h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" aria-labelledby="headingSolution2" data-bs-parent="#accordionSolution2">
<div class="accordion-body">
<p>Example: the task of Conversational agent. Here are 3 instances to
provide supervision for a model:</p>
<p><strong>Input:</strong> “Hello, how are you?”
<strong>Output:</strong> “I am fine thanks!”</p>
<p><strong>Input:</strong> “Do you know at what time is the World Cup
final today?” <strong>Output:</strong> “Yes, the World Cup final will be
at 6pm CET”</p>
<p><strong>Input:</strong> “What color is my shirt?”
<strong>Output:</strong> “Sorry, I am unable to see what you are
wearing.”</p>
</div>
</div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="a-primer-on-linguistics">A Primer on Linguistics<a class="anchor" aria-label="anchor" href="#a-primer-on-linguistics"></a></h2>
<hr class="half-width"><p>Natural language exhibits a set of properties that make it more
challenging to process than other types of data such as tables,
spreadsheets or time series. <strong>Language is hard to process because
it is compositional, ambiguous, discrete and sparse</strong>.</p>
<div class="section level3">
<h3 id="compositionality">Compositionality<a class="anchor" aria-label="anchor" href="#compositionality"></a></h3>
<p>The basic elements of written languages are characters, a sequence of
characters form words, and words in turn denote objects, concepts,
events, actions and ideas (Goldberg, 2016). Subsequently words form
phrases and sentences which are used in communication and depend on the
context in which they are used. We as humans derive the meaning of
utterances from interpreting contextual information that is present at
different levels at the same time:</p>
<figure><img src="../fig/intro2_levels_lang.svg" width="573" alt="Levels of Language" class="figure mx-auto d-block"><div class="figcaption">Levels of Language</div>
</figure><p>The first two levels refer to spoken language only, and the other
four levels are present in both speech and text. Because in principle
machines do not have access to the same levels of information that we do
(they can only have independent audio, textual or visual inputs), we
need to come up with clever methods to overcome this significant
limitation. Knowing the levels of language is important so we consider
what kind of problems we are facing when attempting to solve our NLP
task at hand.</p>
</div>
<div class="section level3">
<h3 id="ambiguity">Ambiguity<a class="anchor" aria-label="anchor" href="#ambiguity"></a></h3>
<p>The disambiguation of meaning is usually a by-product of the context
in which utterances are expressed and also the historic accumulation of
interactions which are transmitted across generations (think for
instance to idioms – these are usually meaningless phrases that acquire
meaning only if situated within their historical and societal context).
These characteristics make NLP a particularly challenging field to work
in.</p>
<p>We cannot expect a machine to process human language and simply
understand it as it is. We need a systematic, scientific approach to
deal with it. It’s within this premise that the field of NLP is born,
primarily interested in converting the building blocks of human/natural
language into something that a machine can understand.</p>
<p>The image below shows how the levels of language relate to a few NLP
applications:</p>
<figure><img src="../fig/intro3_levels_nlp.png" alt="Diagram showing building blocks of language" class="figure mx-auto d-block"><div class="figcaption">Diagram showing building blocks of
language</div>
</figure><div id="levels-of-ambiguity" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="levels-of-ambiguity" class="callout-inner">
<h3 class="callout-title">Levels of ambiguity</h3>
<div class="callout-content">
<p>Discuss what the following sentences mean. What level of ambiguity do
they represent?:</p>
<ul><li>“The door is unlockable from the inside.” vs “Unfortunately, the
cabinet is unlockable, so we can’t secure it”</li>
<li>“I saw the <em>cat with the stripes</em>” vs “I saw the cat <em>with
the telescope</em>”</li>
<li>“Colorless green ideas sleep furiously”</li>
<li>“I never said she stole my money.” (re-write this sentence multiple
times and each time write a different word in italics).</li>
</ul></div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3"> Show me the solution </h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" aria-labelledby="headingSolution3" data-bs-parent="#accordionSolution3">
<div class="accordion-body">
<p>This is why the previous statements were difficult:</p>
<ul><li>“Un-lockable vs Unlock-able” is a <strong>Morphological</strong>
ambiguity: Same word form, two possible meanings</li>
<li>“I saw the cat with the telescope” is a <strong>Syntactic</strong>
ambiguity: Same sentence structure, different properties</li>
<li>“Colorless green ideas sleep furiously” <strong>Semantic</strong>
ambiguity: Grammatical but meaningless (ideas do not have color as a
property. Even if this was true, they would be either colorless or
green)</li>
<li>“I NEVER said she stole MY money.” is a <strong>Pragmatic</strong>
ambiguity: Meaning relies on word emphasis</li>
</ul></div>
</div>
</div>
</div>
<p>Whenever you are solving a specific task, you should ask yourself
what kind of ambiguity can affect your results, and to what degrees? At
what level are your assumptions operating when defining your research
questions? Having the answers to this can save you a lot of time when
debugging your models. Sometimes the most innocent assumptions (for
example using the wrong tokenizer) can create enormous performance drops
even when the higher level assumptions were correct.</p>
</div>
<div class="section level3">
<h3 id="sparsity">Sparsity<a class="anchor" aria-label="anchor" href="#sparsity"></a></h3>
<p>Another key property of linguistic data is its sparsity. This means
that if we are hunting for a specific phenomenon, we may often realize
it barely occurs inside a vast amount of text. Imagine we have the
following brief text and we are interested in <em>pizzas</em> and
<em>hamburgers</em>:</p>
<div class="codewrapper sourceCode" id="cb20">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" tabindex="-1"></a><span class="co"># A mini-corpus where our target words appear</span></span>
<span id="cb20-2"><a href="#cb20-2" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"""</span></span>
<span id="cb20-3"><a href="#cb20-3" tabindex="-1"></a><span class="st">I am hungry. Should I eat delicious pizza?</span></span>
<span id="cb20-4"><a href="#cb20-4" tabindex="-1"></a><span class="st">Or maybe I should eat a juicy hamburger instead.</span></span>
<span id="cb20-5"><a href="#cb20-5" tabindex="-1"></a><span class="st">Many people like to eat pizza because is tasty, they think pizza is delicious as hell!</span></span>
<span id="cb20-6"><a href="#cb20-6" tabindex="-1"></a><span class="st">My friend prefers to eat a hamburger and I agree with him.</span></span>
<span id="cb20-7"><a href="#cb20-7" tabindex="-1"></a><span class="st">We will drive our car to the restaurant to get the succulent hamburger.</span></span>
<span id="cb20-8"><a href="#cb20-8" tabindex="-1"></a><span class="st">Right now, our cat sleeps on the mat so we won't take him.</span></span>
<span id="cb20-9"><a href="#cb20-9" tabindex="-1"></a><span class="st">I did not wash my car, but at least the car has gasoline.</span></span>
<span id="cb20-10"><a href="#cb20-10" tabindex="-1"></a><span class="st">Perhaps when we come back we will take out the cat for a walk.</span></span>
<span id="cb20-11"><a href="#cb20-11" tabindex="-1"></a><span class="st">The cat will be happy then.</span></span>
<span id="cb20-12"><a href="#cb20-12" tabindex="-1"></a><span class="st">"""</span></span></code></pre>
</div>
<p>We can first use spaCy to tokenize the text and do some direct word
count:</p>
<div class="codewrapper sourceCode" id="cb21">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" tabindex="-1"></a><span class="im">import</span> spacy</span>
<span id="cb21-2"><a href="#cb21-2" tabindex="-1"></a>nlp <span class="op">=</span> spacy.load(<span class="st">"en_core_web_sm"</span>)</span>
<span id="cb21-3"><a href="#cb21-3" tabindex="-1"></a></span>
<span id="cb21-4"><a href="#cb21-4" tabindex="-1"></a>doc <span class="op">=</span> nlp(text)</span>
<span id="cb21-5"><a href="#cb21-5" tabindex="-1"></a>words <span class="op">=</span> [token.lower_ <span class="cf">for</span> token <span class="kw">in</span> doc <span class="cf">if</span> token.is_alpha]  <span class="co"># Filter out punctuation and new lines</span></span>
<span id="cb21-6"><a href="#cb21-6" tabindex="-1"></a><span class="bu">print</span>(words)</span>
<span id="cb21-7"><a href="#cb21-7" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(words))</span></code></pre>
</div>
<p>We have in total 104 words, but we actually want to know how many
times each word appears. For that we use the Python Counter and then we
can visualize it inside a chart with matplotlib:</p>
<div class="codewrapper sourceCode" id="cb22">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> Counter</span>
<span id="cb22-2"><a href="#cb22-2" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb22-3"><a href="#cb22-3" tabindex="-1"></a></span>
<span id="cb22-4"><a href="#cb22-4" tabindex="-1"></a>word_count <span class="op">=</span> Counter(words).most_common()</span>
<span id="cb22-5"><a href="#cb22-5" tabindex="-1"></a>tokens <span class="op">=</span> [item[<span class="dv">0</span>] <span class="cf">for</span> item <span class="kw">in</span> word_count]</span>
<span id="cb22-6"><a href="#cb22-6" tabindex="-1"></a>frequencies <span class="op">=</span> [item[<span class="dv">1</span>] <span class="cf">for</span> item <span class="kw">in</span> word_count]</span>
<span id="cb22-7"><a href="#cb22-7" tabindex="-1"></a></span>
<span id="cb22-8"><a href="#cb22-8" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">18</span>, <span class="dv">6</span>))</span>
<span id="cb22-9"><a href="#cb22-9" tabindex="-1"></a>plt.bar(tokens, frequencies)</span>
<span id="cb22-10"><a href="#cb22-10" tabindex="-1"></a>plt.xticks(rotation<span class="op">=</span><span class="dv">90</span>)</span>
<span id="cb22-11"><a href="#cb22-11" tabindex="-1"></a>plt.show()</span></code></pre>
</div>
<p>This bar chart shows us several things about sparsity, even with such
a small text:</p>
<ul><li><p>The most common words are filler words such as “the”, “of”, “not”
etc. These are known as <strong>stopwords</strong> because such words by
themselves generally do not hold a lot of information about the meaning
of the piece of text.</p></li>
<li><p>The two concepts (hamburger and pizza) we are interested in,
appear only 3 times each, out of 104 words (comprising only ~3% of our
corpus). This number only goes lower as the corpus size
increases</p></li>
<li><p>There is a long tail in the distribution, where actually a lot of
meaningful words are located.</p></li>
</ul><div id="callout3" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div class="callout-inner">
<div class="callout-content">
<p>Sparsity is closely related to what is frequently called
<strong>domain-specific data</strong>. The discourse context in which
language is used varies importantly across disciplines (domains). Take
for example law texts and medical texts which are typically filled with
domain-specific jargon. We should expect the top part of the
distribution to contain mostly the same worda as they tend to be stop
words. But once we remove the stop words, the top of the distirbution
will contain very different content words. Also, the meaning of concepts
described in each domain might significantly differ. For example the
word “trial” refers to a procedure for examining evidence in court, but
in the medical domain this could refer to a clinical “trial” which is a
procedure to test the efficacy and safety of treatments on patients. For
this reason there are specialized models and corpora that model language
use in specific domains. The concept of fine-tuning a general purpose
model with domain-specific data is also popular, even when using
LLMs.</p>
</div>
</div>
</div>
<div id="stop-words" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="stop-words" class="callout-inner">
<h3 class="callout-title">Stop Words</h3>
<div class="callout-content">
<p><strong>Stop words</strong> are extremely frequent syntactic filler
words that do not provide relevant semantic information for our use
case. For some use cases it is better to ignore them in order to fight
the sparsity phenomenon. However, consider that in many other use cases
the syntactic information that stop words provide is crucial to solve
the task.</p>
<p>SpaCy has a pre-defined list of stopwords per language. To explicitly
load the English stop words we can do:</p>
<div class="codewrapper sourceCode" id="cb23">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" tabindex="-1"></a><span class="im">from</span> spacy.lang.en.stop_words <span class="im">import</span> STOP_WORDS</span>
<span id="cb23-2"><a href="#cb23-2" tabindex="-1"></a><span class="bu">print</span>(STOP_WORDS)  <span class="co"># a set of common stopwords</span></span>
<span id="cb23-3"><a href="#cb23-3" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(STOP_WORDS)) <span class="co"># There are 326 words considered in this list</span></span></code></pre>
</div>
<p>You can also manually extend the list of stop words if you are
interested in ignoring other unlisted terms that you encounter in your
data.</p>
<p>Alternatively, you can filter out stop words when iterating your
tokens (remember the spaCy token properties!) like this:</p>
<div class="codewrapper sourceCode" id="cb24">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" tabindex="-1"></a>doc <span class="op">=</span> nlp(text)</span>
<span id="cb24-2"><a href="#cb24-2" tabindex="-1"></a>content_words <span class="op">=</span> [token.text <span class="cf">for</span> token <span class="kw">in</span> doc <span class="cf">if</span> token.is_alpha <span class="kw">and</span> <span class="kw">not</span> token.is_stop]  <span class="co"># Filter out stop words and punctuation</span></span>
<span id="cb24-3"><a href="#cb24-3" tabindex="-1"></a><span class="bu">print</span>(content_words)</span></code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="discreteness">Discreteness<a class="anchor" aria-label="anchor" href="#discreteness"></a></h3>
<p>There is no inherent relationship between the form of a word and its
meaning. For this reason, by syntactic or lexical analysis alone, there
is no automatic way of knowing if two words are similar in meaning or
how they relate semantically to each other. For example, “car” and “cat”
appear to be very closely related at the morphological level, only one
letter needs to change to convert one word into the other. But the two
words represent concepts or entities in the world which are very
different. Conversely, “pizza” and “hamburger” look very different (they
only share one letter in common) but are more closely related
semantically, because they both refer to typical fast foods.</p>
<p>How can we automatically know that “pizza” and “hamburger” share more
semantic properties than “car” and “cat”? One way is by looking at the
<strong>context</strong> (neighboring words) of these words. This idea
is the principle behind <strong>distributional semantics</strong>, and
aims to look at the statistical properties of language, such as word
co-occurrences (what words are typically located nearby a given word in
a given corpus of text), to understand how words relate to each
other.</p>
<p>Let’s keep using the list of words from our mini corpus:</p>
<div class="codewrapper sourceCode" id="cb25">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" tabindex="-1"></a>words <span class="op">=</span> [token.lower_ <span class="cf">for</span> token <span class="kw">in</span> doc <span class="cf">if</span> token.is_alpha]</span></code></pre>
</div>
<p>Now we will create a dictionary where we accumulate the words that
appear around our words of interest. In this case we want to find out,
according to our corpus, the most frequent words that occur around
<em>pizza</em>, <em>hamburger</em>, <em>car</em> and <em>cat</em>:</p>
<div class="codewrapper sourceCode" id="cb26">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" tabindex="-1"></a>target_words <span class="op">=</span> [<span class="st">"pizza"</span>, <span class="st">"hamburger"</span>, <span class="st">"car"</span>, <span class="st">"cat"</span>] <span class="co"># words we want to analyze</span></span>
<span id="cb26-2"><a href="#cb26-2" tabindex="-1"></a>co_occurrence <span class="op">=</span> {word: [] <span class="cf">for</span> word <span class="kw">in</span> target_words}</span>
<span id="cb26-3"><a href="#cb26-3" tabindex="-1"></a>co_occurrence</span></code></pre>
</div>
<p>We iterate over each word in our corpus, collecting its surrounding
words within a defined window. A window consists of a set number of
words to the left and right of the target word, as determined by the
window_size parameter. For example, with <code>window_size = 3</code>, a
word <code>W</code> has a window of six neighboring words—three
preceding and three following—excluding <code>W</code> itself:</p>
<div class="codewrapper sourceCode" id="cb27">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" tabindex="-1"></a>window_size <span class="op">=</span> <span class="dv">3</span> <span class="co"># How many words to look at on each side</span></span>
<span id="cb27-2"><a href="#cb27-2" tabindex="-1"></a><span class="cf">for</span> i, word <span class="kw">in</span> <span class="bu">enumerate</span>(words):</span>
<span id="cb27-3"><a href="#cb27-3" tabindex="-1"></a>    <span class="co"># If the current word is one of our target words...</span></span>
<span id="cb27-4"><a href="#cb27-4" tabindex="-1"></a>    <span class="cf">if</span> word <span class="kw">in</span> target_words:</span>
<span id="cb27-5"><a href="#cb27-5" tabindex="-1"></a>        start <span class="op">=</span> <span class="bu">max</span>(<span class="dv">0</span>, i <span class="op">-</span> window_size) <span class="co"># get the start index of the window</span></span>
<span id="cb27-6"><a href="#cb27-6" tabindex="-1"></a>        end <span class="op">=</span> <span class="bu">min</span>(<span class="bu">len</span>(words), i <span class="op">+</span> <span class="dv">1</span> <span class="op">+</span> window_size) <span class="co"># get the end index of the window</span></span>
<span id="cb27-7"><a href="#cb27-7" tabindex="-1"></a>        context <span class="op">=</span> words[start:i] <span class="op">+</span> words[i<span class="op">+</span><span class="dv">1</span>:end]  <span class="co"># Exclude the target word itself</span></span>
<span id="cb27-8"><a href="#cb27-8" tabindex="-1"></a>        co_occurrence[word].extend(context)</span>
<span id="cb27-9"><a href="#cb27-9" tabindex="-1"></a></span>
<span id="cb27-10"><a href="#cb27-10" tabindex="-1"></a><span class="bu">print</span>(co_occurrence)</span></code></pre>
</div>
<p>We call the words that fall inside this window the
<code>context</code> of a target word. We can already see other
interesting related words in the context of each target word, but a lot
of non interesting stuff is in there. To obtain even nicer results, we
can delete the stop words from the context window before adding it to
the dictionary. You can define your own stop words, here we use the
STOP_WORDS list provided by spaCy:</p>
<div class="codewrapper sourceCode" id="cb28">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" tabindex="-1"></a><span class="im">from</span> spacy.lang.en.stop_words <span class="im">import</span> STOP_WORDS</span>
<span id="cb28-2"><a href="#cb28-2" tabindex="-1"></a></span>
<span id="cb28-3"><a href="#cb28-3" tabindex="-1"></a>co_occurrence <span class="op">=</span> {word: [] <span class="cf">for</span> word <span class="kw">in</span> target_words} <span class="co"># Empty the dictionary</span></span>
<span id="cb28-4"><a href="#cb28-4" tabindex="-1"></a></span>
<span id="cb28-5"><a href="#cb28-5" tabindex="-1"></a>window_size <span class="op">=</span> <span class="dv">3</span> <span class="co"># How many words to look at on each side</span></span>
<span id="cb28-6"><a href="#cb28-6" tabindex="-1"></a><span class="cf">for</span> i, word <span class="kw">in</span> <span class="bu">enumerate</span>(words):</span>
<span id="cb28-7"><a href="#cb28-7" tabindex="-1"></a>    <span class="co"># If the current word is one of our target words...</span></span>
<span id="cb28-8"><a href="#cb28-8" tabindex="-1"></a>    <span class="cf">if</span> word <span class="kw">in</span> target_words:</span>
<span id="cb28-9"><a href="#cb28-9" tabindex="-1"></a>        start <span class="op">=</span> <span class="bu">max</span>(<span class="dv">0</span>, i <span class="op">-</span> window_size) <span class="co"># get the start index of the window</span></span>
<span id="cb28-10"><a href="#cb28-10" tabindex="-1"></a>        end <span class="op">=</span> <span class="bu">min</span>(<span class="bu">len</span>(words), i <span class="op">+</span> <span class="dv">1</span> <span class="op">+</span> window_size) <span class="co"># get the end index of the window</span></span>
<span id="cb28-11"><a href="#cb28-11" tabindex="-1"></a>        context <span class="op">=</span> words[start:i] <span class="op">+</span> words[i<span class="op">+</span><span class="dv">1</span>:end]  <span class="co"># Exclude the target word itself</span></span>
<span id="cb28-12"><a href="#cb28-12" tabindex="-1"></a>        context <span class="op">=</span> [w <span class="cf">for</span> w <span class="kw">in</span> context <span class="cf">if</span> w <span class="kw">not</span> <span class="kw">in</span> STOP_WORDS] <span class="co"># Filter out stop words</span></span>
<span id="cb28-13"><a href="#cb28-13" tabindex="-1"></a>        co_occurrence[word].extend(context)</span>
<span id="cb28-14"><a href="#cb28-14" tabindex="-1"></a></span>
<span id="cb28-15"><a href="#cb28-15" tabindex="-1"></a><span class="bu">print</span>(co_occurrence)</span></code></pre>
</div>
<p>Our dictionary keys represent each word of interest, and the values
are a list of the words that occur within <em>window_size</em> distance
of the word. Now we use a Counter to get the most common items:</p>
<div class="codewrapper sourceCode" id="cb29">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" tabindex="-1"></a><span class="co"># Print the most common context words for each target word</span></span>
<span id="cb29-2"><a href="#cb29-2" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Contextual Fingerprints:</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb29-3"><a href="#cb29-3" tabindex="-1"></a><span class="cf">for</span> word, context_list <span class="kw">in</span> co_occurrence.items():</span>
<span id="cb29-4"><a href="#cb29-4" tabindex="-1"></a>    <span class="co"># We use Counter to get a frequency count of context words</span></span>
<span id="cb29-5"><a href="#cb29-5" tabindex="-1"></a>    fingerprint <span class="op">=</span> Counter(context_list).most_common(<span class="dv">5</span>)</span>
<span id="cb29-6"><a href="#cb29-6" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"'</span><span class="sc">{</span>word<span class="sc">}</span><span class="ss">': </span><span class="sc">{</span>fingerprint<span class="sc">}</span><span class="ss">"</span>)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Contextual Fingerprints:

'pizza': [('eat', 2), ('delicious', 2), ('tasty', 2), ('maybe', 1), ('like', 1)]
'hamburger': [('eat', 2), ('juicy', 1), ('instead', 1), ('people', 1), ('agree', 1)]
'car': [('drive', 1), ('restaurant', 1), ('wash', 1), ('gasoline', 1)]
'cat': [('walk', 2), ('right', 1), ('sleeps', 1), ('happy', 1)]</code></pre>
</div>
<p>As our mini experiment demonstrates, discreteness can be combatted
with statistical co-occurrence: words with similar meaning will occur
around similar concepts, giving us an idea of similarity that has
nothing to do with syntactic or lexical form of words. This is the core
idea behind most modern semantic representation models in NLP.</p>
<div id="nlp-libraries" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="nlp-libraries" class="callout-inner">
<h3 class="callout-title">NLP Libraries</h3>
<div class="callout-content">
<p>Related to the need of shaping our problems into a known task, there
are several existing NLP libraries which provide a wide range of models
that we can use out-of-the-box (without further need of modification).
We already saw simple examples using SpaCy for English and jieba for
Chinese. Again, as a non-exhaustive list, we mention some widely used
NLP libraries in Python:</p>
<ul><li><a href="https://github.com/nltk/nltk" class="external-link">NLTK</a></li>
<li><a href="https://github.com/explosion/spaCy" class="external-link">spaCy</a></li>
<li><a href="https://github.com/RaRe-Technologies/gensim" class="external-link">Gensim</a></li>
<li><a href="https://github.com/stanfordnlp/stanza" class="external-link">Stanza</a></li>
<li><a href="https://github.com/flairNLP/flair" class="external-link">Flair</a></li>
<li><a href="https://github.com/facebookresearch/fastText" class="external-link">FastText</a></li>
<li><a href="https://github.com/huggingface/transformers" class="external-link">HuggingFace
Transformers</a></li>
</ul></div>
</div>
</div>
<div id="linguistic-resources" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="linguistic-resources" class="callout-inner">
<h3 class="callout-title">Linguistic Resources</h3>
<div class="callout-content">
<p>There are also several curated resources (textual data) that can help
solve your NLP-related tasks, specifically when you need highly
specialized definitions. An exhaustive list would be impossible as there
are thousands of them, and also them being language and domain
dependent. Below we mention some of the most prominent, just to give you
an idea of the kind of resources you can find, so you don’t need to
reinvent the wheel every time you start a project:</p>
<ul><li>
<a href="https://huggingface.co/datasets" class="external-link">HuggingFace Datasets</a>:
A large collection of datasets for NLP tasks, including text
classification, question answering, and language modeling.</li>
<li>
<a href="https://wordnet.princeton.edu/" class="external-link">WordNet</a>: A large
lexical database of English, where words are grouped into sets of
synonyms (synsets) and linked by semantic relations.</li>
<li>
<a href="https://www.europarl.europa.eu/ep-search/search.do?language=en" class="external-link">Europarl</a>:
A parallel corpus of the proceedings of the European Parliament,
available in 21 languages, which can be used for machine translation and
cross-lingual NLP tasks.</li>
<li>
<a href="https://universaldependencies.org/" class="external-link">Universal
Dependencies</a>: A collection of syntactically annotated treebanks
across 100+ languages, providing a consistent annotation scheme for
syntactic and morphological properties of words, which can be used for
cross-lingual NLP tasks.</li>
<li>
<a href="https://propbank.github.io/" class="external-link">PropBank</a>: A corpus of
texts annotated with information about basic semantic propositions,
which can be used for English semantic tasks.</li>
<li>
<a href="https://framenet.icsi.berkeley.edu/fndrupal/" class="external-link">FrameNet</a>:
A lexical resource that provides information about the semantic frames
that underlie the meanings of words (mainly verbs and nouns), including
their roles and relations.</li>
<li>
<a href="https://babelnet.org/" class="external-link">BabelNet</a>: A multilingual lexical
resource that combines WordNet and Wikipedia, providing a large number
of concepts and their relations in multiple languages.</li>
<li>
<a href="https://www.wikidata.org/" class="external-link">Wikidata</a>: A free and open
knowledge base initially derived from Wikipedia, that contains
structured data about entities, their properties and relations, which
can be used to enrich NLP applications.</li>
<li>
<a href="https://github.com/allenai/dolma" class="external-link">Dolma</a>: An open
dataset of 3 trillion tokens from a diverse mix of clean web content,
academic publications, code, books, and encyclopedic materials, used to
train English large language models.</li>
</ul></div>
</div>
</div>
<p>What did we learn in this lesson?</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<span class="callout-header">Key Points</span>
<div class="callout-inner">
<div class="callout-content">
<ul><li><p>NLP is a subfield of Artificial Intelligence (AI) that, using the
help of Linguistics, deals with approaches to process, understand and
generate natural language</p></li>
<li><p>Linguistic Data has special properties that we should consider
when modeling our solutions</p></li>
<li><p>Key tasks include language modeling, text classification, token
classification and text generation</p></li>
<li><p>Deep learning has significantly advanced NLP, but the challenge
remains in processing the discrete and ambiguous nature of
language</p></li>
<li><p>The ultimate goal of NLP is to enable machines to understand and
process language as humans do</p></li>
</ul></div>
</div>
</div>
</div>
</section></div> <!-- / div.lesson-content -->
    </main><!-- / main#main-content.main-content --><nav class="bottom-pagination mx-md-4" aria-label="Previous and Next Chapter"><div class="d-block d-sm-block d-md-none">
        <a class="chapter-link" href="../instructor/index.html"><i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>Previous</a>
        <a class="chapter-link float-end" href="../instructor/02-word_representations.html">Next<i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i></a>
      </div>
      <!-- content for large screens -->
      <div class="d-none d-sm-none d-md-block">
        <a class="chapter-link" href="../instructor/index.html" rel="prev">
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>
          Home
        </a>
        <a class="chapter-link float-end" href="../instructor/02-word_representations.html" rel="next">
          Next: From words to...
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i>
        </a>
      </div>
    </nav></div> <!-- / div.primary-content.col-xs-12 -->
<!-- END:   inst/pkgdown/templates/content-instructor.html-->

      </div><!--/div.row-->
      		<footer class="row footer mx-md-3"><hr><div class="col-md-6">
        <p>This lesson is subject to the <a href="CODE_OF_CONDUCT.html">Code of Conduct</a></p>
        <p>

        <a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/edit/main/episodes/01-introduction.md" class="external-link">Edit on GitHub</a>

	
        | <a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/blob/main/CONTRIBUTING.md" class="external-link">Contributing</a>
        | <a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/" class="external-link">Source</a></p>
				<p><a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/blob/main/CITATION.cff" class="external-link">Cite</a> | <a href="mailto:team@carpentries.org">Contact</a> | <a href="https://carpentries.org/about/" class="external-link">About</a></p>
			</div>
			<div class="col-md-6">

        <p>Materials licensed under <a href="LICENSE.html">CC-BY 4.0</a> by the authors</p>

        <p>Template licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/" class="external-link">CC-BY 4.0</a> by <a href="https://carpentries.org/" class="external-link">The Carpentries</a></p>
        <p>Built with <a href="https://github.com/carpentries/sandpaper/tree/0.17.1" class="external-link">sandpaper (0.17.1)</a>, <a href="https://github.com/carpentries/pegboard/tree/0.7.9" class="external-link">pegboard (0.7.9)</a>, and <a href="https://github.com/carpentries/varnish/tree/1.0.7" class="external-link">varnish (1.0.7)</a></p>
			</div>
		</footer></div> <!-- / div.container -->
	<div id="to-top">
		<a href="#top">
      <i class="search-icon" data-feather="arrow-up" role="img" aria-label="Back To Top"></i><br><!-- <span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top --><span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top
		</a>
	</div>
  <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "LearningResource",
  "@id": "https://esciencecenter-digital-skills.github.io/Natural-language-processing/instructor/01-introduction.html",
  "inLanguage": "en",
  "dct:conformsTo": "https://bioschemas.org/profiles/LearningResource/1.0-RELEASE",
  "description": "A Carpentries Lesson teaching foundational data and coding skills to researchers worldwide",
  "keywords": "software, data, lesson, The Carpentries, NLP, English, social sciences, pre-alpha",
  "name": "Introduction",
  "creativeWorkStatus": "active",
  "url": "https://esciencecenter-digital-skills.github.io/Natural-language-processing/instructor/01-introduction.html",
  "identifier": "https://esciencecenter-digital-skills.github.io/Natural-language-processing/instructor/01-introduction.html",
  "dateCreated": "2024-04-24",
  "dateModified": "2025-10-24",
  "datePublished": "2025-10-28"
}

  </script><script>
		feather.replace();
	</script></body></html><!-- END:   inst/pkgdown/templates/layout.html-->

