<!DOCTYPE html>
<!-- START: inst/pkgdown/templates/layout.html --><!-- Generated by pkgdown: do not edit by hand --><html lang="en" data-bs-theme="auto"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><title>Fundamentals of Natural Language Processing (NLP) in Python: Introduction</title><meta name="viewport" content="width=device-width, initial-scale=1"><script src="../assets/themetoggle.js"></script><link rel="stylesheet" type="text/css" href="../assets/styles.css"><script src="../assets/scripts.js" type="text/javascript"></script><!-- mathjax --><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      config: ["MMLorHTML.js"],
      jax: ["input/TeX","input/MathML","output/HTML-CSS","output/NativeMML", "output/PreviewHTML"],
      extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
      TeX: {
        extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
      },
      tex2jax: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [ ['$$','$$'], ['\\[', '\\]'] ],
        processEscapes: true
      }
    });
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><!-- Responsive Favicon for The Carpentries --><link rel="apple-touch-icon" sizes="180x180" href="../favicons/incubator/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="../favicons/incubator/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="../favicons/incubator/favicon-16x16.png"><link rel="manifest" href="../favicons/incubator/site.webmanifest"><link rel="mask-icon" href="../favicons/incubator/safari-pinned-tab.svg" color="#5bbad5"><meta name="msapplication-TileColor" content="#da532c"><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"><meta name="theme-color" media="(prefers-color-scheme: dark)" content="black"></head><body>
    <header id="top" class="navbar navbar-expand-md top-nav incubator"><svg xmlns="http://www.w3.org/2000/svg" class="d-none"><symbol id="check2" viewbox="0 0 16 16"><path d="M13.854 3.646a.5.5 0 0 1 0 .708l-7 7a.5.5 0 0 1-.708 0l-3.5-3.5a.5.5 0 1 1 .708-.708L6.5 10.293l6.646-6.647a.5.5 0 0 1 .708 0z"></path></symbol><symbol id="circle-half" viewbox="0 0 16 16"><path d="M8 15A7 7 0 1 0 8 1v14zm0 1A8 8 0 1 1 8 0a8 8 0 0 1 0 16z"></path></symbol><symbol id="moon-stars-fill" viewbox="0 0 16 16"><path d="M6 .278a.768.768 0 0 1 .08.858 7.208 7.208 0 0 0-.878 3.46c0 4.021 3.278 7.277 7.318 7.277.527 0 1.04-.055 1.533-.16a.787.787 0 0 1 .81.316.733.733 0 0 1-.031.893A8.349 8.349 0 0 1 8.344 16C3.734 16 0 12.286 0 7.71 0 4.266 2.114 1.312 5.124.06A.752.752 0 0 1 6 .278z"></path><path d="M10.794 3.148a.217.217 0 0 1 .412 0l.387 1.162c.173.518.579.924 1.097 1.097l1.162.387a.217.217 0 0 1 0 .412l-1.162.387a1.734 1.734 0 0 0-1.097 1.097l-.387 1.162a.217.217 0 0 1-.412 0l-.387-1.162A1.734 1.734 0 0 0 9.31 6.593l-1.162-.387a.217.217 0 0 1 0-.412l1.162-.387a1.734 1.734 0 0 0 1.097-1.097l.387-1.162zM13.863.099a.145.145 0 0 1 .274 0l.258.774c.115.346.386.617.732.732l.774.258a.145.145 0 0 1 0 .274l-.774.258a1.156 1.156 0 0 0-.732.732l-.258.774a.145.145 0 0 1-.274 0l-.258-.774a1.156 1.156 0 0 0-.732-.732l-.774-.258a.145.145 0 0 1 0-.274l.774-.258c.346-.115.617-.386.732-.732L13.863.1z"></path></symbol><symbol id="sun-fill" viewbox="0 0 16 16"><path d="M8 12a4 4 0 1 0 0-8 4 4 0 0 0 0 8zM8 0a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 0zm0 13a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 13zm8-5a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2a.5.5 0 0 1 .5.5zM3 8a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2A.5.5 0 0 1 3 8zm10.657-5.657a.5.5 0 0 1 0 .707l-1.414 1.415a.5.5 0 1 1-.707-.708l1.414-1.414a.5.5 0 0 1 .707 0zm-9.193 9.193a.5.5 0 0 1 0 .707L3.05 13.657a.5.5 0 0 1-.707-.707l1.414-1.414a.5.5 0 0 1 .707 0zm9.193 2.121a.5.5 0 0 1-.707 0l-1.414-1.414a.5.5 0 0 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .707zM4.464 4.465a.5.5 0 0 1-.707 0L2.343 3.05a.5.5 0 1 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .708z"></path></symbol></svg><a class="visually-hidden-focusable skip-link" href="#main-content">Skip to main content</a>
  <div class="container-fluid top-nav-container">
    <div class="col-md-8">
      <div class="large-logo">
        <img id="incubator-logo" alt="Carpentries Incubator" src="../assets/images/incubator-logo.svg"><span class="badge text-bg-danger">
          <abbr title="This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught.">
            <a href="https://docs.carpentries.org/resources/curriculum/lesson-life-cycle.html" class="external-link alert-link">
              <i aria-hidden="true" class="icon" data-feather="alert-octagon" style="border-radius: 5px"></i>
              Pre-Alpha
            </a>
            <span class="visually-hidden">This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught.</span>
          </abbr>
        </span>

      </div>
    </div>
    <div class="selector-container">
      <div id="theme-selector">
        <li class="nav-item dropdown" id="theme-button-list">
          <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
            <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="bd-theme-text"><li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                Light
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                Dark
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                Auto
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
          </ul></li>
      </div>

      <div class="dropdown" id="instructor-dropdown">
        <button class="btn btn-secondary dropdown-toggle bordered-button" type="button" id="dropdownMenu1" data-bs-toggle="dropdown" aria-expanded="false">
          <i aria-hidden="true" class="icon" data-feather="eye"></i> Instructor View <i data-feather="chevron-down"></i>
        </button>
        <ul class="dropdown-menu" aria-labelledby="dropdownMenu1"><li><button class="dropdown-item" type="button" onclick="window.location.href='../01-introduction.html';">Learner View</button></li>
        </ul></div>
    </div>
  </div>
  <hr></header><nav class="navbar navbar-expand-xl bottom-nav incubator" aria-label="Main Navigation"><div class="container-fluid nav-container">
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle Navigation">
      <span class="navbar-toggler-icon"></span>
      <span class="menu-title">Menu</span>
    </button>
    <div class="nav-logo">
      <img class="small-logo" alt="Carpentries Incubator" src="../assets/images/incubator-logo-sm.svg"></div>
    <div class="lesson-title-md">
      Fundamentals of Natural Language Processing (NLP) in Python
    </div>
    <div class="search-icon-sm">
      <!-- TODO: do not show until we have search
        <i role="img" aria-label="Search the All In One page" data-feather="search"></i>
      -->
    </div>
    <div class="desktop-nav">
      <ul class="navbar-nav me-auto mb-2 mb-lg-0"><li class="nav-item">
          <span class="lesson-title">
            Fundamentals of Natural Language Processing (NLP) in Python
          </span>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/key-points.html">Key Points</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/instructor-notes.html">Instructor Notes</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/images.html">Extract All Images</a>
        </li>
        <li class="nav-item dropdown">
          <button class="nav-link dropdown-toggle" id="navbarDropdown" data-bs-toggle="dropdown" aria-expanded="false">
            More <i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu" aria-labelledby="navbarDropdown"><hr></ul></li>
      </ul></div>
    <!--
    <form class="d-flex col-md-2 search-form">
      <fieldset disabled>
      <input class="form-control me-2 searchbox" type="search" placeholder="" aria-label="">
        <button class="btn btn-outline-success tablet-search-button"  type="submit">
          <i class="search-icon" data-feather="search" role="img" aria-label="Search the All In One page"></i>
        </button>
      </fieldset>
    </form>
    -->
    <a id="search-button" class="btn btn-primary" href="../instructor/aio.html" role="button" aria-label="Search the All In One page">Search the All In One page</a>
  </div><!--/div.container-fluid -->
</nav><div class="col-md-12 mobile-title">
  Fundamentals of Natural Language Processing (NLP) in Python
</div>

<aside class="col-md-12 lesson-progress"><div style="width: 0%" class="percentage">
    0%
  </div>
  <div class="progress incubator">
    <div class="progress-bar incubator" role="progressbar" style="width: 0%" aria-valuenow="0" aria-label="Lesson Progress" aria-valuemin="0" aria-valuemax="100">
    </div>
  </div>
</aside><div class="container">
      <div class="row">
        <!-- START: inst/pkgdown/templates/navbar.html -->
<div id="sidebar-col" class="col-lg-4">
  <div id="sidebar" class="sidebar">
      <nav aria-labelledby="flush-headingEleven"><button role="button" aria-label="close menu" alt="close menu" aria-expanded="true" aria-controls="sidebar" class="collapse-toggle" data-collapse="Collapse " data-episodes="Episodes ">
          <i class="search-icon" data-feather="x" role="img"></i>
        </button>
        <div class="sidebar-inner">
          <div class="row mobile-row" id="theme-row-mobile">
            <div class="col" id="theme-selector">
              <li class="nav-item dropdown" id="theme-button-list">
                <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
                  <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><span class="d-lg-none ms-1" id="bd-theme-text">Toggle Theme</span>
                </button>
                <ul class="dropdown-menu dropdown-menu-right" aria-labelledby="bd-theme-text"><li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                      Light
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                      Dark
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                      <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                      Auto
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                </ul></li>
            </div>
          </div>
          <div class="row mobile-row">
            <div class="col">
              <div class="sidenav-view-selector">
                <div class="accordion accordion-flush" id="accordionFlush9">
                  <div class="accordion-item">
                    <h2 class="accordion-header" id="flush-headingNine">
                      <button class="accordion-button collapsed" id="instructor" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseNine" aria-expanded="false" aria-controls="flush-collapseNine">
                        <i id="eye" aria-hidden="true" class="icon" data-feather="eye"></i> Instructor View
                      </button>
                    </h2>
                    <div id="flush-collapseNine" class="accordion-collapse collapse" aria-labelledby="flush-headingNine" data-bs-parent="#accordionFlush2">
                      <div class="accordion-body">
                        <a href="../01-introduction.html">Learner View</a>
                      </div>
                    </div>
                  </div><!--/div.accordion-item-->
                </div><!--/div.accordion-flush-->
              </div><!--div.sidenav-view-selector -->
            </div><!--/div.col -->

            <hr></div><!--/div.mobile-row -->

          <div class="accordion accordion-flush" id="accordionFlush11">
            <div class="accordion-item">

              <button id="chapters" class="accordion-button show" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseEleven" aria-expanded="false" aria-controls="flush-collapseEleven">
                <h2 class="accordion-header chapters" id="flush-headingEleven">
                  EPISODES
                </h2>
              </button>
              <div id="flush-collapseEleven" class="accordion-collapse show collapse" aria-labelledby="flush-headingEleven" data-bs-parent="#accordionFlush11">

                <div class="accordion-body">
                  <div class="accordion accordion-flush" id="accordionFlush1">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading1">
        <a href="index.html">Summary and Schedule</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlushcurrent">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-headingcurrent">
      <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapsecurrent" aria-expanded="true" aria-controls="flush-collapsecurrent">
        <span class="visually-hidden">Current Chapter</span>
        <span class="current-chapter">
        1. Introduction
        </span>
      </button>
    </div><!--/div.accordion-header-->

    <div id="flush-collapsecurrent" class="accordion-collapse collapse show" aria-labelledby="flush-headingcurrent" data-bs-parent="#accordionFlushcurrent">
      <div class="accordion-body">
        <ul><li><a href="#what-is-nlp">What is NLP?</a></li>
<li><a href="#language-as-data">Language as Data</a></li>
<li><a href="#why-should-we-learn-nlp-fundamentals">Why should we learn NLP Fundamentals?</a></li>
<li><a href="#relevant-linguistic-aspects">Relevant Linguistic Aspects</a></li>
<li><a href="#nlp-machine-learning-linguistics">NLP = Machine Learning + Linguistics</a></li>
        </ul></div><!--/div.accordion-body-->
    </div><!--/div.accordion-collapse-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush3">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading3">
        <a href="02-preprocessing.html">2. Episode 1: From text to vectors</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush4">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading4">
        <a href="03-transformers.html">3. Episode 2: BERT and Transformers</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush5">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading5">
        <a href="04-LargeLanguageModels.html">4. Episode 3: Using large language models</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

                </div>
              </div>
            </div>

            <hr class="half-width"><div class="accordion accordion-flush lesson-resources" id="accordionFlush12">
              <div class="accordion-item">
                <h2 class="accordion-header" id="flush-headingTwelve">
                  <button class="accordion-button collapsed" id="lesson-resources" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseTwelve" aria-expanded="false" aria-controls="flush-collapseTwelve">
                    RESOURCES
                  </button>
                </h2>
                <div id="flush-collapseTwelve" class="accordion-collapse collapse" aria-labelledby="flush-headingTwelve" data-bs-parent="#accordionFlush12">
                  <div class="accordion-body">
                    <ul><li>
                        <a href="../instructor/key-points.html">Key Points</a>
                      </li>
                      <li>
                        <a href="../instructor/instructor-notes.html">Instructor Notes</a>
                      </li>
                      <li>
                        <a href="../instructor/images.html">Extract All Images</a>
                      </li>
                      <hr></ul></div>
                </div>
              </div>
            </div>
            <hr class="half-width lesson-resources"><a href="../instructor/aio.html">See all in one page</a>


            <hr class="d-none d-sm-block d-md-none"><div class="d-grid gap-1">

            </div>
          </div><!-- /div.accordion -->
        </div><!-- /div.sidebar-inner -->
      </nav></div><!-- /div.sidebar -->
  </div><!-- /div.sidebar-col -->
<!-- END:   inst/pkgdown/templates/navbar.html-->

        <!-- START: inst/pkgdown/templates/content-instructor.html -->
  <div class="col-xl-8 col-lg-12 primary-content">
    <nav class="lesson-content mx-md-4" aria-label="Previous and Next Chapter"><!-- content for small screens --><div class="d-block d-sm-block d-md-none">
        <a class="chapter-link" href="../instructor/index.html"><i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>Previous</a>
        <a class="chapter-link float-end" href="../instructor/02-preprocessing.html">Next<i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i></a>
      </div>
      <!-- content for large screens -->
      <div class="d-none d-sm-none d-md-block">
        <a class="chapter-link" href="../instructor/index.html" rel="prev">
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>
          Home
        </a>
        <a class="chapter-link float-end" href="../instructor/02-preprocessing.html" rel="next">
          Next: Episode 1: From text...
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i>
        </a>
      </div>
      <hr></nav><main id="main-content" class="main-content"><div class="container lesson-content">
        <h1>Introduction</h1>
        <p>Last updated on 2025-09-18 |

        <a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/edit/main/episodes/01-introduction.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>



        <p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 120 minutes</p>

        <div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>

        

<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul><li>What is Natural Language Processing?</li>
<li>What are some common applications of NLP?</li>
<li>What makes text different from other data?</li>
<li>Why not just learn Large Language Models?</li>
<li>What linguistic properties should we consider when dealing with
texts?</li>
<li>How does NLP relates to Deep Learning methodologies?</li>
</ul></div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul><li>Define Natural Language Processing</li>
<li>Show the most relevant NLP tasks and applications in practice</li>
<li>Learn how to handle Linguistic Data and how is Linguistics relevant
to NLP</li>
<li>Learn a general workflow for solving NLP tasks</li>
</ul></div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="what-is-nlp">What is NLP?<a class="anchor" aria-label="anchor" href="#what-is-nlp"></a></h2>
<hr class="half-width"><p>Natural language processing (NLP) is an area of research and
application that focuses on making human languages accessible to
computers, so that they can perform useful tasks. It is therefore not a
single method, but a collection of techniques that help us deal with
linguistic inputs. The range of techniques covers from simple word
counts, to Machine Learning (ML) methods, and all the way into using
complex Deep Learning (DL) architectures.</p>
<p>The term “natural language” is used as opposed to “artificial
language”, such as programming languages, which are by design
constructed to be easily formalized into machine-readable instructions.
On the contrary, natural languages are complex, ambiguous, and heavily
context-dependent, making them challenging for computers to process. To
complicate it more, there is not only a single <em>human language</em>,
nowadays more than 7000 languages are spoken around the world, each with
its own grammar, vocabulary, and cultural context.</p>
<p>In this course we will mainly focus on written English (and a few
other languages in some specific examples as well), however this is only
a convenience so we can concentrate on the technical aspects of
processing textual data. While ideally most of the concepts from NLP
apply to most languages, one should always be ware that certain
languages require different approaches to solve seemingly similar
problems.</p>
<p>We can already find differences on the most basic step to processing
text. Take the problem of segmenting text into meaningful units, most of
the times these units are words, in NLP this is the task of
<strong>tokenization</strong>. A naive approach is to split text by
spaces, as it seems obvious that we always separate words with spaces.
Let’s see how can we segment the same sentence in English and
Chinese:</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a>english_sentence <span class="op">=</span> <span class="st">"Tokenization isn't always trivial."</span></span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a>chinese_sentence <span class="op">=</span> <span class="st">"标记化并不总是那么简单"</span> <span class="co"># Chinese Translation</span></span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a>english_words <span class="op">=</span> english_sentence.split(<span class="st">" "</span>)</span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="bu">print</span>(english_words)</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(english_words))</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a>chinese_words <span class="op">=</span> chinese_sentence.split(<span class="st">" "</span>)</span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a><span class="bu">print</span>(chinese_words)</span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(chinese_words))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>['Tokenization', "isn't", 'always', 'trivial.']
4
['标记化并不总是那么简单']
1</code></pre>
</div>
<p>Let’s look first at the English sentence. Words are mostly well
separated, however we do not get fully “clean” words (we have
punctuation and also special cases such as “isn’t”), but at least we get
a rough count of the words present in the sentence. The same example
however did not work in Chinese, because Chinese does not use spaces to
separate words. We need to use a Chinese pre-trained tokenizer, which
uses a dictionary-based approach to properly split the words:</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="im">import</span> jieba  <span class="co"># A popular Chinese text segmentation library</span></span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>chinese_sentence <span class="op">=</span> <span class="st">"标记化并不总是那么简单"</span></span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>chinese_words <span class="op">=</span> jieba.lcut(chinese_sentence)</span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a><span class="bu">print</span>(chinese_words)</span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(chinese_words))  <span class="co"># Output: 7</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>['标记', '化', '并', '不', '总是', '那么', '简单']
7</code></pre>
</div>
<p>We can trust that the output valid, because we are using a verified
library, even though we don’t speak Chinese. Another interesting aspect
is that the Chinese sentence has more words than the English one, even
though they convey the same meaning. This shows the complexity of
dealing with more than one language at a time, like in Machine
Translation.</p>
<div id="pre-trained-models-and-fine-tunning" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="pre-trained-models-and-fine-tunning" class="callout-inner">
<h3 class="callout-title">Pre-trained Models and Fine-tunning</h3>
<div class="callout-content">
<p>These two terms will appear very frequently when talking about NLP.
The term <em>pre-trained</em> is taken from Machine Learning and refers
to a model that has been already optimized using relevant data to
perform a task. It is possible to directly load and use the model
out-of-the-box to apply it to our own dataset. Ideally, released
pre-trained models have already been tested for generalization and
quality of outputs, but it is always important to double check the
evaluation process they were subjected to before using them.</p>
<p>Sometimes a pre-trained model is of good quality, but it does not fit
the nuances of our specific dataset. For example, the model was trained
on newspaper articles but you are interested in poetry. In this case, it
is common to perform <em>fine-tunning</em>, this means that instead of
training your own model from scratch, you start with the knowledge
obtained in the pre-trained model and adjust it (fine-tune it) with your
specific data. If this is done well it leads to increased performance in
the specific task you are trying to solve. The advantage of fine-tunning
is that you do not need a large amount of data to improve the results,
hence the popularity of the technique.</p>
</div>
</div>
</div>
<p>In more general terms, NLP deals with the challenges of correctly
processing and generating text, this can be as simple as counting word
frequencies to detect different writing styles, using statistical
methods to classify texts into different categories, or using deep
neural networks to generate human-like text by exploiting word
co-occurrences in large amounts of texts.</p>
</section><section><h2 class="section-heading" id="language-as-data">Language as Data<a class="anchor" aria-label="anchor" href="#language-as-data"></a></h2>
<hr class="half-width"><p>From a more technical perspective, NLP focuses on applying Machine
Learning techniques to linguistic data. This makes all the difference,
since ML methods expect a structured dataset, with a well defined set of
features that engineers can work with. Your first task as an NLP
practitioner is to <strong>understand what aspects of textual data are
relevant for your application</strong> and apply techniques to
systematically extract meaningful features (if using ML) or appropriate
neural architectures (if using DL) from unstructured data that can help
solve our problem at hand.</p>
<div id="nlp-in-the-real-world" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="nlp-in-the-real-world" class="callout-inner">
<h3 class="callout-title">NLP in the real world</h3>
<div class="callout-content">
<p>Name three to five tools/products that you use on a daily basis and
that you think leverage NLP techniques. To solve this exercise you can
get some help from the web.</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<p>These are some of the most popular NLP-based products that we use on
a daily basis:</p>
<ul><li>Agentic Chatbots (ChatGPT, Perplexity)</li>
<li>Voice-based assistants (e.g., Alexa, Siri, Cortana)</li>
<li>Machine translation (e.g., Google translate, Amazon translate)</li>
<li>Search engines (e.g., Google, Bing, DuckDuckGo)</li>
<li>Keyboard autocompletion on smartphones</li>
<li>Spam filtering</li>
<li>Spell and grammar checking apps</li>
<li>Customer care chatbots</li>
<li>Text summarization tools (e.g., news aggregators)</li>
<li>Sentiment analysis tools (e.g., social media monitoring)</li>
</ul></div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="nlp-tasks">NLP tasks<a class="anchor" aria-label="anchor" href="#nlp-tasks"></a></h3>
<p>The exercise above tells us that a great deal of NLP techniques is
embedded in our daily life. Indeed NLP is an important component in a
wide range of software applications that we use in our daily lives.</p>
<p>There are several ways to describe the tasks that NLP solves. From
the Machine Learning perspective, we have:</p>
<ul><li><p>Supervised tasks: learning to classify texts given a labeled set
of examples</p></li>
<li><p>Unsupervised tasks: exploiting existing patterns from large
amounts of text.</p></li>
</ul><p>From the Deep Learning perspective we can consider different neural
network architectures to tackle an NLP task, such as:</p>
<ul><li><p>Multi-layer Perceptron</p></li>
<li><p>Recurrent Neural Network</p></li>
<li><p>Convolutional Neural Network</p></li>
<li><p>LSTM’s</p></li>
<li><p>Transformer</p></li>
</ul><p>Below we show one possible taxonomy of NLP tasks, where we instead
focus on the problem formulation aspect. The tasks are grouped together
with some of their most prominent applications. This is definitely a
non-exhaustive list, as in reality there are hundreds of them, but it is
a good start:</p>
<ul><li><p><strong>Language Modeling</strong>: Given a sequence of words,
the model predicts the next word. For example, in the sentence “The
capital of France is _____”, the model should predict “Paris” based on
the context. This task was initially useful for building solutions that
require speech and optical character recognition (even handwriting),
language translation and spelling correction. Nowadays this has scaled
up to the LLMs that we know.</p></li>
<li>
<p><strong>Text Classification</strong>: Assign one or more labels
to a “Documents”. A document in our context can mean a sentence, a
paragraph, a book chapter, etc…</p>
<ul><li>
<strong>Language Identification</strong>: determining the language
of a given text.</li>
<li>
<strong>Spam Filtering</strong>: classifiying emails into spam or
not spam based on their content.</li>
<li>
<strong>Authorship Attribution</strong>: determining the author of a
text based on its style and content (based on the assumption that each
author has a unique writing style).</li>
<li>
<strong>Sentiment Analysis</strong>: classifying text into positive,
negative or neutral sentiment. For example, in the sentence “I love this
product!”, the model would classify it as positive sentiment.</li>
</ul></li>
<li>
<p><strong>Token Classification</strong>: The task of assigning
label to words individually. Because words do not occur in isolation,
their meaning depend on the sequence of words to the left or the right
of them, this is also called Word-In-Context Classification or Sequence
Labeling and usually involves syntactic and semantic analysis.</p>
<ul><li>
<strong>Part-Of-Speech Tagging</strong>: is the task of assigning a
part-of-speech label (e.g., noun, verb, adjective) to each word in a
sentence.</li>
<li>
<strong>Chunking</strong>: splitting a running text into “chunks” of
words that together represent a meaningful unit: phrases, sentences,
paragraphs, etc.</li>
<li>
<strong>Word Sense Disambiguation</strong>: based on the context
what does a word mean (think of “book” in “I read a book.” vs “I want to
book a flight.”)</li>
<li>
<strong>Named Entity Recognition</strong>: recognize world entities
in text, e.g. Persons, Locations, Book Titles, or many others. For
example “Mary Shelley” is a person, “Frankenstein or the Modern
Prometeus” is a book, etc.</li>
<li>
<strong>Semantic Role Labeling</strong>: the task if finding out
“Who did what to whom?” in a sentence: information from events such as
agents, participants, circumstances, etc.</li>
<li>
<strong>Relation Extraction</strong>: the task of identifying named
relationships between entities in a text, e.g. “Apple is based in
California” has the relation (Apple, based_in, California).</li>
<li>
<strong>Co-reference Resolution</strong>: the task of determining
which words refer to the same entity in a text, e.g. “Mary is a doctor.
She works at the hospital.” Here “She” refers to “Mary”.</li>
<li>
<strong>Entity Linking</strong>: the task of disambiguation of named
entities in a text, linking them to their corresponding entries in a
knowledge base, e.g. Mary Shelley’s biogrpaphy in Wikipedia.</li>
</ul></li>
<li>
<p><strong>Text Similarity</strong>: The task of determining how
similar two pieces of text are.</p>
<ul><li>
<strong>Plagiarism detection</strong>: determining whether a piece
of TextB is close enough to another known piece of TextA, which
increments the likelihood that it was copied from it.</li>
<li>
<strong>Document clustering</strong>: grouping similar texts
together based on their content.</li>
<li>
<strong>Topic modelling</strong>: A specific instance of clustering,
here we automatically identify abstract “topics” that occur in a set of
documents, where each topic is represented as a cluster of words that
frequently appear together.</li>
<li>
<strong>Information Retrieval</strong>: This is the task of finding
relevant information or documents from a large collection of
unstructured data based on user’s query, e.g., “What’s the best
restaurant near me?”.</li>
</ul></li>
<li>
<p><strong>Text Generation</strong>: The task of generating text
based on a given input. This can</p>
<ul><li>
<strong>Machine Translation</strong>: translating text from one
language to another, e.g., “Hello” in English to “Que tal” in
Spanish.</li>
<li>
<strong>Summarization</strong>: generating a concise summary of a
longer text. It can be abstractive (generating new sentences that
capture the main ideas of the original text) but also extractive
(selecting important sentences from the original text).</li>
<li>
<strong>Paraphrasing</strong>: generating a new sentence that
conveys the same meaning as the original sentence, e.g., “The cat is on
the mat.” to “The mat has a cat on it.”.</li>
<li>
<strong>Question Answering</strong>: Given a question and a context,
the model generates an answer. For example, given the question “What is
the capital of France?” and the Wikipedia article about France as the
context, the model should answer “Paris”. This task can be approached as
a text classification problem (where the answer is one of the predefined
options) or as a generative task (where the model generates the answer
from scratch).</li>
<li>
<strong>Conversational Agent (ChatBot)</strong>: Building a system
that interacts with a user via natural language, e.g., “What’s the
weather today, Siri?”. These agents are widely used to improve user
experience in customer service, personal assistance and many other
domains.</li>
</ul></li>
</ul></div>
<div class="section level3">
<h3 id="what-is-a-word">What is a word?<a class="anchor" aria-label="anchor" href="#what-is-a-word"></a></h3>
<p>When dealing with language we deal with sequences of words and with
how they relate to each other to generate meaning. Our first step to
provide structure to text is therefore to split it into words.</p>
<div id="token-vs-word" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="token-vs-word" class="callout-inner">
<h3 class="callout-title">Token vs Word</h3>
<div class="callout-content">
<p>For simplicity, in the rest of the course we will use the terms
“word” and “token” interchangeably, but as we just saw they do not
always have the same granularity. Originally the concept of token
comprised dictionary words, numeric symbols and punctuation. Nowadays,
tokenization has also evolved and became an optimization task on its own
(How can we segment text in a way that neural networks learn optimally
from text?). Tokenizers always allow to “reconstruct back” tokens to
human-readable words even if internally they split the text differently,
hence we can afford to use them as synonyms. If you are curious, you can
visualize how different state-of-the-art tokenizers work <a href="https://tiktokenizer.vercel.app/" class="external-link">here</a></p>
</div>
</div>
</div>
<p>Finally, we will start to working with text data! Let’s open a file,
read it into a string and split it by spaces. We will print the original
text and the list of “words” to see how they look:</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">"text1_clean.txt"</span>) <span class="im">as</span> f:</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>  text <span class="op">=</span> f.read()</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a><span class="bu">print</span>(text[:<span class="dv">100</span>])</span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Length:"</span>, <span class="bu">len</span>(text))</span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a>proto_tokens <span class="op">=</span> text.split()</span>
<span id="cb5-8"><a href="#cb5-8" tabindex="-1"></a><span class="bu">print</span>(proto_tokens[:<span class="dv">40</span>])</span>
<span id="cb5-9"><a href="#cb5-9" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(proto_tokens))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Letter 1 St. Petersburgh, Dec. 11th, 17-- TO Mrs. Saville, England You will rejoice to hear that no disaster has accompanied the commencement of an en
Length: 417931

Proto-Tokens:
['Letter', '1', 'St.', 'Petersburgh,', 'Dec.', '11th,', '17--', 'TO', 'Mrs.', 'Saville,', 'England', 'You', 'will', 'rejoice', 'to', 'hear', 'that', 'no', 'disaster', 'has', 'accompanied', 'the', 'commencement', 'of', 'an', 'enterprise', 'which', 'you', 'have', 'regarded', 'with', 'such', 'evil', 'forebodings.', 'I', 'arrived', 'here', 'yesterday,', 'and', 'my']
74942</code></pre>
</div>
<p>Splitting by white space is possible but needs several extra steps to
do get the clean words and separate the punctuation appropriately.
Instead, we will introduce the [spaCy]((<a href="https://github.com/explosion/spaCy" class="external-link uri">https://github.com/explosion/spaCy</a>) library to segment
the text into human-readable tokens. First we will download the
pre-trained model, in this case we only need the small English
version:</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="op">!</span> python <span class="op">-</span>m spacy download en_core_web_sm</span></code></pre>
</div>
<p>This is a model that spaCy already trained for us on a subset of web
English data. Hence, the model already “knows” how to tokenize into
English words. When the model processes a string, it does not only do
the splitting for us but already provides more advanced linguistic
properties of the tokens (such as part-of-speech tags, or named
entities). Let’s now import the model and use it to parse our
document:</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="im">import</span> spacy</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a>nlp <span class="op">=</span> spacy.load(<span class="st">"en_core_web_sm"</span>) <span class="co"># we load the small English model for efficiency</span></span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a>doc <span class="op">=</span> nlp(text) <span class="co"># Doc is a python object with several methods to retrieve linguistic properties</span></span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" tabindex="-1"></a><span class="co"># SpaCy-Tokens</span></span>
<span id="cb8-8"><a href="#cb8-8" tabindex="-1"></a>tokens <span class="op">=</span> [token.text <span class="cf">for</span> token <span class="kw">in</span> doc] <span class="co"># Note that spacy tokens are also python objects </span></span>
<span id="cb8-9"><a href="#cb8-9" tabindex="-1"></a><span class="bu">print</span>(tokens[:<span class="dv">40</span>])</span>
<span id="cb8-10"><a href="#cb8-10" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(tokens))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>['Letter', '1', 'St.', 'Petersburgh', ',', 'Dec.', '11th', ',', '17', '-', '-', 'TO', 'Mrs.', 'Saville', ',', 'England', 'You', 'will', 'rejoice', 'to', 'hear', 'that', 'no', 'disaster', 'has', 'accompanied', 'the', 'commencement', 'of', 'an', 'enterprise', 'which', 'you', 'have', 'regarded', 'with', 'such', 'evil', 'forebodings', '.']
85713</code></pre>
</div>
<p>The differences look subtle at the beginning, but if we carefully
inspect the way spaCy splits the text, we can see the advantage of using
a proper tokenizer. There are also a several of properties that spaCy
provides us with, for example we can get only symbols or only
alphanumerical tokens, and more advanced linguistic properties, for
example we can remove punctuation and only keep alphanumerical
tokens:</p>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a>only_words <span class="op">=</span> [token <span class="cf">for</span> token <span class="kw">in</span> doc <span class="cf">if</span> token.is_alpha]  <span class="co"># Only alphanumerical tokens</span></span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a><span class="bu">print</span>(only_words[:<span class="dv">50</span>])</span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(only_words))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[Letter, Petersburgh, TO, Saville, England, You, will, rejoice, to, hear]
1199</code></pre>
</div>
<p>or keep only the verbs from our text:</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a>only_verbs <span class="op">=</span> [token <span class="cf">for</span> token <span class="kw">in</span> doc <span class="cf">if</span> token.pos_ <span class="op">==</span> <span class="st">"VERB"</span>]  <span class="co"># Only verbs</span></span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a><span class="bu">print</span>(only_verbs[:<span class="dv">10</span>])</span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(only_verbs))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[rejoice, hear, accompanied, regarded, arrived, assure, increasing, walk, feel, braces]
150</code></pre>
</div>
<p>SpaCy also predicts the sentences under the hood for us. We can
access them like this:</p>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a>sentences <span class="op">=</span> [sent.text <span class="cf">for</span> sent <span class="kw">in</span> doc.sents] <span class="co"># Sentences are also python objects</span></span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a><span class="bu">print</span>(sentences[:<span class="dv">5</span>])</span>
<span id="cb14-3"><a href="#cb14-3" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(sentences))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>48
Letter 1 St. Petersburgh, Dec. 11th, 17-- TO Mrs. Saville, England You will rejoice to hear that no disaster has accompanied the commencement of an enterprise which you have regarded with such evil forebodings.
I arrived here yesterday, and my first task is to assure my dear sister of my welfare and increasing confidence in the success of my undertaking.
I am already far north of London, and as I walk in the streets of Petersburgh, I feel a cold northern breeze play upon my cheeks, which braces my nerves and fills me with delight.
Do you understand this feeling?
This breeze, which has travelled from the regions towards which I am advancing, gives me a foretaste of those icy climes.</code></pre>
</div>
<p>We can also see what named entities the model predicted:</p>
<div class="codewrapper sourceCode" id="cb16">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(doc.ents))</span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a><span class="cf">for</span> ent <span class="kw">in</span> doc.ents[:<span class="dv">5</span>]:</span>
<span id="cb16-3"><a href="#cb16-3" tabindex="-1"></a>    <span class="bu">print</span>(ent.label_, ent.text)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>1713
DATE Dec. 11th
CARDINAL 17
PERSON Saville
GPE England
DATE yesterday</code></pre>
</div>
<p>This are just basic tests to show you how you can right away
structure text using existing NLP libraries. Of course we used a
simplified model so the more complex the task the more errors will
appear. The biggest advantage of using these existing libraries is that
they help you transform unstructured plain text files into structured
data that you can manipulate later for your own goals.</p>
<div id="nlp-libraries" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="nlp-libraries" class="callout-inner">
<h3 class="callout-title">NLP Libraries</h3>
<div class="callout-content">
<p>Related to the need of shaping our problems into a known task, there
are several existing NLP libraries which provide a wide range of models
that we can use out-of-the-box. We already saw simple examples using
SpaCy for English and jieba for Chinese. Again, as a non-exhaustive
list, we mention here some of the most used NLP libraries in python:</p>
<ul><li><a href="https://github.com/nltk/nltk" class="external-link">NLTK</a></li>
<li><a href="https://github.com/explosion/spaCy" class="external-link">spaCy</a></li>
<li><a href="https://github.com/RaRe-Technologies/gensim" class="external-link">Gensim</a></li>
<li><a href="https://github.com/stanfordnlp/stanza" class="external-link">Stanza</a></li>
<li><a href="https://github.com/flairNLP/flair" class="external-link">Flair</a></li>
<li><a href="https://github.com/facebookresearch/fastText" class="external-link">FastText</a></li>
<li><a href="https://github.com/huggingface/transformers" class="external-link">HuggingFace
Transformers</a></li>
</ul><div class="section level3">
<h3 id="linguistic-resources">Linguistic Resources<a class="anchor" aria-label="anchor" href="#linguistic-resources"></a></h3>
<p>There are also several curated resources that can help solve your
NLP-related tasks, specifically when you need highly specialized
definitions. An exhaustive list would be impossible as there are
thousands of them, and also them being language and domain dependent.
Below we mention some of the most prominent, just to give you an idea of
the kind of resources you can find, so you don’t need to reinvent the
wheel every time you start a project:</p>
<ul><li>
<a href="https://huggingface.co/datasets" class="external-link">HuggingFace Datasets</a>:
A large collection of datasets for NLP tasks, including text
classification, question answering, and language modeling.</li>
<li>
<a href="https://wordnet.princeton.edu/" class="external-link">WordNet</a>: A large
lexical database of English, where words are grouped into sets of
synonyms (synsets) and linked by semantic relations.</li>
<li>
<a href="https://www.europarl.europa.eu/ep-search/search.do?language=en" class="external-link">Europarl</a>:
A parallel corpus of the proceedings of the European Parliament,
available in 21 languages, which can be used for machine translation and
cross-lingual NLP tasks.</li>
<li>
<a href="https://universaldependencies.org/" class="external-link">Universal
Dependencies</a>: A collection of syntactically annotated treebanks
across 100+ languages, providing a consistent annotation scheme for
syntactic and morphological properties of words, which can be used for
cross-lingual NLP tasks.</li>
<li>
<a href="https://propbank.github.io/" class="external-link">PropBank</a>: A corpus of
texts annotated with information about basic semantic propositions,
which can be used for English semantic tasks.</li>
<li>
<a href="https://framenet.icsi.berkeley.edu/fndrupal/" class="external-link">FrameNet</a>:
A lexical resource that provides information about the semantic frames
that underlie the meanings of words (mainly verbs and nouns), including
their roles and relations.</li>
<li>
<a href="https://babelnet.org/" class="external-link">BabelNet</a>: A multilingual lexical
resource that combines WordNet and Wikipedia, providing a large number
of concepts and their relations in multiple languages.</li>
<li>
<a href="https://www.wikidata.org/" class="external-link">Wikidata</a>: A free and open
knowledge base initially derived from Wikipedia, that contains
structured data about entities, their properties and relations, which
can be used to enrich NLP applications.</li>
<li>
<a href="https://github.com/allenai/dolma" class="external-link">Dolma</a>: An open
dataset of 3 trillion tokens from a diverse mix of clean web content,
academic publications, code, books, and encyclopedic materials, used to
train English large language models.</li>
</ul></div>
</div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="why-should-we-learn-nlp-fundamentals">Why should we learn NLP Fundamentals?<a class="anchor" aria-label="anchor" href="#why-should-we-learn-nlp-fundamentals"></a></h2>
<hr class="half-width"><p>In the past decade, NLP has evolved significantly, especially in the
field of deep learning, to the point that it has become embedded in our
daily lives, one just needs to look at the term Large Language Models
(LLMs), the latest generation of NLP models, which is now ubiquitous in
news media and tech products we use on a daily basis.</p>
<p>The term LLM now is often (and wrongly) used as a synonym of
Artificial Intelligence. We could therefore think that today we just
need to learn how to manipulate LLMs in order to fulfill our research
goals involving textual data. The truth is that Language Modeling has
always been part of the core tasks of NLP, therefore, by learning NLP
you will understand better where are the main ideas behind LLMs coming
from.</p>
<figure><img src="../fig/intro0_cs_nlp.png" alt="NLP is an interdisciplinary field, and LLMs are just a subset of it" class="figure mx-auto d-block"><div class="figcaption">NLP is an interdisciplinary field, and LLMs are
just a subset of it</div>
</figure><p>LLM is a blanket term for an assembly of large neural networks that
are trained on vast amounts of text data with the objective of
optimizing for language modeling. Once they are trained, they are used
to generate human-like text or fine-tunned to perform much more advanced
tasks. Indeed, the surprising and fascinating properties that emerge
from training models at this scale allows us to solve different complex
tasks such as answer elaborate questions, translate languages, solve
complex problems, generate narratives that emulate reasoning, and many
more, all of this with a single tool.</p>
<p>It is important, however, to pay attention to what is happening
behind the scenes in order to be able <strong>trace sources of errors
and biases</strong> that get hidden in the complexity of these models.
The purpose of this course is precisely to take a step back, and
understand that: - There is a wide variety of tools available, beyond
LLMs, that do not require so much computing power. - Sometimes a much
simpler and easier method is already available that can solve our
problem at hand. - If we learn how previous approaches to solve
linguistic problems were designed, we can better understand the
limitations of LLMs and how to use them effectively. - LLMs excel at
confidently delivering information, without any regards for correctness.
This calls for a careful design of <strong>evaluation metrics</strong>
that give us a better understanding of the quality of the generated
content.</p>
<p>Let’s go back to our problem of segmenting text and see what ChatGPT
has to say about tokenizing Chinese text:</p>
<figure><img src="../fig/intro1.png" alt="ChatGPT Just Works! Does it…?" class="figure mx-auto d-block"><div class="figcaption">ChatGPT Just Works! Does it…?</div>
</figure><p>We got what sounds like a straightforward confident answer. However,
it is not clear how the model arrived at this solution. Second, we do
not know whether the solution is correct or not. In this case ChatGPT
made some assumptions for us, such as choosing a specific kind of
tokenizer to give the answer, and since we do not speak the language, we
do not know if this is indeed the best approach to tokenize Chinese
text. If we understand the concept of Token (which we will today!), then
we can be more informed about the quality of the answer, whether it is
useful to us, and therefore make a better use of the model.</p>
<p>And by the way, ChatGPT was <strong>almost</strong> correct, in the
specific case of the gpt-4 tokenizer, the model will return 12 tokens
(not 11!) for the given Chinese sentence.</p>
<figure><img src="../fig/intro1b.png" alt="GPT-4 Tokenization Example" class="figure mx-auto d-block"><div class="figcaption">GPT-4 Tokenization Example</div>
</figure><p>We can also argue if the statement “Chinese is generally tokenized
character by character” is an overstatement or not. In any case, the
real question here is: Are we ok with <em>almost correct answers</em>?
Please note that this is not a call to avoid using LLM’s but a call for
a careful consideration of usage and more importantly, an attempt to
explain the mechanisms behind via NLP concepts.</p>
</section><section><h2 class="section-heading" id="relevant-linguistic-aspects">Relevant Linguistic Aspects<a class="anchor" aria-label="anchor" href="#relevant-linguistic-aspects"></a></h2>
<hr class="half-width"><p>Natural language exhibits a set fo properties that make it more
challenging to process than other types of data such as tables,
spreadsheets or time series. <strong>Language is hard to process because
it is compositional, ambiguous, discrete and sparse</strong>.</p>
<div class="section level3">
<h3 id="compositionality">Compositionality<a class="anchor" aria-label="anchor" href="#compositionality"></a></h3>
<p>The basic elements of written languages are characters, a sequence of
characters form words, and words in turn denote objects, concepts,
events, actions and ideas (Goldberg, 2016). Subsequently words form
phrases and sentences which are used in communication and depend on the
context in which they are used. We as humans derive the meaning of
utterances from interpreting contextual information that is present at
different levels at the same time:</p>
<figure><img src="../fig/intro2_levels_lang.svg" alt="Levels of Language" class="figure mx-auto d-block"><div class="figcaption">Levels of Language</div>
</figure><p>The first two levels refer to spoken language only, and the other
four levels are present in both speech and text. Because in principle
machines do not have access to the same levels of information that we do
(they can only have independent audio, textual or visual inputs), we
need to come up with clever methods to overcome this significant
limitation. Knowing the levels of language is important so we consider
what kind of problems we are facing when attempting to solve our NLP
task at hand.</p>
</div>
<div class="section level3">
<h3 id="ambiguity">Ambiguity<a class="anchor" aria-label="anchor" href="#ambiguity"></a></h3>
<p>The disambiguation of meaning is usually a by-product of the context
in which utterances are expressed and also the historic accumulation of
interactions which are transmitted across generations (think for
instance to idioms – these are usually meaningless phrases that acquire
meaning only if situated within their historical and societal context).
These characteristics make NLP a particularly challenging field to work
in.</p>
<p>We cannot expect a machine to process human language and simply
understand it as it is. We need a systematic, scientific approach to
deal with it. It’s within this premise that the field of NLP is born,
primarily interested in converting the building blocks of human/natural
language into something that a machine can understand.</p>
<p>The image below shows how the levels of language relate to a few NLP
applications:</p>
<figure><img src="../fig/intro3_levels_nlp.png" alt="Diagram showing building blocks of language" class="figure mx-auto d-block"><div class="figcaption">Diagram showing building blocks of
language</div>
</figure><div id="levels-of-ambiguity" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="levels-of-ambiguity" class="callout-inner">
<h3 class="callout-title">Levels of ambiguity</h3>
<div class="callout-content">
<p>Discuss what do the following sentences mean. What level of ambiguity
do they represent?:</p>
<ul><li>“The door is unlockable from the inside.” vs “Unfortunately, the
cabinet is unlockable, so we can’t secure it”</li>
<li>“I saw the <em>cat with the stripes</em>” vs “I saw the cat <em>with
the telescope</em>”</li>
<li>“Colorless green ideas sleep furiously”</li>
<li>“I never said she stole my money.” vs “I never said she stole my
money.”</li>
</ul></div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2"> Show me the solution </h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" data-bs-parent="#accordionSolution2" aria-labelledby="headingSolution2">
<div class="accordion-body">
<p>This is why the previous statements were difficult:</p>
<ul><li>“Un-lockable vs Unlock-able” is a <strong>Morphological</strong>
ambiguity: Same word form, two possible meanings</li>
<li>“I saw the cat with the telescope” is a <strong>Syntactic</strong>
ambiguity: Same sentence structure, different properties</li>
<li>“Colorless green ideas sleep furiously” <strong>Semantic</strong>
ambiguity: Grammatical but meaningless (ideas do not have color as a
property. Even if this was true, they would be either colorless or
green)</li>
<li>“I NEVER said she stole MY money.” is a <strong>Pragmatic</strong>
ambiguity: Meaning relies on word emphasis</li>
</ul></div>
</div>
</div>
</div>
<p>Whenever you are solving a specific task, you should ask yourself
what kind of ambiguity can affect your results? At what level are your
assumptions operating when defining your research questions? Having the
answers to this can save you a lot of time when debugging your models.
Sometimes the most innocent assumptions (for example using the wrong
tokenizer) can create enormous performance drops even when the higher
level assumptions were correct.</p>
</div>
<div class="section level3">
<h3 id="discreteness">Discreteness<a class="anchor" aria-label="anchor" href="#discreteness"></a></h3>
<p>There is no inherent relationship between the form of a word and its
meaning. For the same reason, by textual means alone, there is no way of
knowing if two words are similar or how do they relate to each other.
Take the word “pizza” and “hamburger”, how can we automatically know
that they share more properties than “car” and “cat”? One way is by
looking at the context in which these words are used, and how they are
related to each other. This idea is the principle behind
<strong>distributional semantics</strong>, and aims to look at the
statistical properties of language, such as word co-occurrences, to
understand how words relate to each other.</p>
<p>Let’s do a simple exercise:</p>
<div class="codewrapper sourceCode" id="cb18">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" tabindex="-1"></a></span>
<span id="cb18-2"><a href="#cb18-2" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> Counter</span>
<span id="cb18-3"><a href="#cb18-3" tabindex="-1"></a></span>
<span id="cb18-4"><a href="#cb18-4" tabindex="-1"></a><span class="co"># A mini-corpus where our target words appear</span></span>
<span id="cb18-5"><a href="#cb18-5" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"""</span></span>
<span id="cb18-6"><a href="#cb18-6" tabindex="-1"></a><span class="st">I am hungry . Should I eat delicious pizza ?</span></span>
<span id="cb18-7"><a href="#cb18-7" tabindex="-1"></a><span class="st">Or maybe I should eat a juicy hamburger instead .</span></span>
<span id="cb18-8"><a href="#cb18-8" tabindex="-1"></a><span class="st">Many people like to eat pizza because is tasty , they think pizza is delicious as hell !</span></span>
<span id="cb18-9"><a href="#cb18-9" tabindex="-1"></a><span class="st">My friend prefers to eat a hamburger and I agree with him .</span></span>
<span id="cb18-10"><a href="#cb18-10" tabindex="-1"></a><span class="st">We will drive our car to the restaurant to get the succulent hamburger .</span></span>
<span id="cb18-11"><a href="#cb18-11" tabindex="-1"></a><span class="st">Right now , our cat sleeps on the mat so we won't take him .</span></span>
<span id="cb18-12"><a href="#cb18-12" tabindex="-1"></a><span class="st">I did not wash my car , but at least the car has gasoline .</span></span>
<span id="cb18-13"><a href="#cb18-13" tabindex="-1"></a><span class="st">Perhaps when we come back we will take out the cat for a walk .</span></span>
<span id="cb18-14"><a href="#cb18-14" tabindex="-1"></a><span class="st">The cat will be happy to see us when we come back .</span></span>
<span id="cb18-15"><a href="#cb18-15" tabindex="-1"></a><span class="st">"""</span></span>
<span id="cb18-16"><a href="#cb18-16" tabindex="-1"></a></span>
<span id="cb18-17"><a href="#cb18-17" tabindex="-1"></a>words <span class="op">=</span> [token.lower() <span class="cf">for</span> token <span class="kw">in</span> text.split()]</span>
<span id="cb18-18"><a href="#cb18-18" tabindex="-1"></a></span>
<span id="cb18-19"><a href="#cb18-19" tabindex="-1"></a>target_words <span class="op">=</span> [<span class="st">"pizza"</span>, <span class="st">"hamburger"</span>, <span class="st">"car"</span>, <span class="st">"cat"</span>] <span class="co"># words we want to analyze</span></span>
<span id="cb18-20"><a href="#cb18-20" tabindex="-1"></a>stop_words <span class="op">=</span> [<span class="st">"i"</span>, <span class="st">"am"</span>, <span class="st">"my"</span>, <span class="st">"to"</span>, <span class="st">"the"</span>, <span class="st">"a"</span>, <span class="st">"and"</span>, <span class="st">"is"</span>, <span class="st">"as"</span>, <span class="st">"at"</span>, <span class="st">"we"</span>, <span class="st">"will"</span>, <span class="st">"not"</span>, <span class="st">"our"</span>, <span class="st">"but"</span>, <span class="st">"least"</span>, <span class="st">"has"</span>, <span class="st">"."</span>, <span class="st">","</span>] <span class="co"># words to ignore</span></span>
<span id="cb18-21"><a href="#cb18-21" tabindex="-1"></a>co_occurrence <span class="op">=</span> {word: [] <span class="cf">for</span> word <span class="kw">in</span> target_words}</span>
<span id="cb18-22"><a href="#cb18-22" tabindex="-1"></a>window_size <span class="op">=</span> <span class="dv">3</span> <span class="co"># How many words to look at on each side</span></span>
<span id="cb18-23"><a href="#cb18-23" tabindex="-1"></a></span>
<span id="cb18-24"><a href="#cb18-24" tabindex="-1"></a><span class="co"># Find the context for each target word</span></span>
<span id="cb18-25"><a href="#cb18-25" tabindex="-1"></a><span class="cf">for</span> i, word <span class="kw">in</span> <span class="bu">enumerate</span>(words):</span>
<span id="cb18-26"><a href="#cb18-26" tabindex="-1"></a>    <span class="cf">if</span> word <span class="kw">in</span> target_words:</span>
<span id="cb18-27"><a href="#cb18-27" tabindex="-1"></a>        start <span class="op">=</span> <span class="bu">max</span>(<span class="dv">0</span>, i <span class="op">-</span> window_size)</span>
<span id="cb18-28"><a href="#cb18-28" tabindex="-1"></a>        end <span class="op">=</span> <span class="bu">min</span>(<span class="bu">len</span>(words), i <span class="op">+</span> <span class="dv">1</span> <span class="op">+</span> window_size)</span>
<span id="cb18-29"><a href="#cb18-29" tabindex="-1"></a>        context <span class="op">=</span> words[start:i] <span class="op">+</span> words[i<span class="op">+</span><span class="dv">1</span>:end] <span class="co"># Exclude the target word itself</span></span>
<span id="cb18-30"><a href="#cb18-30" tabindex="-1"></a>        context <span class="op">=</span> [w <span class="cf">for</span> w <span class="kw">in</span> context <span class="cf">if</span> w <span class="kw">not</span> <span class="kw">in</span> stop_words] <span class="co"># Filter out stop words from context</span></span>
<span id="cb18-31"><a href="#cb18-31" tabindex="-1"></a>        co_occurrence[word].extend(context)</span>
<span id="cb18-32"><a href="#cb18-32" tabindex="-1"></a></span>
<span id="cb18-33"><a href="#cb18-33" tabindex="-1"></a><span class="co"># Print the most common context words for each target word</span></span>
<span id="cb18-34"><a href="#cb18-34" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Contextual Fingerprints:</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb18-35"><a href="#cb18-35" tabindex="-1"></a><span class="cf">for</span> word, context_list <span class="kw">in</span> co_occurrence.items():</span>
<span id="cb18-36"><a href="#cb18-36" tabindex="-1"></a>    <span class="co"># We use Counter to get a frequency count of context words</span></span>
<span id="cb18-37"><a href="#cb18-37" tabindex="-1"></a>    fingerprint <span class="op">=</span> Counter(context_list).most_common(<span class="dv">5</span>)</span>
<span id="cb18-38"><a href="#cb18-38" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"'</span><span class="sc">{</span>word<span class="sc">}</span><span class="ss">': </span><span class="sc">{</span>fingerprint<span class="sc">}</span><span class="ss">"</span>)</span></code></pre>
</div>
<pre class="ouptut"><code>Contextual Fingerprints:

'pizza': [('eat', 2), ('delicious', 2), ('?', 1), ('or', 1), ('maybe', 1)]
'hamburger': [('eat', 2), ('juicy', 1), ('instead', 1), ('many', 1), ('agree', 1)]
'car': [('drive', 1), ('restaurant', 1), ('wash', 1), ('gasoline', 1)]
'cat': [('walk', 2), ('now', 1), ('sleeps', 1), ('on', 1), ('take', 1)]</code></pre>
<div id="stop-words" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="stop-words" class="callout-inner">
<h3 class="callout-title">Stop Words</h3>
<div class="callout-content">
<p>Depending on the use case you will always perform some preprocessing
steps in order to have the data as normalized and clean as possible
before making any computations. These steps are not exhaustive and are
task-dependent. In this case, we introduced the concept of “stop_words”
(extremely common words that do not provide relevant information for our
use case and, given their high frequency, they tend to obscure the
results we are interested in). Of course, our list could have been much
bigger, but it served the purpose for this toy example.</p>
<p>Spacy has a pre-defined list of stopwords per language. To explicitly
load the English stop words we can do</p>
<div class="codewrapper sourceCode" id="cb20">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" tabindex="-1"></a><span class="im">from</span> spacy.lang.en.stop_words <span class="im">import</span> STOP_WORDS</span>
<span id="cb20-2"><a href="#cb20-2" tabindex="-1"></a><span class="bu">print</span>(STOP_WORDS)  <span class="co"># a set of common stopwords</span></span>
<span id="cb20-3"><a href="#cb20-3" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(STOP_WORDS)) <span class="co"># There are 326 words considered in this list</span></span></code></pre>
</div>
<p>Alternatively you can filter out stop words when iterating your
tokens (remember the spacy token properties!) like this:</p>
<div class="codewrapper sourceCode" id="cb21">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" tabindex="-1"></a>doc <span class="op">=</span> nlp(text)</span>
<span id="cb21-2"><a href="#cb21-2" tabindex="-1"></a>content_words <span class="op">=</span> [token.text <span class="cf">for</span> token <span class="kw">in</span> doc <span class="cf">if</span> token.is_alpha <span class="kw">and</span> <span class="kw">not</span> token.is_stop]  <span class="co"># Filter out stop words and punctuation</span></span>
<span id="cb21-3"><a href="#cb21-3" tabindex="-1"></a><span class="bu">print</span>(content_words)</span></code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="sparsity">Sparsity<a class="anchor" aria-label="anchor" href="#sparsity"></a></h3>
<p>Another key property of linguistic data is its sparsity. This means
that if we are hunting for a specific phenomenon, we will realize it
barely occurs inside a enormous amount of text. Our previous example of
<em>pizzas</em> and <em>hamburgers</em> worked for us because our
experiment was run in an extremely hand-crafted text for the purpose.
However, it would be hard to scale this up in a real setting, where we
would possibly need to dive into public reviews that specifically
mentions those two foods to construct a corpus.</p>
<p>Sparsity is tightly link to what is frequently called
<strong>domain-specific data</strong>. The discourse context in which
language is used varies importantly across disciplines (domains). Take
for example law texts and medical texts, the meaning of concepts
described in each domain will significantly differ. Another example
would be to compare texts written in 16th century English vs 21st
century English, where significant shifts have occurred at all
linguistic levels. For this reason there are specialized models and
corpora that model language use in specific domains. The concept of
fine-tunning a general purpose model with domain-specific data is also
popular.</p>
<p>While it’s true that sparsity has been reduced after the era of LLMs
and big training corpora, especially when dealing with general purpose
tasks, if you have domain specific objectives you should take special
care on the assumptions and results you get out of pre-trained models,
including LLMs.</p>
<div id="nlp-in-the-real-world-1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="nlp-in-the-real-world-1" class="callout-inner">
<h3 class="callout-title">NLP in the real world</h3>
<div class="callout-content">
<p>Use what you have learned so far to search inside the Frankenstein
book how many times does the word “love” appear, and how many times does
“hate” appear. Compute what percentage of the content words in the text
do these two terms together represent? To do this experiment you
should:</p>
<ol style="list-style-type: decimal"><li>Read the file and save it into a text variable</li>
<li>Use spacy to load the text into a Doc object.</li>
<li>Iterate the document and keep all tokens that are alphanumeric (use
the token.is_alpha property), and are not stopwords (use the property
token.is_stop).</li>
<li>Lowercase all the tokens to merge the instances of “Love” and “love”
into a single one.</li>
<li>Iterate the tokens and count how many of them are exactly
“love”</li>
<li>Iterate the tokens and count how many of them are exactly
“hate”</li>
<li>compute the percentage of hate + love compared to all content words.
For example with: (len(hate_words) + len(love_words)) /
len(content_words) * 100</li>
</ol></div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3"> Show me the solution </h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" data-bs-parent="#accordionSolution3" aria-labelledby="headingSolution3">
<div class="accordion-body">
<p>Following our preprocessing procedure, there are <strong>30,500
content words</strong>. The word <strong>love appears 59 times</strong>
and the word <strong>hate appears only 9 times</strong>. These are 0.22%
of the total words in the text. Even though intuitively these words
should be quite common, in reality they occur only a handful of times.
So if we are interested in studying the occurrences of love/hate in the
novel, we can only rely on those occurrences. Code:</p>
<div class="codewrapper sourceCode" id="cb22">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">"84_frankenstein_clean.txt"</span>) <span class="im">as</span> f:</span>
<span id="cb22-2"><a href="#cb22-2" tabindex="-1"></a>  text <span class="op">=</span> f.read()</span>
<span id="cb22-3"><a href="#cb22-3" tabindex="-1"></a></span>
<span id="cb22-4"><a href="#cb22-4" tabindex="-1"></a>doc <span class="op">=</span> nlp(text)  <span class="co"># Process the text with SpaCy</span></span>
<span id="cb22-5"><a href="#cb22-5" tabindex="-1"></a>words <span class="op">=</span> [token.lower_ <span class="cf">for</span> token <span class="kw">in</span> doc <span class="cf">if</span> token.is_alpha <span class="kw">and</span> <span class="kw">not</span> token.is_stop]</span>
<span id="cb22-6"><a href="#cb22-6" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Total Words:"</span>, <span class="bu">len</span>(words))</span>
<span id="cb22-7"><a href="#cb22-7" tabindex="-1"></a></span>
<span id="cb22-8"><a href="#cb22-8" tabindex="-1"></a>love_words <span class="op">=</span> [word <span class="cf">for</span> word <span class="kw">in</span> words <span class="cf">if</span> <span class="st">"love"</span> <span class="op">==</span> word]</span>
<span id="cb22-9"><a href="#cb22-9" tabindex="-1"></a>hate_words <span class="op">=</span> [word <span class="cf">for</span> word <span class="kw">in</span> words <span class="cf">if</span> <span class="st">"hate"</span> <span class="op">==</span> word]</span>
<span id="cb22-10"><a href="#cb22-10" tabindex="-1"></a></span>
<span id="cb22-11"><a href="#cb22-11" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Love and Hate percentage:"</span>, (<span class="bu">len</span>(love_words) <span class="op">+</span> <span class="bu">len</span>(hate_words)) <span class="op">/</span> <span class="bu">len</span>(words) <span class="op">*</span> <span class="dv">100</span>, <span class="st">"</span><span class="sc">% o</span><span class="st">f content words"</span>)</span></code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="nlp-machine-learning-linguistics">NLP = Machine Learning + Linguistics<a class="anchor" aria-label="anchor" href="#nlp-machine-learning-linguistics"></a></h2>
<hr class="half-width"><p>So far we saw how important it is to consider the linguistic
properties of our data. We now recall that NLP is also built on top of
ideas from Machine Learning, Deep Learning. A general workflow for
solving an NLP task therefore looks quite close to the general Machine
Learning Workflow:</p>
<ol style="list-style-type: decimal"><li>Formulate the problem</li>
<li>Gather relevant data</li>
<li>Data pre-processing</li>
<li>Structure your Corpus (Inputs/Outputs)</li>
<li>Split your Data (Train/Validation/Test)</li>
<li>Choose Approach/Model/Architecture</li>
<li>If necessary Train or Fine-tune a new Model</li>
<li>Evaluate Results on Test Set</li>
<li>Refine your approach</li>
<li>Share model/results</li>
</ol><p>With this in mind we can start now exploring more NLP techniques!</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<span class="callout-header">Key Points</span>
<div class="callout-inner">
<div class="callout-content">
<ul><li>NLP is embedded in numerous daily-use products</li>
<li>Key tasks include language modeling, text classification,
information extraction, information retrieval, conversational agents,
and topic modeling, each supporting various real-world
applications.</li>
<li>NLP is a subfield of Artificial Intelligence (AI) that deals with
approaches to process, understand and generate natural language</li>
<li>Deep learning has significantly advanced NLP, but the challenge
remains in processing the discrete and ambiguous nature of language</li>
<li>The ultimate goal of NLP is to enable machines to understand and
process language as humans do, but challenges in measuring and
interpreting linguistic information still exist.</li>
</ul></div>
</div>
</div>
</section></div> <!-- / div.lesson-content -->
    </main><!-- / main#main-content.main-content --><nav class="bottom-pagination mx-md-4" aria-label="Previous and Next Chapter"><div class="d-block d-sm-block d-md-none">
        <a class="chapter-link" href="../instructor/index.html"><i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>Previous</a>
        <a class="chapter-link float-end" href="../instructor/02-preprocessing.html">Next<i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i></a>
      </div>
      <!-- content for large screens -->
      <div class="d-none d-sm-none d-md-block">
        <a class="chapter-link" href="../instructor/index.html" rel="prev">
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>
          Home
        </a>
        <a class="chapter-link float-end" href="../instructor/02-preprocessing.html" rel="next">
          Next: Episode 1: From text...
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i>
        </a>
      </div>
    </nav></div> <!-- / div.primary-content.col-xs-12 -->
<!-- END:   inst/pkgdown/templates/content-instructor.html-->

      </div><!--/div.row-->
      		<footer class="row footer mx-md-3"><hr><div class="col-md-6">
        <p>This lesson is subject to the <a href="CODE_OF_CONDUCT.html">Code of Conduct</a></p>
        <p>

        <a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/edit/main/episodes/01-introduction.md" class="external-link">Edit on GitHub</a>

	
        | <a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/blob/main/CONTRIBUTING.md" class="external-link">Contributing</a>
        | <a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/" class="external-link">Source</a></p>
				<p><a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/blob/main/CITATION.cff" class="external-link">Cite</a> | <a href="mailto:team@carpentries.org">Contact</a> | <a href="https://carpentries.org/about/" class="external-link">About</a></p>
			</div>
			<div class="col-md-6">

        <p>Materials licensed under <a href="LICENSE.html">CC-BY 4.0</a> by the authors</p>

        <p>Template licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/" class="external-link">CC-BY 4.0</a> by <a href="https://carpentries.org/" class="external-link">The Carpentries</a></p>
        <p>Built with <a href="https://github.com/carpentries/sandpaper/tree/0.17.1" class="external-link">sandpaper (0.17.1)</a>, <a href="https://github.com/carpentries/pegboard/tree/0.7.9" class="external-link">pegboard (0.7.9)</a>, and <a href="https://github.com/carpentries/varnish/tree/1.0.7" class="external-link">varnish (1.0.7)</a></p>
			</div>
		</footer></div> <!-- / div.container -->
	<div id="to-top">
		<a href="#top">
      <i class="search-icon" data-feather="arrow-up" role="img" aria-label="Back To Top"></i><br><!-- <span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top --><span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top
		</a>
	</div>
  <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "LearningResource",
  "@id": "https://esciencecenter-digital-skills.github.io/Natural-language-processing/instructor/01-introduction.html",
  "inLanguage": "en",
  "dct:conformsTo": "https://bioschemas.org/profiles/LearningResource/1.0-RELEASE",
  "description": "A Carpentries Lesson teaching foundational data and coding skills to researchers worldwide",
  "keywords": "software, data, lesson, The Carpentries, NLP, English, social sciences, pre-alpha",
  "name": "Introduction",
  "creativeWorkStatus": "active",
  "url": "https://esciencecenter-digital-skills.github.io/Natural-language-processing/instructor/01-introduction.html",
  "identifier": "https://esciencecenter-digital-skills.github.io/Natural-language-processing/instructor/01-introduction.html",
  "dateCreated": "2024-04-24",
  "dateModified": "2025-09-18",
  "datePublished": "2025-09-23"
}

  </script><script>
		feather.replace();
	</script></body></html><!-- END:   inst/pkgdown/templates/layout.html-->

