<!DOCTYPE html>
<!-- START: inst/pkgdown/templates/layout.html --><!-- Generated by pkgdown: do not edit by hand --><html lang="en" data-bs-theme="auto">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<title>Fundamentals of Natural Language Processing (NLP) in Python: All in One View</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<script src="../assets/themetoggle.js"></script><link rel="stylesheet" type="text/css" href="../assets/styles.css">
<script src="../assets/scripts.js" type="text/javascript"></script><!-- mathjax --><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      config: ["MMLorHTML.js"],
      jax: ["input/TeX","input/MathML","output/HTML-CSS","output/NativeMML", "output/PreviewHTML"],
      extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
      TeX: {
        extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
      },
      tex2jax: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [ ['$$','$$'], ['\\[', '\\]'] ],
        processEscapes: true
      }
    });
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><!-- Responsive Favicon for The Carpentries --><link rel="apple-touch-icon" sizes="180x180" href="../favicons/incubator/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicons/incubator/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="../favicons/incubator/favicon-16x16.png">
<link rel="manifest" href="../favicons/incubator/site.webmanifest">
<link rel="mask-icon" href="../favicons/incubator/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" media="(prefers-color-scheme: light)" content="white">
<meta name="theme-color" media="(prefers-color-scheme: dark)" content="black">
</head>
<body>
    <header id="top" class="navbar navbar-expand-md top-nav incubator"><svg xmlns="http://www.w3.org/2000/svg" class="d-none"><symbol id="check2" viewbox="0 0 16 16"><path d="M13.854 3.646a.5.5 0 0 1 0 .708l-7 7a.5.5 0 0 1-.708 0l-3.5-3.5a.5.5 0 1 1 .708-.708L6.5 10.293l6.646-6.647a.5.5 0 0 1 .708 0z"></path></symbol><symbol id="circle-half" viewbox="0 0 16 16"><path d="M8 15A7 7 0 1 0 8 1v14zm0 1A8 8 0 1 1 8 0a8 8 0 0 1 0 16z"></path></symbol><symbol id="moon-stars-fill" viewbox="0 0 16 16"><path d="M6 .278a.768.768 0 0 1 .08.858 7.208 7.208 0 0 0-.878 3.46c0 4.021 3.278 7.277 7.318 7.277.527 0 1.04-.055 1.533-.16a.787.787 0 0 1 .81.316.733.733 0 0 1-.031.893A8.349 8.349 0 0 1 8.344 16C3.734 16 0 12.286 0 7.71 0 4.266 2.114 1.312 5.124.06A.752.752 0 0 1 6 .278z"></path><path d="M10.794 3.148a.217.217 0 0 1 .412 0l.387 1.162c.173.518.579.924 1.097 1.097l1.162.387a.217.217 0 0 1 0 .412l-1.162.387a1.734 1.734 0 0 0-1.097 1.097l-.387 1.162a.217.217 0 0 1-.412 0l-.387-1.162A1.734 1.734 0 0 0 9.31 6.593l-1.162-.387a.217.217 0 0 1 0-.412l1.162-.387a1.734 1.734 0 0 0 1.097-1.097l.387-1.162zM13.863.099a.145.145 0 0 1 .274 0l.258.774c.115.346.386.617.732.732l.774.258a.145.145 0 0 1 0 .274l-.774.258a1.156 1.156 0 0 0-.732.732l-.258.774a.145.145 0 0 1-.274 0l-.258-.774a1.156 1.156 0 0 0-.732-.732l-.774-.258a.145.145 0 0 1 0-.274l.774-.258c.346-.115.617-.386.732-.732L13.863.1z"></path></symbol><symbol id="sun-fill" viewbox="0 0 16 16"><path d="M8 12a4 4 0 1 0 0-8 4 4 0 0 0 0 8zM8 0a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 0zm0 13a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 13zm8-5a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2a.5.5 0 0 1 .5.5zM3 8a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2A.5.5 0 0 1 3 8zm10.657-5.657a.5.5 0 0 1 0 .707l-1.414 1.415a.5.5 0 1 1-.707-.708l1.414-1.414a.5.5 0 0 1 .707 0zm-9.193 9.193a.5.5 0 0 1 0 .707L3.05 13.657a.5.5 0 0 1-.707-.707l1.414-1.414a.5.5 0 0 1 .707 0zm9.193 2.121a.5.5 0 0 1-.707 0l-1.414-1.414a.5.5 0 0 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .707zM4.464 4.465a.5.5 0 0 1-.707 0L2.343 3.05a.5.5 0 1 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .708z"></path></symbol></svg><a class="visually-hidden-focusable skip-link" href="#main-content">Skip to main content</a>
  <div class="container-fluid top-nav-container">
    <div class="col-md-8">
      <div class="large-logo">
        <img id="incubator-logo" alt="Carpentries Incubator" src="../assets/images/incubator-logo.svg"><span class="badge text-bg-danger">
          <abbr title="This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught.">
            <a href="https://docs.carpentries.org/resources/curriculum/lesson-life-cycle.html" class="external-link alert-link">
              <i aria-hidden="true" class="icon" data-feather="alert-octagon" style="border-radius: 5px"></i>
              Pre-Alpha
            </a>
            <span class="visually-hidden">This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught.</span>
          </abbr>
        </span>

      </div>
    </div>
    <div class="selector-container">
      <div id="theme-selector">
        <li class="nav-item dropdown" id="theme-button-list">
          <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
            <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="bd-theme-text">
<li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                Light
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                Dark
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                Auto
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
          </ul>
</li>
      </div>

      <div class="dropdown" id="instructor-dropdown">
        <button class="btn btn-secondary dropdown-toggle bordered-button" type="button" id="dropdownMenu1" data-bs-toggle="dropdown" aria-expanded="false">
          <i aria-hidden="true" class="icon" data-feather="eye"></i> Instructor View <i data-feather="chevron-down"></i>
        </button>
        <ul class="dropdown-menu" aria-labelledby="dropdownMenu1">
<li><button class="dropdown-item" type="button" onclick="window.location.href='../aio.html';">Learner View</button></li>
        </ul>
</div>
    </div>
  </div>
  <hr></header><nav class="navbar navbar-expand-xl bottom-nav incubator" aria-label="Main Navigation"><div class="container-fluid nav-container">
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle Navigation">
      <span class="navbar-toggler-icon"></span>
      <span class="menu-title">Menu</span>
    </button>
    <div class="nav-logo">
      <img class="small-logo" alt="Carpentries Incubator" src="../assets/images/incubator-logo-sm.svg">
</div>
    <div class="lesson-title-md">
      Fundamentals of Natural Language Processing (NLP) in Python
    </div>
    <div class="search-icon-sm">
      <!-- TODO: do not show until we have search
        <i role="img" aria-label="Search the All In One page" data-feather="search"></i>
      -->
    </div>
    <div class="desktop-nav">
      <ul class="navbar-nav me-auto mb-2 mb-lg-0">
<li class="nav-item">
          <span class="lesson-title">
            Fundamentals of Natural Language Processing (NLP) in Python
          </span>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/key-points.html">Key Points</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/instructor-notes.html">Instructor Notes</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/images.html">Extract All Images</a>
        </li>
        <li class="nav-item dropdown">
          <button class="nav-link dropdown-toggle" id="navbarDropdown" data-bs-toggle="dropdown" aria-expanded="false">
            More <i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu" aria-labelledby="navbarDropdown"><hr></ul>
</li>
      </ul>
</div>
    <!--
    <form class="d-flex col-md-2 search-form">
      <fieldset disabled>
      <input class="form-control me-2 searchbox" type="search" placeholder="" aria-label="">
        <button class="btn btn-outline-success tablet-search-button"  type="submit">
          <i class="search-icon" data-feather="search" role="img" aria-label="Search the All In One page"></i>
        </button>
      </fieldset>
    </form>
    -->
    <a id="search-button" class="btn btn-primary" href="../instructor/aio.html" role="button" aria-label="Search the All In One page">Search the All In One page</a>
  </div>
<!--/div.container-fluid -->
</nav><div class="col-md-12 mobile-title">
  Fundamentals of Natural Language Processing (NLP) in Python
</div>

<aside class="col-md-12 lesson-progress"><div style="width: %" class="percentage">
    %
  </div>
  <div class="progress incubator">
    <div class="progress-bar incubator" role="progressbar" style="width: %" aria-valuenow="" aria-label="Lesson Progress" aria-valuemin="0" aria-valuemax="100">
    </div>
  </div>
</aside><div class="container">
      <div class="row">
        <!-- START: inst/pkgdown/templates/navbar.html -->
<div id="sidebar-col" class="col-lg-4">
  <div id="sidebar" class="sidebar">
      <nav aria-labelledby="flush-headingEleven"><button role="button" aria-label="close menu" alt="close menu" aria-expanded="true" aria-controls="sidebar" class="collapse-toggle" data-collapse="Collapse " data-episodes="Episodes ">
          <i class="search-icon" data-feather="x" role="img"></i>
        </button>
        <div class="sidebar-inner">
          <div class="row mobile-row" id="theme-row-mobile">
            <div class="col" id="theme-selector">
              <li class="nav-item dropdown" id="theme-button-list">
                <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
                  <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><span class="d-lg-none ms-1" id="bd-theme-text">Toggle Theme</span>
                </button>
                <ul class="dropdown-menu dropdown-menu-right" aria-labelledby="bd-theme-text">
<li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                      Light
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                      Dark
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                      <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                      Auto
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                </ul>
</li>
            </div>
          </div>
          <div class="row mobile-row">
            <div class="col">
              <div class="sidenav-view-selector">
                <div class="accordion accordion-flush" id="accordionFlush9">
                  <div class="accordion-item">
                    <h2 class="accordion-header" id="flush-headingNine">
                      <button class="accordion-button collapsed" id="instructor" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseNine" aria-expanded="false" aria-controls="flush-collapseNine">
                        <i id="eye" aria-hidden="true" class="icon" data-feather="eye"></i> Instructor View
                      </button>
                    </h2>
                    <div id="flush-collapseNine" class="accordion-collapse collapse" aria-labelledby="flush-headingNine" data-bs-parent="#accordionFlush2">
                      <div class="accordion-body">
                        <a href="../aio.html">Learner View</a>
                      </div>
                    </div>
                  </div>
<!--/div.accordion-item-->
                </div>
<!--/div.accordion-flush-->
              </div>
<!--div.sidenav-view-selector -->
            </div>
<!--/div.col -->

            <hr>
</div>
<!--/div.mobile-row -->

          <div class="accordion accordion-flush" id="accordionFlush11">
            <div class="accordion-item">

              <button id="chapters" class="accordion-button show" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseEleven" aria-expanded="false" aria-controls="flush-collapseEleven">
                <h2 class="accordion-header chapters" id="flush-headingEleven">
                  EPISODES
                </h2>
              </button>
              <div id="flush-collapseEleven" class="accordion-collapse show collapse" aria-labelledby="flush-headingEleven" data-bs-parent="#accordionFlush11">

                <div class="accordion-body">
                  <div class="accordion accordion-flush" id="accordionFlush1">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading1">
        <a href="index.html">Summary and Schedule</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush2">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading2">
        <a href="01-introduction.html">1. Introduction</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush3">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading3">
        <a href="02-preprocessing.html">2. Episode 1: From text to vectors</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush4">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading4">
        <a href="03-transformers.html">3. Episode 2: BERT and Transformers</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush5">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading5">
        <a href="04-LargeLanguageModels.html">4. Episode 3: Using large language models</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

                </div>
              </div>
            </div>

            <hr class="half-width">
<div class="accordion accordion-flush lesson-resources" id="accordionFlush12">
              <div class="accordion-item">
                <h2 class="accordion-header" id="flush-headingTwelve">
                  <button class="accordion-button collapsed" id="lesson-resources" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseTwelve" aria-expanded="false" aria-controls="flush-collapseTwelve">
                    RESOURCES
                  </button>
                </h2>
                <div id="flush-collapseTwelve" class="accordion-collapse collapse" aria-labelledby="flush-headingTwelve" data-bs-parent="#accordionFlush12">
                  <div class="accordion-body">
                    <ul>
<li>
                        <a href="../instructor/key-points.html">Key Points</a>
                      </li>
                      <li>
                        <a href="../instructor/instructor-notes.html">Instructor Notes</a>
                      </li>
                      <li>
                        <a href="../instructor/images.html">Extract All Images</a>
                      </li>
                      <hr>
</ul>
</div>
                </div>
              </div>
            </div>
            <hr class="half-width lesson-resources">
<a href="../instructor/aio.html">See all in one page</a>


            <hr class="d-none d-sm-block d-md-none">
<div class="d-grid gap-1">

            </div>
          </div>
<!-- /div.accordion -->
        </div>
<!-- /div.sidebar-inner -->
      </nav>
</div>
<!-- /div.sidebar -->
  </div>
<!-- /div.sidebar-col -->
<!-- END:   inst/pkgdown/templates/navbar.html-->

        <!-- START: inst/pkgdown/templates/content-extra.html -->
  <div class="col-xl-8 col-lg-12 primary-content">
    <main id="main-content" class="main-content"><div class="container lesson-content">
        
        
<section id="aio-01-introduction"><p>Content from <a href="01-introduction.html">Introduction</a></p>
<hr>
<p>Last updated on 2025-09-18 |

        <a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/edit/main/episodes/01-introduction.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 120 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What is Natural Language Processing?</li>
<li>What are some common applications of NLP?</li>
<li>What makes text different from other data?</li>
<li>Why not just learn Large Language Models?</li>
<li>What linguistic properties should we consider when dealing with
texts?</li>
<li>How does NLP relates to Deep Learning methodologies?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Define Natural Language Processing</li>
<li>Show the most relevant NLP tasks and applications in practice</li>
<li>Learn how to handle Linguistic Data and how is Linguistics relevant
to NLP</li>
<li>Learn a general workflow for solving NLP tasks</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="what-is-nlp">What is NLP?<a class="anchor" aria-label="anchor" href="#what-is-nlp"></a>
</h2>
<hr class="half-width">
<p>Natural language processing (NLP) is an area of research and
application that focuses on making human languages accessible to
computers, so that they can perform useful tasks. It is therefore not a
single method, but a collection of techniques that help us deal with
linguistic inputs. The range of techniques covers from simple word
counts, to Machine Learning (ML) methods, and all the way into using
complex Deep Learning (DL) architectures.</p>
<p>The term “natural language” is used as opposed to “artificial
language”, such as programming languages, which are by design
constructed to be easily formalized into machine-readable instructions.
On the contrary, natural languages are complex, ambiguous, and heavily
context-dependent, making them challenging for computers to process. To
complicate it more, there is not only a single <em>human language</em>,
nowadays more than 7000 languages are spoken around the world, each with
its own grammar, vocabulary, and cultural context.</p>
<p>In this course we will mainly focus on written English (and a few
other languages in some specific examples as well), however this is only
a convenience so we can concentrate on the technical aspects of
processing textual data. While ideally most of the concepts from NLP
apply to most languages, one should always be ware that certain
languages require different approaches to solve seemingly similar
problems.</p>
<p>We can already find differences on the most basic step to processing
text. Take the problem of segmenting text into meaningful units, most of
the times these units are words, in NLP this is the task of
<strong>tokenization</strong>. A naive approach is to split text by
spaces, as it seems obvious that we always separate words with spaces.
Let’s see how can we segment the same sentence in English and
Chinese:</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a>english_sentence <span class="op">=</span> <span class="st">"Tokenization isn't always trivial."</span></span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a>chinese_sentence <span class="op">=</span> <span class="st">"标记化并不总是那么简单"</span> <span class="co"># Chinese Translation</span></span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a>english_words <span class="op">=</span> english_sentence.split(<span class="st">" "</span>)</span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="bu">print</span>(english_words)</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(english_words))</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a>chinese_words <span class="op">=</span> chinese_sentence.split(<span class="st">" "</span>)</span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a><span class="bu">print</span>(chinese_words)</span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(chinese_words))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>['Tokenization', "isn't", 'always', 'trivial.']
4
['标记化并不总是那么简单']
1</code></pre>
</div>
<p>Let’s look first at the English sentence. Words are mostly well
separated, however we do not get fully “clean” words (we have
punctuation and also special cases such as “isn’t”), but at least we get
a rough count of the words present in the sentence. The same example
however did not work in Chinese, because Chinese does not use spaces to
separate words. We need to use a Chinese pre-trained tokenizer, which
uses a dictionary-based approach to properly split the words:</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="im">import</span> jieba  <span class="co"># A popular Chinese text segmentation library</span></span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>chinese_sentence <span class="op">=</span> <span class="st">"标记化并不总是那么简单"</span></span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>chinese_words <span class="op">=</span> jieba.lcut(chinese_sentence)</span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a><span class="bu">print</span>(chinese_words)</span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(chinese_words))  <span class="co"># Output: 7</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>['标记', '化', '并', '不', '总是', '那么', '简单']
7</code></pre>
</div>
<p>We can trust that the output valid, because we are using a verified
library, even though we don’t speak Chinese. Another interesting aspect
is that the Chinese sentence has more words than the English one, even
though they convey the same meaning. This shows the complexity of
dealing with more than one language at a time, like in Machine
Translation.</p>
<div id="pre-trained-models-and-fine-tunning" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="pre-trained-models-and-fine-tunning" class="callout-inner">
<h3 class="callout-title">Pre-trained Models and Fine-tunning</h3>
<div class="callout-content">
<p>These two terms will appear very frequently when talking about NLP.
The term <em>pre-trained</em> is taken from Machine Learning and refers
to a model that has been already optimized using relevant data to
perform a task. It is possible to directly load and use the model
out-of-the-box to apply it to our own dataset. Ideally, released
pre-trained models have already been tested for generalization and
quality of outputs, but it is always important to double check the
evaluation process they were subjected to before using them.</p>
<p>Sometimes a pre-trained model is of good quality, but it does not fit
the nuances of our specific dataset. For example, the model was trained
on newspaper articles but you are interested in poetry. In this case, it
is common to perform <em>fine-tunning</em>, this means that instead of
training your own model from scratch, you start with the knowledge
obtained in the pre-trained model and adjust it (fine-tune it) with your
specific data. If this is done well it leads to increased performance in
the specific task you are trying to solve. The advantage of fine-tunning
is that you do not need a large amount of data to improve the results,
hence the popularity of the technique.</p>
</div>
</div>
</div>
<p>In more general terms, NLP deals with the challenges of correctly
processing and generating text, this can be as simple as counting word
frequencies to detect different writing styles, using statistical
methods to classify texts into different categories, or using deep
neural networks to generate human-like text by exploiting word
co-occurrences in large amounts of texts.</p>
</section><section><h2 class="section-heading" id="language-as-data">Language as Data<a class="anchor" aria-label="anchor" href="#language-as-data"></a>
</h2>
<hr class="half-width">
<p>From a more technical perspective, NLP focuses on applying Machine
Learning techniques to linguistic data. This makes all the difference,
since ML methods expect a structured dataset, with a well defined set of
features that engineers can work with. Your first task as an NLP
practitioner is to <strong>understand what aspects of textual data are
relevant for your application</strong> and apply techniques to
systematically extract meaningful features (if using ML) or appropriate
neural architectures (if using DL) from unstructured data that can help
solve our problem at hand.</p>
<div id="nlp-in-the-real-world" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="nlp-in-the-real-world" class="callout-inner">
<h3 class="callout-title">NLP in the real world</h3>
<div class="callout-content">
<p>Name three to five tools/products that you use on a daily basis and
that you think leverage NLP techniques. To solve this exercise you can
get some help from the web.</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<p>These are some of the most popular NLP-based products that we use on
a daily basis:</p>
<ul>
<li>Agentic Chatbots (ChatGPT, Perplexity)</li>
<li>Voice-based assistants (e.g., Alexa, Siri, Cortana)</li>
<li>Machine translation (e.g., Google translate, Amazon translate)</li>
<li>Search engines (e.g., Google, Bing, DuckDuckGo)</li>
<li>Keyboard autocompletion on smartphones</li>
<li>Spam filtering</li>
<li>Spell and grammar checking apps</li>
<li>Customer care chatbots</li>
<li>Text summarization tools (e.g., news aggregators)</li>
<li>Sentiment analysis tools (e.g., social media monitoring)</li>
</ul>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="nlp-tasks">NLP tasks<a class="anchor" aria-label="anchor" href="#nlp-tasks"></a>
</h3>
<p>The exercise above tells us that a great deal of NLP techniques is
embedded in our daily life. Indeed NLP is an important component in a
wide range of software applications that we use in our daily lives.</p>
<p>There are several ways to describe the tasks that NLP solves. From
the Machine Learning perspective, we have:</p>
<ul>
<li><p>Supervised tasks: learning to classify texts given a labeled set
of examples</p></li>
<li><p>Unsupervised tasks: exploiting existing patterns from large
amounts of text.</p></li>
</ul>
<p>From the Deep Learning perspective we can consider different neural
network architectures to tackle an NLP task, such as:</p>
<ul>
<li><p>Multi-layer Perceptron</p></li>
<li><p>Recurrent Neural Network</p></li>
<li><p>Convolutional Neural Network</p></li>
<li><p>LSTM’s</p></li>
<li><p>Transformer</p></li>
</ul>
<p>Below we show one possible taxonomy of NLP tasks, where we instead
focus on the problem formulation aspect. The tasks are grouped together
with some of their most prominent applications. This is definitely a
non-exhaustive list, as in reality there are hundreds of them, but it is
a good start:</p>
<ul>
<li><p><strong>Language Modeling</strong>: Given a sequence of words,
the model predicts the next word. For example, in the sentence “The
capital of France is _____”, the model should predict “Paris” based on
the context. This task was initially useful for building solutions that
require speech and optical character recognition (even handwriting),
language translation and spelling correction. Nowadays this has scaled
up to the LLMs that we know.</p></li>
<li>
<p><strong>Text Classification</strong>: Assign one or more labels
to a “Documents”. A document in our context can mean a sentence, a
paragraph, a book chapter, etc…</p>
<ul>
<li>
<strong>Language Identification</strong>: determining the language
of a given text.</li>
<li>
<strong>Spam Filtering</strong>: classifiying emails into spam or
not spam based on their content.</li>
<li>
<strong>Authorship Attribution</strong>: determining the author of a
text based on its style and content (based on the assumption that each
author has a unique writing style).</li>
<li>
<strong>Sentiment Analysis</strong>: classifying text into positive,
negative or neutral sentiment. For example, in the sentence “I love this
product!”, the model would classify it as positive sentiment.</li>
</ul>
</li>
<li>
<p><strong>Token Classification</strong>: The task of assigning
label to words individually. Because words do not occur in isolation,
their meaning depend on the sequence of words to the left or the right
of them, this is also called Word-In-Context Classification or Sequence
Labeling and usually involves syntactic and semantic analysis.</p>
<ul>
<li>
<strong>Part-Of-Speech Tagging</strong>: is the task of assigning a
part-of-speech label (e.g., noun, verb, adjective) to each word in a
sentence.</li>
<li>
<strong>Chunking</strong>: splitting a running text into “chunks” of
words that together represent a meaningful unit: phrases, sentences,
paragraphs, etc.</li>
<li>
<strong>Word Sense Disambiguation</strong>: based on the context
what does a word mean (think of “book” in “I read a book.” vs “I want to
book a flight.”)</li>
<li>
<strong>Named Entity Recognition</strong>: recognize world entities
in text, e.g. Persons, Locations, Book Titles, or many others. For
example “Mary Shelley” is a person, “Frankenstein or the Modern
Prometeus” is a book, etc.</li>
<li>
<strong>Semantic Role Labeling</strong>: the task if finding out
“Who did what to whom?” in a sentence: information from events such as
agents, participants, circumstances, etc.</li>
<li>
<strong>Relation Extraction</strong>: the task of identifying named
relationships between entities in a text, e.g. “Apple is based in
California” has the relation (Apple, based_in, California).</li>
<li>
<strong>Co-reference Resolution</strong>: the task of determining
which words refer to the same entity in a text, e.g. “Mary is a doctor.
She works at the hospital.” Here “She” refers to “Mary”.</li>
<li>
<strong>Entity Linking</strong>: the task of disambiguation of named
entities in a text, linking them to their corresponding entries in a
knowledge base, e.g. Mary Shelley’s biogrpaphy in Wikipedia.</li>
</ul>
</li>
<li>
<p><strong>Text Similarity</strong>: The task of determining how
similar two pieces of text are.</p>
<ul>
<li>
<strong>Plagiarism detection</strong>: determining whether a piece
of TextB is close enough to another known piece of TextA, which
increments the likelihood that it was copied from it.</li>
<li>
<strong>Document clustering</strong>: grouping similar texts
together based on their content.</li>
<li>
<strong>Topic modelling</strong>: A specific instance of clustering,
here we automatically identify abstract “topics” that occur in a set of
documents, where each topic is represented as a cluster of words that
frequently appear together.</li>
<li>
<strong>Information Retrieval</strong>: This is the task of finding
relevant information or documents from a large collection of
unstructured data based on user’s query, e.g., “What’s the best
restaurant near me?”.</li>
</ul>
</li>
<li>
<p><strong>Text Generation</strong>: The task of generating text
based on a given input. This can</p>
<ul>
<li>
<strong>Machine Translation</strong>: translating text from one
language to another, e.g., “Hello” in English to “Que tal” in
Spanish.</li>
<li>
<strong>Summarization</strong>: generating a concise summary of a
longer text. It can be abstractive (generating new sentences that
capture the main ideas of the original text) but also extractive
(selecting important sentences from the original text).</li>
<li>
<strong>Paraphrasing</strong>: generating a new sentence that
conveys the same meaning as the original sentence, e.g., “The cat is on
the mat.” to “The mat has a cat on it.”.</li>
<li>
<strong>Question Answering</strong>: Given a question and a context,
the model generates an answer. For example, given the question “What is
the capital of France?” and the Wikipedia article about France as the
context, the model should answer “Paris”. This task can be approached as
a text classification problem (where the answer is one of the predefined
options) or as a generative task (where the model generates the answer
from scratch).</li>
<li>
<strong>Conversational Agent (ChatBot)</strong>: Building a system
that interacts with a user via natural language, e.g., “What’s the
weather today, Siri?”. These agents are widely used to improve user
experience in customer service, personal assistance and many other
domains.</li>
</ul>
</li>
</ul>
</div>
<div class="section level3">
<h3 id="what-is-a-word">What is a word?<a class="anchor" aria-label="anchor" href="#what-is-a-word"></a>
</h3>
<p>When dealing with language we deal with sequences of words and with
how they relate to each other to generate meaning. Our first step to
provide structure to text is therefore to split it into words.</p>
<div id="token-vs-word" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="token-vs-word" class="callout-inner">
<h3 class="callout-title">Token vs Word</h3>
<div class="callout-content">
<p>For simplicity, in the rest of the course we will use the terms
“word” and “token” interchangeably, but as we just saw they do not
always have the same granularity. Originally the concept of token
comprised dictionary words, numeric symbols and punctuation. Nowadays,
tokenization has also evolved and became an optimization task on its own
(How can we segment text in a way that neural networks learn optimally
from text?). Tokenizers always allow to “reconstruct back” tokens to
human-readable words even if internally they split the text differently,
hence we can afford to use them as synonyms. If you are curious, you can
visualize how different state-of-the-art tokenizers work <a href="https://tiktokenizer.vercel.app/" class="external-link">here</a></p>
</div>
</div>
</div>
<p>Finally, we will start to working with text data! Let’s open a file,
read it into a string and split it by spaces. We will print the original
text and the list of “words” to see how they look:</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">"text1_clean.txt"</span>) <span class="im">as</span> f:</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>  text <span class="op">=</span> f.read()</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a><span class="bu">print</span>(text[:<span class="dv">100</span>])</span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Length:"</span>, <span class="bu">len</span>(text))</span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a>proto_tokens <span class="op">=</span> text.split()</span>
<span id="cb5-8"><a href="#cb5-8" tabindex="-1"></a><span class="bu">print</span>(proto_tokens[:<span class="dv">40</span>])</span>
<span id="cb5-9"><a href="#cb5-9" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(proto_tokens))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Letter 1 St. Petersburgh, Dec. 11th, 17-- TO Mrs. Saville, England You will rejoice to hear that no disaster has accompanied the commencement of an en
Length: 417931

Proto-Tokens:
['Letter', '1', 'St.', 'Petersburgh,', 'Dec.', '11th,', '17--', 'TO', 'Mrs.', 'Saville,', 'England', 'You', 'will', 'rejoice', 'to', 'hear', 'that', 'no', 'disaster', 'has', 'accompanied', 'the', 'commencement', 'of', 'an', 'enterprise', 'which', 'you', 'have', 'regarded', 'with', 'such', 'evil', 'forebodings.', 'I', 'arrived', 'here', 'yesterday,', 'and', 'my']
74942</code></pre>
</div>
<p>Splitting by white space is possible but needs several extra steps to
do get the clean words and separate the punctuation appropriately.
Instead, we will introduce the [spaCy]((<a href="https://github.com/explosion/spaCy" class="external-link uri">https://github.com/explosion/spaCy</a>) library to segment
the text into human-readable tokens. First we will download the
pre-trained model, in this case we only need the small English
version:</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="op">!</span> python <span class="op">-</span>m spacy download en_core_web_sm</span></code></pre>
</div>
<p>This is a model that spaCy already trained for us on a subset of web
English data. Hence, the model already “knows” how to tokenize into
English words. When the model processes a string, it does not only do
the splitting for us but already provides more advanced linguistic
properties of the tokens (such as part-of-speech tags, or named
entities). Let’s now import the model and use it to parse our
document:</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="im">import</span> spacy</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a>nlp <span class="op">=</span> spacy.load(<span class="st">"en_core_web_sm"</span>) <span class="co"># we load the small English model for efficiency</span></span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a>doc <span class="op">=</span> nlp(text) <span class="co"># Doc is a python object with several methods to retrieve linguistic properties</span></span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" tabindex="-1"></a><span class="co"># SpaCy-Tokens</span></span>
<span id="cb8-8"><a href="#cb8-8" tabindex="-1"></a>tokens <span class="op">=</span> [token.text <span class="cf">for</span> token <span class="kw">in</span> doc] <span class="co"># Note that spacy tokens are also python objects </span></span>
<span id="cb8-9"><a href="#cb8-9" tabindex="-1"></a><span class="bu">print</span>(tokens[:<span class="dv">40</span>])</span>
<span id="cb8-10"><a href="#cb8-10" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(tokens))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>['Letter', '1', 'St.', 'Petersburgh', ',', 'Dec.', '11th', ',', '17', '-', '-', 'TO', 'Mrs.', 'Saville', ',', 'England', 'You', 'will', 'rejoice', 'to', 'hear', 'that', 'no', 'disaster', 'has', 'accompanied', 'the', 'commencement', 'of', 'an', 'enterprise', 'which', 'you', 'have', 'regarded', 'with', 'such', 'evil', 'forebodings', '.']
85713</code></pre>
</div>
<p>The differences look subtle at the beginning, but if we carefully
inspect the way spaCy splits the text, we can see the advantage of using
a proper tokenizer. There are also a several of properties that spaCy
provides us with, for example we can get only symbols or only
alphanumerical tokens, and more advanced linguistic properties, for
example we can remove punctuation and only keep alphanumerical
tokens:</p>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a>only_words <span class="op">=</span> [token <span class="cf">for</span> token <span class="kw">in</span> doc <span class="cf">if</span> token.is_alpha]  <span class="co"># Only alphanumerical tokens</span></span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a><span class="bu">print</span>(only_words[:<span class="dv">50</span>])</span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(only_words))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[Letter, Petersburgh, TO, Saville, England, You, will, rejoice, to, hear]
1199</code></pre>
</div>
<p>or keep only the verbs from our text:</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a>only_verbs <span class="op">=</span> [token <span class="cf">for</span> token <span class="kw">in</span> doc <span class="cf">if</span> token.pos_ <span class="op">==</span> <span class="st">"VERB"</span>]  <span class="co"># Only verbs</span></span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a><span class="bu">print</span>(only_verbs[:<span class="dv">10</span>])</span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(only_verbs))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[rejoice, hear, accompanied, regarded, arrived, assure, increasing, walk, feel, braces]
150</code></pre>
</div>
<p>SpaCy also predicts the sentences under the hood for us. We can
access them like this:</p>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a>sentences <span class="op">=</span> [sent.text <span class="cf">for</span> sent <span class="kw">in</span> doc.sents] <span class="co"># Sentences are also python objects</span></span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a><span class="bu">print</span>(sentences[:<span class="dv">5</span>])</span>
<span id="cb14-3"><a href="#cb14-3" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(sentences))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>48
Letter 1 St. Petersburgh, Dec. 11th, 17-- TO Mrs. Saville, England You will rejoice to hear that no disaster has accompanied the commencement of an enterprise which you have regarded with such evil forebodings.
I arrived here yesterday, and my first task is to assure my dear sister of my welfare and increasing confidence in the success of my undertaking.
I am already far north of London, and as I walk in the streets of Petersburgh, I feel a cold northern breeze play upon my cheeks, which braces my nerves and fills me with delight.
Do you understand this feeling?
This breeze, which has travelled from the regions towards which I am advancing, gives me a foretaste of those icy climes.</code></pre>
</div>
<p>We can also see what named entities the model predicted:</p>
<div class="codewrapper sourceCode" id="cb16">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(doc.ents))</span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a><span class="cf">for</span> ent <span class="kw">in</span> doc.ents[:<span class="dv">5</span>]:</span>
<span id="cb16-3"><a href="#cb16-3" tabindex="-1"></a>    <span class="bu">print</span>(ent.label_, ent.text)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>1713
DATE Dec. 11th
CARDINAL 17
PERSON Saville
GPE England
DATE yesterday</code></pre>
</div>
<p>This are just basic tests to show you how you can right away
structure text using existing NLP libraries. Of course we used a
simplified model so the more complex the task the more errors will
appear. The biggest advantage of using these existing libraries is that
they help you transform unstructured plain text files into structured
data that you can manipulate later for your own goals.</p>
<div id="nlp-libraries" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="nlp-libraries" class="callout-inner">
<h3 class="callout-title">NLP Libraries</h3>
<div class="callout-content">
<p>Related to the need of shaping our problems into a known task, there
are several existing NLP libraries which provide a wide range of models
that we can use out-of-the-box. We already saw simple examples using
SpaCy for English and jieba for Chinese. Again, as a non-exhaustive
list, we mention here some of the most used NLP libraries in python:</p>
<ul>
<li><a href="https://github.com/nltk/nltk" class="external-link">NLTK</a></li>
<li><a href="https://github.com/explosion/spaCy" class="external-link">spaCy</a></li>
<li><a href="https://github.com/RaRe-Technologies/gensim" class="external-link">Gensim</a></li>
<li><a href="https://github.com/stanfordnlp/stanza" class="external-link">Stanza</a></li>
<li><a href="https://github.com/flairNLP/flair" class="external-link">Flair</a></li>
<li><a href="https://github.com/facebookresearch/fastText" class="external-link">FastText</a></li>
<li><a href="https://github.com/huggingface/transformers" class="external-link">HuggingFace
Transformers</a></li>
</ul>
<div class="section level3">
<h3 id="linguistic-resources">Linguistic Resources<a class="anchor" aria-label="anchor" href="#linguistic-resources"></a>
</h3>
<p>There are also several curated resources that can help solve your
NLP-related tasks, specifically when you need highly specialized
definitions. An exhaustive list would be impossible as there are
thousands of them, and also them being language and domain dependent.
Below we mention some of the most prominent, just to give you an idea of
the kind of resources you can find, so you don’t need to reinvent the
wheel every time you start a project:</p>
<ul>
<li>
<a href="https://huggingface.co/datasets" class="external-link">HuggingFace Datasets</a>:
A large collection of datasets for NLP tasks, including text
classification, question answering, and language modeling.</li>
<li>
<a href="https://wordnet.princeton.edu/" class="external-link">WordNet</a>: A large
lexical database of English, where words are grouped into sets of
synonyms (synsets) and linked by semantic relations.</li>
<li>
<a href="https://www.europarl.europa.eu/ep-search/search.do?language=en" class="external-link">Europarl</a>:
A parallel corpus of the proceedings of the European Parliament,
available in 21 languages, which can be used for machine translation and
cross-lingual NLP tasks.</li>
<li>
<a href="https://universaldependencies.org/" class="external-link">Universal
Dependencies</a>: A collection of syntactically annotated treebanks
across 100+ languages, providing a consistent annotation scheme for
syntactic and morphological properties of words, which can be used for
cross-lingual NLP tasks.</li>
<li>
<a href="https://propbank.github.io/" class="external-link">PropBank</a>: A corpus of
texts annotated with information about basic semantic propositions,
which can be used for English semantic tasks.</li>
<li>
<a href="https://framenet.icsi.berkeley.edu/fndrupal/" class="external-link">FrameNet</a>:
A lexical resource that provides information about the semantic frames
that underlie the meanings of words (mainly verbs and nouns), including
their roles and relations.</li>
<li>
<a href="https://babelnet.org/" class="external-link">BabelNet</a>: A multilingual lexical
resource that combines WordNet and Wikipedia, providing a large number
of concepts and their relations in multiple languages.</li>
<li>
<a href="https://www.wikidata.org/" class="external-link">Wikidata</a>: A free and open
knowledge base initially derived from Wikipedia, that contains
structured data about entities, their properties and relations, which
can be used to enrich NLP applications.</li>
<li>
<a href="https://github.com/allenai/dolma" class="external-link">Dolma</a>: An open
dataset of 3 trillion tokens from a diverse mix of clean web content,
academic publications, code, books, and encyclopedic materials, used to
train English large language models.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="why-should-we-learn-nlp-fundamentals">Why should we learn NLP Fundamentals?<a class="anchor" aria-label="anchor" href="#why-should-we-learn-nlp-fundamentals"></a>
</h2>
<hr class="half-width">
<p>In the past decade, NLP has evolved significantly, especially in the
field of deep learning, to the point that it has become embedded in our
daily lives, one just needs to look at the term Large Language Models
(LLMs), the latest generation of NLP models, which is now ubiquitous in
news media and tech products we use on a daily basis.</p>
<p>The term LLM now is often (and wrongly) used as a synonym of
Artificial Intelligence. We could therefore think that today we just
need to learn how to manipulate LLMs in order to fulfill our research
goals involving textual data. The truth is that Language Modeling has
always been part of the core tasks of NLP, therefore, by learning NLP
you will understand better where are the main ideas behind LLMs coming
from.</p>
<figure><img src="../fig/intro0_cs_nlp.png" alt="NLP is an interdisciplinary field, and LLMs are just a subset of it" class="figure mx-auto d-block"><div class="figcaption">NLP is an interdisciplinary field, and LLMs are
just a subset of it</div>
</figure><p>LLM is a blanket term for an assembly of large neural networks that
are trained on vast amounts of text data with the objective of
optimizing for language modeling. Once they are trained, they are used
to generate human-like text or fine-tunned to perform much more advanced
tasks. Indeed, the surprising and fascinating properties that emerge
from training models at this scale allows us to solve different complex
tasks such as answer elaborate questions, translate languages, solve
complex problems, generate narratives that emulate reasoning, and many
more, all of this with a single tool.</p>
<p>It is important, however, to pay attention to what is happening
behind the scenes in order to be able <strong>trace sources of errors
and biases</strong> that get hidden in the complexity of these models.
The purpose of this course is precisely to take a step back, and
understand that: - There is a wide variety of tools available, beyond
LLMs, that do not require so much computing power. - Sometimes a much
simpler and easier method is already available that can solve our
problem at hand. - If we learn how previous approaches to solve
linguistic problems were designed, we can better understand the
limitations of LLMs and how to use them effectively. - LLMs excel at
confidently delivering information, without any regards for correctness.
This calls for a careful design of <strong>evaluation metrics</strong>
that give us a better understanding of the quality of the generated
content.</p>
<p>Let’s go back to our problem of segmenting text and see what ChatGPT
has to say about tokenizing Chinese text:</p>
<figure><img src="../fig/intro1.png" alt="ChatGPT Just Works! Does it…?" class="figure mx-auto d-block"><div class="figcaption">ChatGPT Just Works! Does it…?</div>
</figure><p>We got what sounds like a straightforward confident answer. However,
it is not clear how the model arrived at this solution. Second, we do
not know whether the solution is correct or not. In this case ChatGPT
made some assumptions for us, such as choosing a specific kind of
tokenizer to give the answer, and since we do not speak the language, we
do not know if this is indeed the best approach to tokenize Chinese
text. If we understand the concept of Token (which we will today!), then
we can be more informed about the quality of the answer, whether it is
useful to us, and therefore make a better use of the model.</p>
<p>And by the way, ChatGPT was <strong>almost</strong> correct, in the
specific case of the gpt-4 tokenizer, the model will return 12 tokens
(not 11!) for the given Chinese sentence.</p>
<figure><img src="../fig/intro1b.png" alt="GPT-4 Tokenization Example" class="figure mx-auto d-block"><div class="figcaption">GPT-4 Tokenization Example</div>
</figure><p>We can also argue if the statement “Chinese is generally tokenized
character by character” is an overstatement or not. In any case, the
real question here is: Are we ok with <em>almost correct answers</em>?
Please note that this is not a call to avoid using LLM’s but a call for
a careful consideration of usage and more importantly, an attempt to
explain the mechanisms behind via NLP concepts.</p>
</section><section><h2 class="section-heading" id="relevant-linguistic-aspects">Relevant Linguistic Aspects<a class="anchor" aria-label="anchor" href="#relevant-linguistic-aspects"></a>
</h2>
<hr class="half-width">
<p>Natural language exhibits a set fo properties that make it more
challenging to process than other types of data such as tables,
spreadsheets or time series. <strong>Language is hard to process because
it is compositional, ambiguous, discrete and sparse</strong>.</p>
<div class="section level3">
<h3 id="compositionality">Compositionality<a class="anchor" aria-label="anchor" href="#compositionality"></a>
</h3>
<p>The basic elements of written languages are characters, a sequence of
characters form words, and words in turn denote objects, concepts,
events, actions and ideas (Goldberg, 2016). Subsequently words form
phrases and sentences which are used in communication and depend on the
context in which they are used. We as humans derive the meaning of
utterances from interpreting contextual information that is present at
different levels at the same time:</p>
<figure><img src="../fig/intro2_levels_lang.svg" alt="Levels of Language" class="figure mx-auto d-block"><div class="figcaption">Levels of Language</div>
</figure><p>The first two levels refer to spoken language only, and the other
four levels are present in both speech and text. Because in principle
machines do not have access to the same levels of information that we do
(they can only have independent audio, textual or visual inputs), we
need to come up with clever methods to overcome this significant
limitation. Knowing the levels of language is important so we consider
what kind of problems we are facing when attempting to solve our NLP
task at hand.</p>
</div>
<div class="section level3">
<h3 id="ambiguity">Ambiguity<a class="anchor" aria-label="anchor" href="#ambiguity"></a>
</h3>
<p>The disambiguation of meaning is usually a by-product of the context
in which utterances are expressed and also the historic accumulation of
interactions which are transmitted across generations (think for
instance to idioms – these are usually meaningless phrases that acquire
meaning only if situated within their historical and societal context).
These characteristics make NLP a particularly challenging field to work
in.</p>
<p>We cannot expect a machine to process human language and simply
understand it as it is. We need a systematic, scientific approach to
deal with it. It’s within this premise that the field of NLP is born,
primarily interested in converting the building blocks of human/natural
language into something that a machine can understand.</p>
<p>The image below shows how the levels of language relate to a few NLP
applications:</p>
<figure><img src="../fig/intro3_levels_nlp.png" alt="Diagram showing building blocks of language" class="figure mx-auto d-block"><div class="figcaption">Diagram showing building blocks of
language</div>
</figure><div id="levels-of-ambiguity" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="levels-of-ambiguity" class="callout-inner">
<h3 class="callout-title">Levels of ambiguity</h3>
<div class="callout-content">
<p>Discuss what do the following sentences mean. What level of ambiguity
do they represent?:</p>
<ul>
<li>“The door is unlockable from the inside.” vs “Unfortunately, the
cabinet is unlockable, so we can’t secure it”</li>
<li>“I saw the <em>cat with the stripes</em>” vs “I saw the cat <em>with
the telescope</em>”</li>
<li>“Colorless green ideas sleep furiously”</li>
<li>“I never said she stole my money.” vs “I never said she stole my
money.”</li>
</ul>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2"> Show me the solution </h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" data-bs-parent="#accordionSolution2" aria-labelledby="headingSolution2">
<div class="accordion-body">
<p>This is why the previous statements were difficult:</p>
<ul>
<li>“Un-lockable vs Unlock-able” is a <strong>Morphological</strong>
ambiguity: Same word form, two possible meanings</li>
<li>“I saw the cat with the telescope” is a <strong>Syntactic</strong>
ambiguity: Same sentence structure, different properties</li>
<li>“Colorless green ideas sleep furiously” <strong>Semantic</strong>
ambiguity: Grammatical but meaningless (ideas do not have color as a
property. Even if this was true, they would be either colorless or
green)</li>
<li>“I NEVER said she stole MY money.” is a <strong>Pragmatic</strong>
ambiguity: Meaning relies on word emphasis</li>
</ul>
</div>
</div>
</div>
</div>
<p>Whenever you are solving a specific task, you should ask yourself
what kind of ambiguity can affect your results? At what level are your
assumptions operating when defining your research questions? Having the
answers to this can save you a lot of time when debugging your models.
Sometimes the most innocent assumptions (for example using the wrong
tokenizer) can create enormous performance drops even when the higher
level assumptions were correct.</p>
</div>
<div class="section level3">
<h3 id="discreteness">Discreteness<a class="anchor" aria-label="anchor" href="#discreteness"></a>
</h3>
<p>There is no inherent relationship between the form of a word and its
meaning. For the same reason, by textual means alone, there is no way of
knowing if two words are similar or how do they relate to each other.
Take the word “pizza” and “hamburger”, how can we automatically know
that they share more properties than “car” and “cat”? One way is by
looking at the context in which these words are used, and how they are
related to each other. This idea is the principle behind
<strong>distributional semantics</strong>, and aims to look at the
statistical properties of language, such as word co-occurrences, to
understand how words relate to each other.</p>
<p>Let’s do a simple exercise:</p>
<div class="codewrapper sourceCode" id="cb18">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" tabindex="-1"></a></span>
<span id="cb18-2"><a href="#cb18-2" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> Counter</span>
<span id="cb18-3"><a href="#cb18-3" tabindex="-1"></a></span>
<span id="cb18-4"><a href="#cb18-4" tabindex="-1"></a><span class="co"># A mini-corpus where our target words appear</span></span>
<span id="cb18-5"><a href="#cb18-5" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"""</span></span>
<span id="cb18-6"><a href="#cb18-6" tabindex="-1"></a><span class="st">I am hungry . Should I eat delicious pizza ?</span></span>
<span id="cb18-7"><a href="#cb18-7" tabindex="-1"></a><span class="st">Or maybe I should eat a juicy hamburger instead .</span></span>
<span id="cb18-8"><a href="#cb18-8" tabindex="-1"></a><span class="st">Many people like to eat pizza because is tasty , they think pizza is delicious as hell !</span></span>
<span id="cb18-9"><a href="#cb18-9" tabindex="-1"></a><span class="st">My friend prefers to eat a hamburger and I agree with him .</span></span>
<span id="cb18-10"><a href="#cb18-10" tabindex="-1"></a><span class="st">We will drive our car to the restaurant to get the succulent hamburger .</span></span>
<span id="cb18-11"><a href="#cb18-11" tabindex="-1"></a><span class="st">Right now , our cat sleeps on the mat so we won't take him .</span></span>
<span id="cb18-12"><a href="#cb18-12" tabindex="-1"></a><span class="st">I did not wash my car , but at least the car has gasoline .</span></span>
<span id="cb18-13"><a href="#cb18-13" tabindex="-1"></a><span class="st">Perhaps when we come back we will take out the cat for a walk .</span></span>
<span id="cb18-14"><a href="#cb18-14" tabindex="-1"></a><span class="st">The cat will be happy to see us when we come back .</span></span>
<span id="cb18-15"><a href="#cb18-15" tabindex="-1"></a><span class="st">"""</span></span>
<span id="cb18-16"><a href="#cb18-16" tabindex="-1"></a></span>
<span id="cb18-17"><a href="#cb18-17" tabindex="-1"></a>words <span class="op">=</span> [token.lower() <span class="cf">for</span> token <span class="kw">in</span> text.split()]</span>
<span id="cb18-18"><a href="#cb18-18" tabindex="-1"></a></span>
<span id="cb18-19"><a href="#cb18-19" tabindex="-1"></a>target_words <span class="op">=</span> [<span class="st">"pizza"</span>, <span class="st">"hamburger"</span>, <span class="st">"car"</span>, <span class="st">"cat"</span>] <span class="co"># words we want to analyze</span></span>
<span id="cb18-20"><a href="#cb18-20" tabindex="-1"></a>stop_words <span class="op">=</span> [<span class="st">"i"</span>, <span class="st">"am"</span>, <span class="st">"my"</span>, <span class="st">"to"</span>, <span class="st">"the"</span>, <span class="st">"a"</span>, <span class="st">"and"</span>, <span class="st">"is"</span>, <span class="st">"as"</span>, <span class="st">"at"</span>, <span class="st">"we"</span>, <span class="st">"will"</span>, <span class="st">"not"</span>, <span class="st">"our"</span>, <span class="st">"but"</span>, <span class="st">"least"</span>, <span class="st">"has"</span>, <span class="st">"."</span>, <span class="st">","</span>] <span class="co"># words to ignore</span></span>
<span id="cb18-21"><a href="#cb18-21" tabindex="-1"></a>co_occurrence <span class="op">=</span> {word: [] <span class="cf">for</span> word <span class="kw">in</span> target_words}</span>
<span id="cb18-22"><a href="#cb18-22" tabindex="-1"></a>window_size <span class="op">=</span> <span class="dv">3</span> <span class="co"># How many words to look at on each side</span></span>
<span id="cb18-23"><a href="#cb18-23" tabindex="-1"></a></span>
<span id="cb18-24"><a href="#cb18-24" tabindex="-1"></a><span class="co"># Find the context for each target word</span></span>
<span id="cb18-25"><a href="#cb18-25" tabindex="-1"></a><span class="cf">for</span> i, word <span class="kw">in</span> <span class="bu">enumerate</span>(words):</span>
<span id="cb18-26"><a href="#cb18-26" tabindex="-1"></a>    <span class="cf">if</span> word <span class="kw">in</span> target_words:</span>
<span id="cb18-27"><a href="#cb18-27" tabindex="-1"></a>        start <span class="op">=</span> <span class="bu">max</span>(<span class="dv">0</span>, i <span class="op">-</span> window_size)</span>
<span id="cb18-28"><a href="#cb18-28" tabindex="-1"></a>        end <span class="op">=</span> <span class="bu">min</span>(<span class="bu">len</span>(words), i <span class="op">+</span> <span class="dv">1</span> <span class="op">+</span> window_size)</span>
<span id="cb18-29"><a href="#cb18-29" tabindex="-1"></a>        context <span class="op">=</span> words[start:i] <span class="op">+</span> words[i<span class="op">+</span><span class="dv">1</span>:end] <span class="co"># Exclude the target word itself</span></span>
<span id="cb18-30"><a href="#cb18-30" tabindex="-1"></a>        context <span class="op">=</span> [w <span class="cf">for</span> w <span class="kw">in</span> context <span class="cf">if</span> w <span class="kw">not</span> <span class="kw">in</span> stop_words] <span class="co"># Filter out stop words from context</span></span>
<span id="cb18-31"><a href="#cb18-31" tabindex="-1"></a>        co_occurrence[word].extend(context)</span>
<span id="cb18-32"><a href="#cb18-32" tabindex="-1"></a></span>
<span id="cb18-33"><a href="#cb18-33" tabindex="-1"></a><span class="co"># Print the most common context words for each target word</span></span>
<span id="cb18-34"><a href="#cb18-34" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Contextual Fingerprints:</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb18-35"><a href="#cb18-35" tabindex="-1"></a><span class="cf">for</span> word, context_list <span class="kw">in</span> co_occurrence.items():</span>
<span id="cb18-36"><a href="#cb18-36" tabindex="-1"></a>    <span class="co"># We use Counter to get a frequency count of context words</span></span>
<span id="cb18-37"><a href="#cb18-37" tabindex="-1"></a>    fingerprint <span class="op">=</span> Counter(context_list).most_common(<span class="dv">5</span>)</span>
<span id="cb18-38"><a href="#cb18-38" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"'</span><span class="sc">{</span>word<span class="sc">}</span><span class="ss">': </span><span class="sc">{</span>fingerprint<span class="sc">}</span><span class="ss">"</span>)</span></code></pre>
</div>
<pre class="ouptut"><code>Contextual Fingerprints:

'pizza': [('eat', 2), ('delicious', 2), ('?', 1), ('or', 1), ('maybe', 1)]
'hamburger': [('eat', 2), ('juicy', 1), ('instead', 1), ('many', 1), ('agree', 1)]
'car': [('drive', 1), ('restaurant', 1), ('wash', 1), ('gasoline', 1)]
'cat': [('walk', 2), ('now', 1), ('sleeps', 1), ('on', 1), ('take', 1)]</code></pre>
<div id="stop-words" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="stop-words" class="callout-inner">
<h3 class="callout-title">Stop Words</h3>
<div class="callout-content">
<p>Depending on the use case you will always perform some preprocessing
steps in order to have the data as normalized and clean as possible
before making any computations. These steps are not exhaustive and are
task-dependent. In this case, we introduced the concept of “stop_words”
(extremely common words that do not provide relevant information for our
use case and, given their high frequency, they tend to obscure the
results we are interested in). Of course, our list could have been much
bigger, but it served the purpose for this toy example.</p>
<p>Spacy has a pre-defined list of stopwords per language. To explicitly
load the English stop words we can do</p>
<div class="codewrapper sourceCode" id="cb20">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" tabindex="-1"></a><span class="im">from</span> spacy.lang.en.stop_words <span class="im">import</span> STOP_WORDS</span>
<span id="cb20-2"><a href="#cb20-2" tabindex="-1"></a><span class="bu">print</span>(STOP_WORDS)  <span class="co"># a set of common stopwords</span></span>
<span id="cb20-3"><a href="#cb20-3" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(STOP_WORDS)) <span class="co"># There are 326 words considered in this list</span></span></code></pre>
</div>
<p>Alternatively you can filter out stop words when iterating your
tokens (remember the spacy token properties!) like this:</p>
<div class="codewrapper sourceCode" id="cb21">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" tabindex="-1"></a>doc <span class="op">=</span> nlp(text)</span>
<span id="cb21-2"><a href="#cb21-2" tabindex="-1"></a>content_words <span class="op">=</span> [token.text <span class="cf">for</span> token <span class="kw">in</span> doc <span class="cf">if</span> token.is_alpha <span class="kw">and</span> <span class="kw">not</span> token.is_stop]  <span class="co"># Filter out stop words and punctuation</span></span>
<span id="cb21-3"><a href="#cb21-3" tabindex="-1"></a><span class="bu">print</span>(content_words)</span></code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="sparsity">Sparsity<a class="anchor" aria-label="anchor" href="#sparsity"></a>
</h3>
<p>Another key property of linguistic data is its sparsity. This means
that if we are hunting for a specific phenomenon, we will realize it
barely occurs inside a enormous amount of text. Our previous example of
<em>pizzas</em> and <em>hamburgers</em> worked for us because our
experiment was run in an extremely hand-crafted text for the purpose.
However, it would be hard to scale this up in a real setting, where we
would possibly need to dive into public reviews that specifically
mentions those two foods to construct a corpus.</p>
<p>Sparsity is tightly link to what is frequently called
<strong>domain-specific data</strong>. The discourse context in which
language is used varies importantly across disciplines (domains). Take
for example law texts and medical texts, the meaning of concepts
described in each domain will significantly differ. Another example
would be to compare texts written in 16th century English vs 21st
century English, where significant shifts have occurred at all
linguistic levels. For this reason there are specialized models and
corpora that model language use in specific domains. The concept of
fine-tunning a general purpose model with domain-specific data is also
popular.</p>
<p>While it’s true that sparsity has been reduced after the era of LLMs
and big training corpora, especially when dealing with general purpose
tasks, if you have domain specific objectives you should take special
care on the assumptions and results you get out of pre-trained models,
including LLMs.</p>
<div id="nlp-in-the-real-world-1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="nlp-in-the-real-world-1" class="callout-inner">
<h3 class="callout-title">NLP in the real world</h3>
<div class="callout-content">
<p>Use what you have learned so far to search inside the Frankenstein
book how many times does the word “love” appear, and how many times does
“hate” appear. Compute what percentage of the content words in the text
do these two terms together represent? To do this experiment you
should:</p>
<ol style="list-style-type: decimal">
<li>Read the file and save it into a text variable</li>
<li>Use spacy to load the text into a Doc object.</li>
<li>Iterate the document and keep all tokens that are alphanumeric (use
the token.is_alpha property), and are not stopwords (use the property
token.is_stop).</li>
<li>Lowercase all the tokens to merge the instances of “Love” and “love”
into a single one.</li>
<li>Iterate the tokens and count how many of them are exactly
“love”</li>
<li>Iterate the tokens and count how many of them are exactly
“hate”</li>
<li>compute the percentage of hate + love compared to all content words.
For example with: (len(hate_words) + len(love_words)) /
len(content_words) * 100</li>
</ol>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3"> Show me the solution </h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" data-bs-parent="#accordionSolution3" aria-labelledby="headingSolution3">
<div class="accordion-body">
<p>Following our preprocessing procedure, there are <strong>30,500
content words</strong>. The word <strong>love appears 59 times</strong>
and the word <strong>hate appears only 9 times</strong>. These are 0.22%
of the total words in the text. Even though intuitively these words
should be quite common, in reality they occur only a handful of times.
So if we are interested in studying the occurrences of love/hate in the
novel, we can only rely on those occurrences. Code:</p>
<div class="codewrapper sourceCode" id="cb22">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">"84_frankenstein_clean.txt"</span>) <span class="im">as</span> f:</span>
<span id="cb22-2"><a href="#cb22-2" tabindex="-1"></a>  text <span class="op">=</span> f.read()</span>
<span id="cb22-3"><a href="#cb22-3" tabindex="-1"></a></span>
<span id="cb22-4"><a href="#cb22-4" tabindex="-1"></a>doc <span class="op">=</span> nlp(text)  <span class="co"># Process the text with SpaCy</span></span>
<span id="cb22-5"><a href="#cb22-5" tabindex="-1"></a>words <span class="op">=</span> [token.lower_ <span class="cf">for</span> token <span class="kw">in</span> doc <span class="cf">if</span> token.is_alpha <span class="kw">and</span> <span class="kw">not</span> token.is_stop]</span>
<span id="cb22-6"><a href="#cb22-6" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Total Words:"</span>, <span class="bu">len</span>(words))</span>
<span id="cb22-7"><a href="#cb22-7" tabindex="-1"></a></span>
<span id="cb22-8"><a href="#cb22-8" tabindex="-1"></a>love_words <span class="op">=</span> [word <span class="cf">for</span> word <span class="kw">in</span> words <span class="cf">if</span> <span class="st">"love"</span> <span class="op">==</span> word]</span>
<span id="cb22-9"><a href="#cb22-9" tabindex="-1"></a>hate_words <span class="op">=</span> [word <span class="cf">for</span> word <span class="kw">in</span> words <span class="cf">if</span> <span class="st">"hate"</span> <span class="op">==</span> word]</span>
<span id="cb22-10"><a href="#cb22-10" tabindex="-1"></a></span>
<span id="cb22-11"><a href="#cb22-11" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Love and Hate percentage:"</span>, (<span class="bu">len</span>(love_words) <span class="op">+</span> <span class="bu">len</span>(hate_words)) <span class="op">/</span> <span class="bu">len</span>(words) <span class="op">*</span> <span class="dv">100</span>, <span class="st">"</span><span class="sc">% o</span><span class="st">f content words"</span>)</span></code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="nlp-machine-learning-linguistics">NLP = Machine Learning + Linguistics<a class="anchor" aria-label="anchor" href="#nlp-machine-learning-linguistics"></a>
</h2>
<hr class="half-width">
<p>So far we saw how important it is to consider the linguistic
properties of our data. We now recall that NLP is also built on top of
ideas from Machine Learning, Deep Learning. A general workflow for
solving an NLP task therefore looks quite close to the general Machine
Learning Workflow:</p>
<ol style="list-style-type: decimal">
<li>Formulate the problem</li>
<li>Gather relevant data</li>
<li>Data pre-processing</li>
<li>Structure your Corpus (Inputs/Outputs)</li>
<li>Split your Data (Train/Validation/Test)</li>
<li>Choose Approach/Model/Architecture</li>
<li>If necessary Train or Fine-tune a new Model</li>
<li>Evaluate Results on Test Set</li>
<li>Refine your approach</li>
<li>Share model/results</li>
</ol>
<p>With this in mind we can start now exploring more NLP techniques!</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<span class="callout-header">Key Points</span>
<div class="callout-inner">
<div class="callout-content">
<ul>
<li>NLP is embedded in numerous daily-use products</li>
<li>Key tasks include language modeling, text classification,
information extraction, information retrieval, conversational agents,
and topic modeling, each supporting various real-world
applications.</li>
<li>NLP is a subfield of Artificial Intelligence (AI) that deals with
approaches to process, understand and generate natural language</li>
<li>Deep learning has significantly advanced NLP, but the challenge
remains in processing the discrete and ambiguous nature of language</li>
<li>The ultimate goal of NLP is to enable machines to understand and
process language as humans do, but challenges in measuring and
interpreting linguistic information still exist.</li>
</ul>
</div>
</div>
</div>
</section></section><section id="aio-02-preprocessing"><p>Content from <a href="02-preprocessing.html">Episode 1: From text to vectors</a></p>
<hr>
<p>Last updated on 2025-09-18 |

        <a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/edit/main/episodes/02-preprocessing.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 120 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How do I load text and do basic linguistic analysis?</li>
<li>Why do we need to prepare a text for training?</li>
<li>How do I use words as features in a machine learning model?</li>
<li>What is a word2vec model?</li>
<li>What properties do word embeddings have?</li>
<li>What insights can I get from word embeddings?</li>
<li>How do we train a word2vec model?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<p>After following this lesson, learners will be able to:</p>
<ul>
<li>Implement a full preprocessing pipeline on a text</li>
<li>Use Word2Vec to train a model</li>
<li>Inspect word embeddings</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<hr class="half-width">
<p>In the previous episode we emphasized how text is different from
structured datasets. Given the linguistic properties embedded in
unstructured text, we also learned how to use existing libraries such as
SpaCy for segmenting text and accessing basic linguistic properties.</p>
<p>We learned the different levels of language and that it is ambiugous,
compositional and discrete. Because of this, it is hard to know how
words relate with each other, therefore obtaining meaning from text
alone is possible only through proxies that can be quantified. We made
our first attempt to approach word meaning by using co-occurrences of
words in a fixed text window around specific target words of
interest.</p>
<p>In this episode, we will expand on this idea and continue working
with words as individual features of text. We will introduce the concept
of Term-Document matrix, one of the most basic techniques to use words
as features that represent the texts where they appear, which can be fed
directly it into Machine Learning classifiers.</p>
<p>We will then visit the distributional hypothesis, which the linguist
J.R. Firth, in the 1950s, summarized with the phrase: “You shall know a
word by the company it keeps”. Based on this hypothesis, Mikolov et.
al. decided to train neural networks on large amounts of text in order
to predict a word based on it’s surrounding context or viceversa, in the
famous Word2Vec model. We will learn how to use these models, and
understand how do they map discrete words into numerical vectors that
capture the semantic similarity of words in a continuous space. By
representing words with vectors, we can mathematically manipulate them
through vector arithmetic and exploit the similarity patterns that
emerge from a collection of texts. Finally, we will show how to train
your own Word2Vec models.</p>
</section><section><h2 class="section-heading" id="preprocessing-text">Preprocessing Text<a class="anchor" aria-label="anchor" href="#preprocessing-text"></a>
</h2>
<hr class="half-width">
<p>NLP models work by learning the statistical regularities within the
constituent parts of the language (i.e, letters, digits, words and
sentences) in a text. However, text contains also other type of
information that humans find useful to convey meaning. To signal pauses,
give emphasis and convey tone, for instance, we use punctuation.
Articles, conjunctions and prepositions also alter the meaning of a
sentence. The machine does not know the difference among all of these
linguistic units, as it treats them all as equal.</p>
<p>We have already done some basic data pre-processing in the
introduction. Here we will formalize this initial step and present some
of the most common pre-processing steps when dealing with structured
data. This is analogue to the data cleaning and sanitation step in any
Machine Learning task. In the case of linguistic data, we are interested
in getting rid of unwanted components (such as rare punctuation or
formatting characters) that can confuse a tokenizer and, depending on
the task at hand, we might also be interested in normalizing our tokens
to avoid possible noise in our final results. As we already know, an NLP
module such as SpaCy comes in handy to deal with the preprocessing of
text, here is the list of the recommended (always optional!) steps:</p>
<ul>
<li>
<strong>Tokenization:</strong> splitting strings into
meaningful/useful units. This step also includes a method for “mapping
back” the segments to their character position in the original
string.</li>
<li>
<strong>Lowercasing:</strong> removing uppercases to e.g. avoid
treating “Dog” and “dog” as two different words)</li>
<li>
<strong>Punctuation and Special Character Removal:</strong> if we
are interested in <em>content only,</em> we can filter out anything that
is not alphanumerical. We can also explicitly exlude symbols that are
just noise in our dataset. Note that getting rid of punctuation can
significantly change meaning! A special mention is that new lines are a
character in text, sometimes we can use them in our benefit (for example
to separate paragraphs) but many times they are just noise.</li>
<li>
<strong>Stop Word Removal:</strong> as we’ve seen, the most frequent
words in texts are those which contribute little semantic value on their
own: articles (‘the’, ‘a’ , ‘an’), conjunctions (‘and’, ‘or’, ‘but’),
prepositions (‘on’, ‘by’), auxiliary verbs (‘is’, ‘am’), pronouns (‘he’,
‘which’), or any highly frequent word that might not be of interest in
several <em>content only</em> related tasks. A special case is the word
‘not’ which carries the significant semantic value of negation.</li>
<li>
<strong>Lemmatization:</strong> although it has become less
frequent, normalizing words into their <em>dictionary form</em> can help
to focus on relevant aspects of text. Think how “eating”, “ate”, “eaten”
are all a variation of the verb “eat”.</li>
</ul>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a> <span class="co">"Here my python code using spacy... Here we load the 'Dirty Book of Frankenstein' and the task is to arrive to the clean tokens to train Word2Vec"</span></span></code></pre>
</div>
<div id="callout1" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div class="callout-inner">
<div class="callout-content">
<ul>
<li><p>Preprocessing approaches affect significantly the quality of the
training when working with word embeddings. For example, [Rahimi &amp;
Homayounpour (2022)] (<a href="https://link.springer.com/article/10.1007/s10579-022-09620-5" class="external-link uri">https://link.springer.com/article/10.1007/s10579-022-09620-5</a>)
demonstrated that for text classification and sentiment analysis, the
removal of punctuation and stopwords leads to higher
performance.</p></li>
<li><p>You do not always need to do all the preprocessing steps, and
which ones you should do depends on what you want to do. For example, if
you want to segment text into sentences then characters such as ‘.’, ‘,’
or ‘?’ are the most important; if you want to extract Named Entities
from text, you explicitly do not want to lowercase the text, as capitals
are a component in the identification process, and if you are interested
in gender bias you definitely want to keep the pronouns, etc…</p></li>
<li><p>Preprocessing can be very diffent for different languages. This
is both in terms of which steps to apply, but also which methods to use
for a specific step.</p></li>
</ul>
</div>
</div>
</div>
<p>We will prepare the data for the two experiments in this episode 1.
Build a Term-Document Matrix 2. Train a Word2Vec model</p>
<p>For both taska we need to prepare our texts by applying the same
preprocessing steps. We are focusing on content words for now, so even
though our preprocessin will unfortunately loose a lot of the original
information, in exchange we will be able manipulate words as individual
numeric representations. Therefore the preprocessing includes: cleaning
the text, tokenizing, lowercasing words, removing punctuation,
lemmatizing words and removing stop words. Let’s apply this step by
step.</p>
<div class="section level3">
<h3 id="cleaning-the-text">1. Cleaning the text<a class="anchor" aria-label="anchor" href="#cleaning-the-text"></a>
</h3>
<p>We start by importing the <code>spaCy</code> library that will help
us go through the preprocessing steps. SpaCy is a popular open-source
library for NLP in Python and it works with pre-trained languages models
that we can load and use to process and analyse the text efficiently. We
can then load the SpaCy model into the pipeline function.</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="im">import</span> spacy</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>doc <span class="op">=</span> nlp(corpus)</span></code></pre>
</div>
<p>Next, we’ll eliminate the triple dashes that separate different news
articles, as well as the vertical bars used to divide some columns.</p>
</div>
<div class="section level3">
<h3 id="tokenizing">2. Tokenizing<a class="anchor" aria-label="anchor" href="#tokenizing"></a>
</h3>
<p>Tokenization is essential in NLP, as it helps to create structure
from raw text. It involves the segmentation of the text into smaller
units referred as <code>tokens</code>. Tokens can be sentences
(e.g. <code>'the happy cat'</code>), words
(<code>'the', 'happy', 'cat'</code>), subwords
(<code>'un', 'happiness'</code>) or characters
(<code>'c','a', 't'</code>). The choice of tokens depends by the
requirement of the model used for training, and the text. This step is
carried out by a pre-trained model (called tokeniser) that has been
fine-tuned for the target language. In our case, this is
<code>en_core_web_sm</code> loaded before.</p>
<div id="callout2" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div class="callout-inner">
<div class="callout-content">
<p>A good word tokeniser for example, does not simply break up a text
based on spaces and punctuation, but it should be able to
distinguish:</p>
<ul>
<li>abbreviations that include points (e.g.: <em>e.g.</em>)</li>
<li>times (<em>11:15</em>) and dates written in various formats
(<em>01/01/2024</em> or <em>01-01-2024</em>)</li>
<li>word contractions such as <em>don’t</em>, these should be split into
<em>do</em> and <em>n’t</em>
</li>
<li>URLs</li>
</ul>
<p>Many older tokenisers are rule-based, meaning that they iterate over
a number of predefined rules to split the text into tokens, which is
useful for splitting text into word tokens for example. Modern large
language models use subword tokenisation, which are more flexible.</p>
</div>
</div>
</div>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a>spacy_corpus <span class="op">=</span> nlp(corpus_clean)</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a><span class="co"># Get the tokens from the pipeline</span></span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>tokens <span class="op">=</span> [token.text <span class="cf">for</span> token <span class="kw">in</span> spacy_corpus]</span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a>tokens[:<span class="dv">10</span>]</span></code></pre>
</div>
<p><code>['mens', 'op', 'maan', '\n ', '„', 'de', 'eagle', 'is', 'geland', '”']</code></p>
<p>As one can see the tokeniser has split each word in a token, however
it has considered also blank spaces <code>\n</code> and also
punctuation.</p>
</div>
<div class="section level3">
<h3 id="lowercasing">3. Lowercasing<a class="anchor" aria-label="anchor" href="#lowercasing"></a>
</h3>
<p>Our next step is to lowercase the text. Our goal here is to generate
a list of unique words from the text, so in order to not have words
twice in the list - once normal and once capitalised when it is at the
start of a sentence for example - we can lowercase the full text.</p>
<p>corpus_lower = corpus_clean.lower()</p>
<p>print(corpus_lower)</p>
<p>mens op maan „ de eagle is geland ” reisduur : 102 uur , uitstappen
binnen 20 iuli , 21.17 uur 45 […]</p>
</div>
<div class="section level3">
<h3 id="remove-punctuation">4. Remove punctuation<a class="anchor" aria-label="anchor" href="#remove-punctuation"></a>
</h3>
<p>The next step we will apply is to remove punctuation. We are
interested in training our model to learn the meaning of the words. This
task is highly influenced by the state of our text and punctuation would
decrease the quality of the learning as it would add spurious
information. We’ll see how the learning process works later in the
episode.</p>
<p>The punctuation symbols are defined in:</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="im">import</span> string</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>string.punctuation</span></code></pre>
</div>
<p>We can loop over these symbols to remove them from the text:</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="co"># remove punctuation from set</span></span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>tokens_no_punct <span class="op">=</span> [token <span class="cf">for</span> token <span class="kw">in</span> tokens <span class="cf">if</span> token <span class="kw">not</span> <span class="kw">in</span> string.punctuation]</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a><span class="co"># remove also blank spaces</span></span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a>tokens_no_punct <span class="op">=</span> [token <span class="cf">for</span> token <span class="kw">in</span> tokens_no_punct <span class="cf">if</span> token.strip() <span class="op">!=</span> <span class="st">''</span>]</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="bu">print</span>(tokens_no_punct[:<span class="dv">10</span>])</span></code></pre>
</div>
<p><code>['mens', 'op', 'maan', 'de', 'eagle', 'is', 'geland', 'reisduur', '102', 'uur']</code></p>
<div class="section level4">
<h4 id="visualise-the-tokens">Visualise the tokens<a class="anchor" aria-label="anchor" href="#visualise-the-tokens"></a>
</h4>
<p>This was the end of our preprocessing step. Let’s look at what tokens
we have extracted and how frequently they occur in the text.</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> Counter</span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a><span class="co"># count the frequency of occurrence of each token</span></span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a>token_counts <span class="op">=</span> Counter(tokens_no_punct)</span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a><span class="co"># get the top n most common tokens (otherwise the plot would be too crowded) and their relative frequencies</span></span>
<span id="cb7-8"><a href="#cb7-8" tabindex="-1"></a>most_common <span class="op">=</span> token_counts.most_common(<span class="dv">100</span>)</span>
<span id="cb7-9"><a href="#cb7-9" tabindex="-1"></a>tokens <span class="op">=</span> [item[<span class="dv">0</span>] <span class="cf">for</span> item <span class="kw">in</span> most_common]</span>
<span id="cb7-10"><a href="#cb7-10" tabindex="-1"></a>frequencies <span class="op">=</span> [item[<span class="dv">1</span>] <span class="cf">for</span> item <span class="kw">in</span> most_common]</span>
<span id="cb7-11"><a href="#cb7-11" tabindex="-1"></a></span>
<span id="cb7-12"><a href="#cb7-12" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">6</span>))</span>
<span id="cb7-13"><a href="#cb7-13" tabindex="-1"></a>plt.bar(tokens, frequencies)</span>
<span id="cb7-14"><a href="#cb7-14" tabindex="-1"></a>plt.xlabel(<span class="st">'Tokens'</span>)</span>
<span id="cb7-15"><a href="#cb7-15" tabindex="-1"></a>plt.ylabel(<span class="st">'Frequency'</span>)</span>
<span id="cb7-16"><a href="#cb7-16" tabindex="-1"></a>plt.title(<span class="st">'Token Frequencies'</span>)</span>
<span id="cb7-17"><a href="#cb7-17" tabindex="-1"></a>plt.xticks(rotation<span class="op">=</span><span class="dv">90</span>)</span>
<span id="cb7-18"><a href="#cb7-18" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb7-19"><a href="#cb7-19" tabindex="-1"></a>plt.show()</span></code></pre>
</div>
<p>As one can see, words in the text have a very specific <a href="https://link.springer.com/article/10.3758/s13423-014-0585-6" class="external-link">skewed
distribution</a>, such that there are few very high-frequency words that
account for most of the tokens in text (e.g., articles, conjunctions)
and many low frequency words.</p>
</div>
</div>
<div class="section level3">
<h3 id="stop-word-removal">5. Stop word removal<a class="anchor" aria-label="anchor" href="#stop-word-removal"></a>
</h3>
<p>For some NLP tasks only the important words in the text are needed. A
text however often contains many <code>stop words</code>: common words
such as <code>de</code>, <code>het</code>, <code>een</code> that add
little meaningful content compared to nouns and words. In those cases,
it is best to remove stop words from your corpus to reduce the number of
words to process.</p>
</div>
</section><section><h2 class="section-heading" id="term-document-matrix">Term-Document Matrix<a class="anchor" aria-label="anchor" href="#term-document-matrix"></a>
</h2>
<hr class="half-width">
<p>A Term-Document Matrix (TDM) is a matrix where:</p>
<ul>
<li>Each <strong>row</strong> is a unique word (term) in the corpus</li>
<li>Each <strong>column</strong> is a document in the corpus</li>
<li>Each <strong>cell</strong> <span class="math inline">\((i,j)\)</span> has a value of 1 if the <span class="math inline">\(term_i\)</span> appears in <span class="math inline">\(column_j\)</span> or 0 otherwise</li>
</ul>
<p>This is also sometimes known as a bag-of-words as it ignores grammar
and word sequences in exchange of emphazising content, where each
document is characterized by the words that appear in it. Similar
documents will contain similar bags of words and documents that talk
about different topics will be associated with numerical columns that
are different from each other. Let’s look at a quick example:</p>
<ul>
<li>Doc 1: “Natural language processing is exciting”</li>
<li>Doc 2: “Processing natural language helps computers understand”</li>
<li>Doc 3: “Language processing with computers is NLP”</li>
<li>Doc 4: “Today it rained a lot”</li>
</ul>
<table class="table">
<thead><tr class="header">
<th>Term</th>
<th>Doc1</th>
<th>Doc2</th>
<th>Doc3</th>
<th>Doc3</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>natural</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>language</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="odd">
<td>processing</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="even">
<td>is</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="odd">
<td>exciting</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>helps</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="odd">
<td>computers</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="even">
<td>understand</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="odd">
<td>with</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="even">
<td>NLP</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="odd">
<td>today</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr class="even">
<td>it</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr class="odd">
<td>rained</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr class="even">
<td>a</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr class="odd">
<td>lot</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>We can represent each document by taking its column and treating it
as a vector of 0’s and 1’s. The vector is of fixed size (in this case
the vocabulary size is 15), therefore suitable for traditional ML
classifiers. With TDM there is a problem of scalability, as the matrix
size grows with the amount of documents times the vocabulary found in
the documents we are processing. This means that if we have 100
documents in which 5,000 unique words appear, we would have to store a
matrix of 500,000 numbers! We also have the problem of sparsity present:
document “vectors” will have mostly 0’s. TDM is also a good solution to
characterize documents based on their vocabulary, however the converse
is even more desirable: to characterize words based on the context where
they appear, so we can study words independently of their documents of
origin, and more importantly, how do they relate to each other. To solve
these and other limitations we enter the world of word embeddings!</p>
</section><section><h2 class="section-heading" id="what-are-word-embeddings">What are word embeddings?<a class="anchor" aria-label="anchor" href="#what-are-word-embeddings"></a>
</h2>
<hr class="half-width">
<p>A Word Embedding is a word representation type that maps words in a
numerical manner (i.e., into vectors) in a multidimensional space,
capturing their meaning based on characteristics or context. Since
similar words occur in similar contexts, or have same characteristics,
the system automatically learns to assign similar vectors to similar
words.</p>
<p>Let’s illustrate this concept using animals. This example will show
us an intuitive way of representing things into vectors.</p>
<p>Suppose we want to represent a <code>cat</code> using measurable
characteristics:</p>
<ul>
<li>Furriness: Let’s assign a score of 70 to a cat</li>
<li>Number of legs: A cat has 4 legs</li>
</ul>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a>cat <span class="op">=</span> np.array([[<span class="dv">70</span>, <span class="dv">4</span>]])</span></code></pre>
</div>
<p>So the vector representation of a cat becomes:
<code>[70 (furriness), 4 (legs)]</code></p>
<figure><img src="../fig/emb3.png" class="figure mx-auto d-block"></figure><p>This vector doesn’t fully describe a cat but provides a basis for
comparison with other animals.</p>
<p>Let’s add vectors for a dog and a caterpillar:</p>
<ul>
<li>Dog: [56, 4]</li>
<li>Caterpillar: [70, 100]</li>
</ul>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a></span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a>dog <span class="op">=</span> np.array([[<span class="dv">56</span>, <span class="dv">4</span>]])</span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a>caterpillar <span class="op">=</span> np.array([[<span class="dv">70</span>, <span class="dv">100</span>]])</span></code></pre>
</div>
<figure><img src="../fig/emb5.png" class="figure mx-auto d-block"></figure><p>To determine which animal is more similar to a cat, we use
<code>cosine similarity</code>, which measures the cosine of the angle
between two vectors.</p>
<div id="callout3" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div class="callout-inner">
<div class="callout-content">
<p><a href="https://en.wikipedia.org/wiki/Cosine_similarity" class="external-link">cosine
similarity</a> ranges between [<code>-1</code> and <code>1</code>]. It
is the cosine of the angle between two vectors, divided by the product
of their length. It is a useful metric to measure how similar two
vectors are likely to be.</p>
<figure><img src="../fig/emb12.png" alt="" class="figure mx-auto d-block"></figure>
</div>
</div>
</div>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a><span class="im">from</span> sklearn.metrics.pairwise <span class="im">import</span> cosine_similarity</span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a>similarity_cat_dog <span class="op">=</span> cosine_similarity(cat, dog)[<span class="dv">0</span>][<span class="dv">0</span>]</span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a>similarity_cat_caterpillar <span class="op">=</span> cosine_similarity(cat, caterpillar)[<span class="dv">0</span>][<span class="dv">0</span>]</span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a></span>
<span id="cb10-6"><a href="#cb10-6" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Cosine similarity between cat and dog: </span><span class="sc">{</span>similarity_cat_dog<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-7"><a href="#cb10-7" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Cosine similarity between cat and caterpillar: </span><span class="sc">{</span>similarity_cat_caterpillar<span class="sc">}</span><span class="ss">"</span>)</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a></span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a>Cosine similarity between cat <span class="kw">and</span> dog: <span class="fl">0.9998987965747193</span></span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a>Cosine similarity between cat <span class="kw">and</span> caterpillar: <span class="fl">0.6192653797321375</span></span></code></pre>
</div>
<p>The higher similarity score between the cat and the dog indicates
they are more similar based on these characteristics. Adding more
characteristics can enrich our vectors, detecting more semantic
nuances.</p>
<figure><img src="../fig/emb6.png" class="figure mx-auto d-block"></figure><div id="challenge1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div class="callout-inner">
<h3 class="callout-title">Challenge</h3>
<div class="callout-content">
<ul>
<li>Add one of two other dimensions. What characteristics could they
map?</li>
<li>Add another animal and map their dimensions</li>
<li>Compute again the cosine similarity among those animals and find the
couple that is the least similar and the most similar</li>
</ul>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<ol style="list-style-type: decimal">
<li>Add one of two other dimensions</li>
</ol>
<p>We could add the dimension of “velocity” or “speed” that goes from 0
to 100 meters/second.</p>
<ul>
<li>Caterpillar: 0.001 m/s</li>
<li>Cat: 1.5 m/s</li>
<li>Dog: 2.5 m/s</li>
</ul>
<p>(just as an example, actual speeds may vary)</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a>cat <span class="op">=</span> np.asarray([[<span class="dv">70</span>, <span class="dv">4</span>, <span class="fl">1.5</span>]])</span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a>dog <span class="op">=</span> np.asarray([[<span class="dv">56</span>, <span class="dv">4</span>, <span class="fl">2.5</span>]])</span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a>caterpillar <span class="op">=</span> np.asarray([[<span class="dv">70</span>, <span class="dv">100</span>, <span class="fl">.001</span>]])</span></code></pre>
</div>
<p>Another dimension could be weight in Kg:</p>
<ul>
<li>Caterpillar: .05 Kg</li>
<li>Cat: 4 Kg</li>
<li>Dog: 15 Kg</li>
</ul>
<p>(just as an example, actual weight may vary)</p>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a>cat <span class="op">=</span> np.asarray([[<span class="dv">70</span>, <span class="dv">4</span>, <span class="fl">1.5</span>, <span class="dv">4</span>]])</span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a>dog <span class="op">=</span> np.asarray([[<span class="dv">56</span>, <span class="dv">4</span>, <span class="fl">2.5</span>, <span class="dv">15</span>]])</span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a>caterpillar <span class="op">=</span> np.asarray([[<span class="dv">70</span>, <span class="dv">100</span>, <span class="fl">.001</span>, <span class="fl">.05</span>]])</span></code></pre>
</div>
<p>Then the cosine similarity would be:</p>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a>cosine_similarity(cat, caterpillar)</span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" tabindex="-1"></a>cosine_similarity(cat, dog)</span></code></pre>
</div>
<p>Output:</p>
<div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a>array([[<span class="fl">0.61814254</span>]])</span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a>array([[<span class="fl">0.97893809</span>]])</span></code></pre>
</div>
<ol start="2" style="list-style-type: decimal">
<li>Add another animal and map their dimensions</li>
</ol>
<p>Another animal that we could add is the Tarantula!</p>
<div class="codewrapper sourceCode" id="cb16">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a>cat <span class="op">=</span> np.asarray([[<span class="dv">70</span>, <span class="dv">4</span>, <span class="fl">1.5</span>, <span class="dv">4</span>]])</span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a>dog <span class="op">=</span> np.asarray([[<span class="dv">56</span>, <span class="dv">4</span>, <span class="fl">2.5</span>, <span class="dv">15</span>]])</span>
<span id="cb16-3"><a href="#cb16-3" tabindex="-1"></a>caterpillar <span class="op">=</span> np.asarray([[<span class="dv">70</span>, <span class="dv">100</span>, <span class="fl">.001</span>, <span class="fl">.05</span>]])</span>
<span id="cb16-4"><a href="#cb16-4" tabindex="-1"></a>tarantula <span class="op">=</span> np.asarray([[<span class="dv">80</span>, <span class="dv">6</span>, <span class="fl">.1</span>, <span class="fl">.3</span>]])</span></code></pre>
</div>
<ol start="3" style="list-style-type: decimal">
<li>Compute again the cosine similarity among those animals - find out
the most and least similar couple</li>
</ol>
<p>Given the values above, the least similar couple is the dog and the
caterpillar, whose cosine similarity is
<code>array([[0.60855407]])</code>.</p>
<p>The most similar couple is the cat and the tarantula:
<code>array([[0.99822302]])</code></p>
</div>
</div>
</div>
</div>
<p>By representing words as vectors with multiple dimensions, we capture
more nuances of their meanings or characteristics.</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<span class="callout-header">Key Points</span>
<div class="callout-inner">
<div class="callout-content">
<ul>
<li>We can represent text as vectors of numbers (which makes it
interpretable for machines)</li>
<li>The most efficient and useful way is to use word embeddings</li>
<li>We can easily compute how words are similar to each other with the
cosine similarity</li>
</ul>
</div>
</div>
</div>
<p>When semantic change occurs, words in their context <em>also</em>
change. We can trace how a word evolves semantically over time through
comparison of that word with other similar words. The idea is that the
most similar words are not always fixed in each different year, if a
word acquires a new meaning.</p>
</section><section><h2 class="section-heading" id="train-the-word2vec-model">Train the Word2Vec model<a class="anchor" aria-label="anchor" href="#train-the-word2vec-model"></a>
</h2>
<hr class="half-width">
<div class="section level3">
<h3 id="load-the-embeddings-and-inspect-them">Load the embeddings and inspect them<a class="anchor" aria-label="anchor" href="#load-the-embeddings-and-inspect-them"></a>
</h3>
<p>We proceed to load our models. We will load all pre-trained model
files from the original Word2Vec paper, which was trained on a big
corpus from Google News. The library <code>gensim</code> contains a
method called <code>KeyedVectors</code> which allows us to load
them.</p>
</div>
<div class="section level3">
<h3 id="prepare-the-data-to-be-ingested-by-the-model-preprocessing">Prepare the data to be ingested by the model (preprocessing)<a class="anchor" aria-label="anchor" href="#prepare-the-data-to-be-ingested-by-the-model-preprocessing"></a>
</h3>
<p>Also, the decision to remove or retain these parts of text is quite
crucial for training our model, as it affects the quality of generated
word vectors.</p>
<div id="dataset-size-in-training" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="dataset-size-in-training" class="callout-inner">
<h3 class="callout-title">Dataset size in training</h3>
<div class="callout-content">
<p>To obtain high-quality embeddings, the size/length of your training
dataset plays a crucial role. Generally <a href="https://cs.stanford.edu/~quocle/paragraph_vector.pdf" class="external-link">tens of
thousands of documents</a> are considered a reasonable amount of data
for decent results.</p>
<p>Is there however a strict minimum? Not really. Things to keep in mind
is that <code>vocabulary size</code>, <code>document length</code> and
<code>desired vector size</code> interacts with each other. The higher
the dimensional vectors (e.g. 200-300 dimensions) the more data is
required, and of high quality, i.e. that allows the learning of words in
a variety of contexts.</p>
<p>While word2vec models typically perform better with large datasets
containing millions of words, using a single page is sufficient for
demonstration and learning purposes. This smaller dataset allows us to
train the model quickly and understand how word2vec works without the
need for extensive computational resources.</p>
</div>
</div>
</div>
<p>For the purpose of this episode and to make training easy on our
laptop, we’ll train our word2vec model using <strong>just one
book</strong>. Subsequently, we’ll load pre-trained models for tackling
our task.</p>
<p>Now we will train a two-layer neural network to transform our tokens
into word embeddings. We will be using the library <code>gensim</code>
and the model we will be using is called <code>Word2Vec</code>,
developed by Tomas Mikolov et al. in 2013.</p>
<p>Import the necessary libraries:</p>
<div class="codewrapper sourceCode" id="cb17">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a><span class="im">import</span> gensim</span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a><span class="im">from</span> gensim.models <span class="im">import</span> Word2Vec</span>
<span id="cb17-3"><a href="#cb17-3" tabindex="-1"></a></span>
<span id="cb17-4"><a href="#cb17-4" tabindex="-1"></a><span class="co"># import logging to monitor training</span></span>
<span id="cb17-5"><a href="#cb17-5" tabindex="-1"></a><span class="im">import</span> logging</span>
<span id="cb17-6"><a href="#cb17-6" tabindex="-1"></a></span>
<span id="cb17-7"><a href="#cb17-7" tabindex="-1"></a><span class="co"># set up logging</span></span>
<span id="cb17-8"><a href="#cb17-8" tabindex="-1"></a>logging.basicConfig(<span class="bu">format</span><span class="op">=</span><span class="st">'</span><span class="sc">%(asctime)s</span><span class="st"> : </span><span class="sc">%(levelname)s</span><span class="st"> : </span><span class="sc">%(message)s</span><span class="st">'</span>, level<span class="op">=</span>logging.INFO)</span></code></pre>
</div>
<p>There are two main architectures for training Word2Vec:</p>
<ul>
<li>Continuous Bag-of-Words (CBOW): Predicts a target word based on its
surrounding context words.</li>
<li>Continuous Skip-Gram: Predicts surrounding context words given a
target word.</li>
</ul>
<figure><img src="../fig/emb13.png" class="figure mx-auto d-block"></figure><div id="callout5" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div class="callout-inner">
<div class="callout-content">
<p>CBOW is faster to train, while Skip-Gram is more effective for
infrequent words. Increasing context size improves embeddings but
increases training time.</p>
</div>
</div>
</div>
<p>We will be using CBOW. We are interested in having vectors with 300
dimensions and a context size of 5 surrounding words. We include all
words present in the corpora, regardless of their frequency of
occurrence and use 4 CPU cores for training. All these specifics are
translated in only one line of code.</p>
<p>Let’s train our model then:</p>
<div class="codewrapper sourceCode" id="cb18">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" tabindex="-1"></a>model <span class="op">=</span> Word2Vec([tokens_no_stopwords], vector_size<span class="op">=</span><span class="dv">300</span>, window<span class="op">=</span><span class="dv">5</span>, min_count<span class="op">=</span><span class="dv">1</span>, workers<span class="op">=</span><span class="dv">4</span>, sg<span class="op">=</span><span class="dv">0</span>)</span></code></pre>
</div>
<p>We can inspect already what’s the output of this training, by
checking the top 5 most similar words to “maan” (moon):</p>
<div class="codewrapper sourceCode" id="cb19">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" tabindex="-1"></a>word_vectors.most_similar(<span class="st">'maan'</span>, topn<span class="op">=</span><span class="dv">5</span>)</span></code></pre>
</div>
<p><code>[('plek', 0.48467501997947693), ('ouders', 0.46935707330703735), ('supe|', 0.3929591178894043), ('rotterdam', 0.37788015604019165), ('verkeerden', 0.33672046661376953)]</code></p>
<p>We have trained our model on one page only of the newspaper and the
training was very quick. However, to approach our problem it’s best to
train our model on the entire dataset. We dont’ have the resources for
doing that on our local laptop, but luckily for us, <a href="https://zenodo.org/records/3237380" class="external-link">Wevers, M (2019)</a> did that
already for us and released it publicly. Let’s download this dataset on
our laptop and let’s save them in a folder called <code>w2v</code>.</p>
<div class="codewrapper sourceCode" id="cb20">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" tabindex="-1"></a>folder_path <span class="op">=</span> <span class="st">'data/w2v/'</span></span></code></pre>
</div>
</div>
</section></section><section id="aio-03-transformers"><p>Content from <a href="03-transformers.html">Episode 2: BERT and Transformers</a></p>
<hr>
<p>Last updated on 2025-09-18 |

        <a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/edit/main/episodes/03-transformers.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 120 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What are some drawbacks of static word embeddings?</li>
<li>What are Transformers?</li>
<li>What is BERT and how does it work?</li>
<li>How can I use BERT to solve NLP tasks?</li>
<li>How should I evaluate my classifiers?</li>
<li>Which other Transformer variants are available?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Understand how a Transformer works and recognize their different use
cases.</li>
<li>Understand how to use pre-trained tranfromers (Use Case: BERT)</li>
<li>Use BERT to classify texts.</li>
<li>Use BERT as a Named Entity Recognizer.</li>
<li>Understand assumptions and basic evaluation for NLP outputs.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<p>Static word embeddings such as Word2Vec can be used to represent each
word as a unique vectors. Vector representations also allow us to apply
numerical operations that can be mapped to some syntactic and semantic
properties of words, such as the cases of analogies or finding synonyms.
Once we transform words into vectors, these can also be used as
<strong>features</strong> for classifiers that can be trained predict
any supervised NLP task.</p>
<p>However, a big drawback of Word2Vec is that <strong>each word is
represented in isolation</strong>, and unfortunately that is not how
language works. Words get their meanings based on the specific context
in which they are used (take for example polysemy, the cases where the
same word can have very different meanings depending on the context);
therefore, we would like to have richer vector representations of words
that also integrate context into account in order to obtain more
powerful representations.</p>
<div id="polysemy-in-language" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="polysemy-in-language" class="callout-inner">
<h3 class="callout-title">Polysemy in Language</h3>
<div class="callout-content">
<p>Think of (at least 2) different words that can have more than one
meaning depending on the context. Come up with one simple sentence per
meaning and explain what they mean in each context. Discuss: How do you
know what of the possible meanings does the word have when you use
it?</p>
<p>OPTIONAL: Why do you think Word2Vec can’t caputure different meanings
of words?</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<p>Two possible examples can be the words ‘fine’ and ‘run’</p>
<p>Sentences for ‘fine’: - She has a fine watch (fine == high-quality) -
He had to pay a fine (fine == penalty) - I am feeling fine (fine == not
bad)</p>
<p>Sentences for ‘run’: - I had to run to catch the bus (run == moving
fast) - Stop talking, before you run out of ideas (run (out) ==
exhaust)</p>
<p>Note how in the “run out” example we even have to understand that the
meaning of run is not literal but goes accompained with a preposition
that changes its meaning.</p>
</div>
</div>
</div>
</div>
<p>In 2019, the BERT language model was introduced. Using a novel
architecture called Transformer (2017), BERT can integrate context into
word representations. To understand BERT, we will first look at what a
transformer is and we will then directly use some code to make use of
BERT.</p>
<div class="section level1">
<h1 id="transformers">Transformers<a class="anchor" aria-label="anchor" href="#transformers"></a>
</h1>
<p>The Transformer is a neural network architecture proposed by Google
researchers <a href="https://arxiv.org/pdf/1706.03762" class="external-link">in 2017</a> in a
paper called <em>Attention is all you Need</em>. They tackled
specifically the NLP task of Machine Translation (MT), which is stated
as: how to generate a sentence (sequence of words) in target language B
given a sentence in source language A? We all know that translation
cannot be done word by word in isolations, therefore integrating the
context from both the source language and the target language is
necessary. In order to translate, first one neural network needs to
<em>encode</em> the whole meaning of the senetence in language A into a
single vector representation, then a second neural network needs to
<em>decode</em> that representation into tokens that are both coherent
with the meaning of language A and understandable in language B.
Therefore we say that translation is modeling language B
<em>conditioned</em> on what language A originally said.</p>
<figure><img src="../fig/trans1.png" alt="Transformer Architecture" class="figure mx-auto d-block"><div class="figcaption">Transformer Architecture</div>
</figure><p>As seen in the picture, the original Transformer is an
Encoder-Decoder network that tackles translation. We first need a token
embedder which converts the string of words into a sequence of vectors
that the Transformer network can process. The first component, the
<strong>Encoder</strong>, is optimized for creating <strong>rich
representations</strong> of the source sequence (in this case an English
sentence) while the second one, the <strong>Decoder</strong> is a
<strong>generative network</strong> that is conditioned on the encoded
representation. The third component we see is the infamous attention
mechanism, a third neural network what computes the correlation between
source and target tokens (<em>Which word in Dutch should I pay attention
to decide a better next English word?</em>) to generate the most likely
token in the target sequence (in this case Dutch words).</p>
<div id="emulate-the-attention-mechanism" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="emulate-the-attention-mechanism" class="callout-inner">
<h3 class="callout-title">Emulate the Attention Mechanism</h3>
<div class="callout-content">
<p>Pair with a person who speaks a language different from English (we
will cal it language B). This time you should think of 2 simple
sentences in English and come up with their translations in the second
language. In a piece of paper write down both sentences (one on top of
the other) and try to: 1. Draw a one to one mapping of words in English
to language B. Is it always possible to do this? 2. Think of each word
in language B and draw as many lines as necessary to the relevant
English words that can “help you” predict the word in language B. If you
managed, congratulations, this is how attention works!</p>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2"> Show me the solution </h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" data-bs-parent="#accordionSolution2" aria-labelledby="headingSolution2">
<div class="accordion-body">
<p>Here an image of a bilingual “manual attention” example</p>
</div>
</div>
</div>
</div>
<p>Next, we will see how BERT exploits the idea of a <strong>Transformer
Encoder</strong> to perform the NLP Task we are interested in:
generating powerful word representations.</p>
</div>
<div class="section level1">
<h1 id="bert">BERT<a class="anchor" aria-label="anchor" href="#bert"></a>
</h1>
<p><a href="https://aclanthology.org/N19-1423.pdf" class="external-link">BERT</a> is an
acronym that stands for <strong>B</strong>idirectional
<strong>E</strong>ncoder <strong>R</strong>epresentations from
<strong>T</strong>ransformers. The name describes it all: the idea is to
use the power of the Encoder component of the Transformer architecture
to create powerful token representations that preserve the contextual
meaning of the whole input segment, instead of each word in isolation.
The BERT vector representations of each token take into account both the
left context (what comes before the word) and the right context (what
comes after the word). Another advantage of the transformer Encoder is
that it is parallelizable, which made it posible for the first time to
train these networks on millions of datapoints, dramatically improving
model generalization.</p>
<div id="pretraining-bert" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="pretraining-bert" class="callout-inner">
<h3 class="callout-title">Pretraining BERT</h3>
<div class="callout-content">
<p>To obtain the BERT vector representations the Encoder is pre-trained
with two different tasks: - <strong>Masked Language Model:</strong> for
each sentence, mask one token at a time and predict which token is
missing based on the context from both sides. A training input example
would be “Maria [MASK] Groningen” and the model should predict the word
“loves”. - <strong>Next Sentence Prediction:</strong> the Encoder gets a
linear binary classifier on top, which is trained to decide for each
pair of sequences A and B, if sequence A precedes sequence B in a text.
For the sentence pair: “Maria loves Groningen.” and “This is a city in
the Netherlands.” the output of the classifier is “True” and for the
pair “Maria loves Groningen.” and “It was a tasty cake.” the output
should be “false” as there is no obvious continuation between the two
sentences.</p>
<p>Already the second pre-training task gives us an idea of the power of
BERT: after it has been pretrained on hundreds of thousands of texts,
one can plug-in a classifier on top and re-use the <em>linguistic</em>
knowledge previously acquired to fine-tune it for a specific task,
without needing to learn the weights of the whole network from scratch
all over again. In the next sections we will describe the components of
BERT and show how to use it. This model and hundreds of related
transformer-based pre-trained encoders can also be found on <a href="https://huggingface.co/google-bert/bert-base-cased" class="external-link">Hugging
Face</a>.</p>
</div>
</div>
</div>
</div>
<div class="section level1">
<h1 id="bert-architecture">BERT Architecture<a class="anchor" aria-label="anchor" href="#bert-architecture"></a>
</h1>
<p>The BERT Architecture can be seen as a basic NLP pipeline on its
own:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Tokenizer:</strong> splits text into tokens that the model
recognizes</li>
<li>
<strong>Embedder:</strong> converts each token into a fixed-sized
vector that represents it. These vectors are the actual input for the
Encoder.</li>
<li>
<strong>Encoder</strong> several neural layers that model the
token-level interactions of the input sequence to enhance meaning
representation. The output of the encoder is a set of
<strong>H</strong>idden layers, the vector representation of the
ingested sequence.</li>
<li>
<strong>Output Layer:</strong> the final encoder layer (which we
depict as a sequence <strong>H</strong>’s in the figure) contains
arguably the best token-level representations that encode syntactic and
semantic properties of each token, but this time each vector is already
contextualized with the specific sequence.</li>
<li>
<em>OPTIONAL</em> <strong>Classifier Layer:</strong> an additional
classifier can be connected on top of the BERT token vectors which are
used as features for performing a downstream task. This can be used to
classify at the text level, for example sentiment analysis of a
sentence, or at the token-level, for example Named Entity
Recognition.</li>
</ol>
<figure><img src="../fig/bert3.png" alt="BERT Architecture" class="figure mx-auto d-block"><div class="figcaption">BERT Architecture</div>
</figure><p>BERT uses (self-) attention, which is very useful to capture
longer-range word dependencies such as correference, where, for example,
a pronoun can be linked to the noun it refers to previously in the same
sentence. See the following example:</p>
<figure><img src="../fig/trans5.png" alt="The Encoder Self-Attention Mechanism" class="figure mx-auto d-block"><div class="figcaption">The Encoder Self-Attention Mechanism</div>
</figure><div class="section level2">
<h2 id="bert-for-word-based-analysis">BERT for Word-Based Analysis<a class="anchor" aria-label="anchor" href="#bert-for-word-based-analysis"></a>
</h2>
<p>Let’s see how these components can be manipulated with code. For this
we will be using the HugingFace’s <em>transformers</em> python library.
The first two main components we need to initialize are the model and
tokenizer. The HuggingFace hub contains thousands of models based on a
Transformer architecture for dozens of tasks, data domains and also
hundreds of languages. Here we will explore the vanilla English BERT
which was how everything started. We can initialize this model with the
next lines:</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> BertTokenizer, BertModel</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a>tokenizer <span class="op">=</span> BertTokenizer.from_pretrained(<span class="st">'bert-base-cased'</span>)</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a>model <span class="op">=</span> BertModel.from_pretrained(<span class="st">"bert-base-cased"</span>)</span></code></pre>
</div>
<div class="section level3">
<h3 id="bert-tokenizer">BERT Tokenizer<a class="anchor" aria-label="anchor" href="#bert-tokenizer"></a>
</h3>
<p>We start with a string of text as written in any blog, book,
newspaper etcetera. The <code>tokenizer</code> object is responsible of
splitting the string into recognizable tokens for the model and
embedding the tokens into their vector representations</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"Maria loves Groningen"</span></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>encoded_input <span class="op">=</span> tokenizer(text, return_tensors<span class="op">=</span><span class="st">'pt'</span>)</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a><span class="bu">print</span>(encoded_input)</span></code></pre>
</div>
<p>The print shows the <code>encoded_input</code> object returned by the
tokenizer, with its attributes and values. The <code>input_ids</code>
are the most important output for now, as these are the token IDs
recognized by BERT</p>
<pre><code>{
    'input_ids': tensor([[  101,  3406,  7871,   144,  3484, 15016,   102]]),
    'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]),
    'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])
}
</code></pre>
<p>NOTE: the printing function shows transformers objects as
dictionaries; however, to access the attributes, you must use the python
object syntax, such as in the following example:</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="bu">print</span>(encoded_input.input_ids.shape)</span></code></pre>
</div>
<p>Output:</p>
<p><code>torch.Size([1, 7])</code></p>
<p>The output is a 2-dimensional tensor where the first dimention
contains 1 element (this dimension represents the batch size), and the
second dimension contains 7 elements which are equivalent to the 7
tokens that BERT generated with our string input.</p>
<p>In order to see what these Token IDs represent, we can
<em>translate</em> them into human readable strings. This includes
converting the tensors into numpy arrays and converting each ID into its
string representation:</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a>token_ids <span class="op">=</span> <span class="bu">list</span>(encoded_input.input_ids[<span class="dv">0</span>].detach().numpy())</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>string_tokens <span class="op">=</span> tokenizer.convert_ids_to_tokens(token_ids)</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"IDs:"</span>, token_ids)</span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"TOKENS:"</span>, string_tokens)</span></code></pre>
</div>
<p><code>IDs: [101, 3406, 7871, 144, 3484, 15016, 102]</code></p>
<p><code>TOKENS: ['[CLS]', 'Maria', 'loves', 'G', '##ron', '##ingen', '[SEP]']</code></p>
<div id="callout2" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div class="callout-inner">
<div class="callout-content">
<p>In the case of wanting to obtain a single vector for
<em>enchanting</em>, you can average the three vectors that belong to
the token pieces that ultimately form that word. For example:</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>tok_en <span class="op">=</span> output.last_hidden_state[<span class="dv">0</span>][<span class="dv">15</span>].detach().numpy()</span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a>tok_chan <span class="op">=</span> output.last_hidden_state[<span class="dv">0</span>][<span class="dv">16</span>].detach().numpy()</span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a>tok_ting <span class="op">=</span> output.last_hidden_state[<span class="dv">0</span>][<span class="dv">17</span>].detach().numpy()</span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a>tok_enchanting <span class="op">=</span> np.mean([tok_en, tok_chan, tok_ting], axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a>tok_enchanting.shape</span></code></pre>
</div>
<p>We use the functions <code>detach().numpy()</code> to bring the
values from the Pytorch execution environment (for example a GPU) into
the main python thread and treat it as a numpy vector for convenvience.
Then, since we are dealing with three numpy vectors we can average the
three of them and end op with a single <code>enchanting</code> vector of
768-dimensions representing the average of
<code>'en', '##chan', '##ting'</code>.</p>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="polysemy-in-bert">Polysemy in BERT<a class="anchor" aria-label="anchor" href="#polysemy-in-bert"></a>
</h3>
<p>We can encode two sentences containing the word <em>note</em> to see
how BERT actually handles polysemy (<em>note</em> means something very
different in each sentence) thanks to the representation of each word
now being contextualized instead of isolated as was the case with
word2vec.</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="co"># Search for the index of 'note' and obtain its vector from the sequence</span></span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>note_index_1 <span class="op">=</span> string_tokens.index(<span class="st">"note"</span>)</span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a>note_vector_1 <span class="op">=</span> output.last_hidden_state[<span class="dv">0</span>][note_index_1].detach().numpy()</span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a>note_token_id_1 <span class="op">=</span> token_ids[note_index_1]</span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a><span class="bu">print</span>(note_index_1, note_token_id_1, string_tokens)</span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a><span class="bu">print</span>(note_vector_1[:<span class="dv">5</span>])</span></code></pre>
</div>
<p>We are basically printing the tokenized sentence from the previous
example and showing the index of the token <code>note</code> in the list
of tokens. We are also printing the tokenID assigned to this token and
the list of tokens. Finally, the last print shows the first five
dimensions of the vector representing the token <code>note</code>.</p>
<pre><code>12 3805 ['[CLS]', 'Maria', "'", 's', 'passion', 'for', 'music', 'is', 'clearly', 'heard', 'in', 'every', 'note', 'and', 'every', 'en', '##chan', '##ting', 'melody', '.', '[SEP]']
[0.15780845 0.38866335 0.41498923 0.03389652 0.40278202]</code></pre>
<p>Let’s encode now another sentence, also containing the word
<code>note</code>, and confirm that the same token string, with the same
assigned tokenID holds a vector with different weights:</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="co"># Encode and then take the 'note' token from the second sentence</span></span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a>note_text_2 <span class="op">=</span> <span class="st">"I could not buy milk in the supermarket because the bank note I wanted to use was fake."</span></span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a>encoded_note_2 <span class="op">=</span> tokenizer(note_text_2, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a>token_ids <span class="op">=</span> <span class="bu">list</span>(encoded_note_2.input_ids[<span class="dv">0</span>].detach().numpy())</span>
<span id="cb9-5"><a href="#cb9-5" tabindex="-1"></a>string_tokens_2 <span class="op">=</span> tokenizer.convert_ids_to_tokens(token_ids)</span>
<span id="cb9-6"><a href="#cb9-6" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" tabindex="-1"></a>note_index_2 <span class="op">=</span> string_tokens_2.index(<span class="st">"note"</span>)</span>
<span id="cb9-8"><a href="#cb9-8" tabindex="-1"></a>note_vector_2 <span class="op">=</span> model(<span class="op">**</span>encoded_note_2).last_hidden_state[<span class="dv">0</span>][note_index_2].detach().numpy()</span>
<span id="cb9-9"><a href="#cb9-9" tabindex="-1"></a>note_token_id_2 <span class="op">=</span> token_ids[note_index_2]</span>
<span id="cb9-10"><a href="#cb9-10" tabindex="-1"></a></span>
<span id="cb9-11"><a href="#cb9-11" tabindex="-1"></a><span class="bu">print</span>(note_index_2, note_token_id_2, string_tokens_2)</span>
<span id="cb9-12"><a href="#cb9-12" tabindex="-1"></a><span class="bu">print</span>(note_vector_2[:<span class="dv">5</span>])</span></code></pre>
</div>
<pre><code>12 3805 ['[CLS]', 'I', 'could', 'not', 'buy', 'milk', 'in', 'the', 'supermarket', 'because', 'the', 'bank', 'note', 'I', 'wanted', 'to', 'use', 'was', 'fake', '.', '[SEP]']
[ 0.5003222   0.653664    0.22919582 -0.32637975  0.52929205]</code></pre>
<p>To be sure, we can compute the cosine similarity of the word
<em>note</em> in the first sentence and the word <em>note</em> in the
second sentence confirming that they are indeed two different
representations, even when in both cases they have the same token-id and
they are the 12th token of the sentence:</p>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a><span class="im">from</span> sklearn.metrics.pairwise <span class="im">import</span> cosine_similarity</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a>vector1 <span class="op">=</span> np.array(note_vector_1).reshape(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a>vector2 <span class="op">=</span> np.array(note_vector_2).reshape(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb11-5"><a href="#cb11-5" tabindex="-1"></a></span>
<span id="cb11-6"><a href="#cb11-6" tabindex="-1"></a>similarity <span class="op">=</span> cosine_similarity(vector1, vector2)</span>
<span id="cb11-7"><a href="#cb11-7" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Cosine Similarity 'note' vs 'note': </span><span class="sc">{</span>similarity[<span class="dv">0</span>][<span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code></pre>
</div>
<p>With this small experiment, we have confirmed that the Encoder
produces context-dependent word representations, as opposed to Word2Vec,
where <em>note</em> would always have the same vector no matter where it
appeared.</p>
<div id="callout3" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div class="callout-inner">
<div class="callout-content">
<p>When running examples in a BERT pre-trained model, it is advisable to
wrap your code inside a <code>torch.no_grad():</code> context. This is
linked to the fact that BERT is a Neural Network that has been trained
(and can be further finetuned) with the Backpropagation algorithm.
Essentially, this wrapper tells the model that we are not in training
mode, and we are not interested in <em>updating</em> the weights (as it
would happen when training any neural network), because the weights are
already optimal enough. By using this wrapper, we make the model more
efficient as it does not need to calculate the gradients for an eventual
backpropagation step, since we are only interested in what <em>comes
out</em> of the Encoder. So the previous code can be made more efficient
like this:</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a><span class="im">import</span> torch </span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb12-4"><a href="#cb12-4" tabindex="-1"></a>    output <span class="op">=</span> model(<span class="op">**</span>encoded_input)</span>
<span id="cb12-5"><a href="#cb12-5" tabindex="-1"></a>    <span class="bu">print</span>(output)</span>
<span id="cb12-6"><a href="#cb12-6" tabindex="-1"></a>    <span class="bu">print</span>(output.last_hidden_state.shape)</span></code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="section level1">
<h1 id="bert-as-a-language-model">BERT as a Language Model<a class="anchor" aria-label="anchor" href="#bert-as-a-language-model"></a>
</h1>
<p>As mentioned before, the main pre-training task of BERT is Language
Modelling (LM): calculating the probability of a word based on the known
neighboring words (yes, Word2Vec was also a kind of LM!). Obtaining
training data for this task is very cheap, as all we need is millions of
sentences from existing texts, without any labels. In this setting, BERT
encodes a sequence of words, and predicts from a set of English tokens,
what is the most likely token that could be inserted in the
<code>[MASK]</code> position</p>
<figure><img src="../fig/bert1b.png" alt="BERT Language Modeling" class="figure mx-auto d-block"><div class="figcaption">BERT Language Modeling</div>
</figure><p>We can therefore start using BERT as a predictor for word completion.
From now own, we will learn how to use the <code>pipeline</code> object,
this is very useful when we only want to use a pre-trained model for
predictions (no need to fine-tune or do word-specific analysis). The
<code>pipeline</code> will internally initialize both model and
tokenizer for us and also merge back word pieces into complete
words.</p>
<p>In this case again we use <code>bert-base-cased</code>, which refers
to the vanilla BERT English model. Once we declared a pipeline, we can
feed it with sentences that contain one masked token at a time (beware
that BERT can only predict one word at a time, since that was its
training scheme). For example:</p>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a><span class="kw">def</span> pretty_print_outputs(sentences, model_outputs):</span>
<span id="cb13-4"><a href="#cb13-4" tabindex="-1"></a>    <span class="cf">for</span> i, model_out <span class="kw">in</span> <span class="bu">enumerate</span>(model_outputs):</span>
<span id="cb13-5"><a href="#cb13-5" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">=====</span><span class="ch">\t</span><span class="st">"</span>,sentences[i])</span>
<span id="cb13-6"><a href="#cb13-6" tabindex="-1"></a>        <span class="cf">for</span> label_scores <span class="kw">in</span> model_out:</span>
<span id="cb13-7"><a href="#cb13-7" tabindex="-1"></a>            <span class="bu">print</span>(label_scores)</span>
<span id="cb13-8"><a href="#cb13-8" tabindex="-1"></a></span>
<span id="cb13-9"><a href="#cb13-9" tabindex="-1"></a></span>
<span id="cb13-10"><a href="#cb13-10" tabindex="-1"></a>nlp <span class="op">=</span> pipeline(task<span class="op">=</span><span class="st">"fill-mask"</span>, model<span class="op">=</span><span class="st">"bert-base-cased"</span>, tokenizer<span class="op">=</span><span class="st">"bert-base-cased"</span>)</span>
<span id="cb13-11"><a href="#cb13-11" tabindex="-1"></a>sentences <span class="op">=</span> [<span class="st">"Paris is the [MASK] of France"</span>, <span class="st">"I want to eat a cold [MASK] this afternoon"</span>, <span class="st">"Maria [MASK] Groningen"</span>]</span>
<span id="cb13-12"><a href="#cb13-12" tabindex="-1"></a>model_outputs <span class="op">=</span> nlp(sentences, top_k<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb13-13"><a href="#cb13-13" tabindex="-1"></a>pretty_print_outputs(sentences, model_outputs)</span></code></pre>
</div>
<pre><code>=====	 Paris is the [MASK] of France
{'score': 0.9807965755462646, 'token': 2364, 'token_str': 'capital', 'sequence': 'Paris is the capital of France'}
{'score': 0.004513159394264221, 'token': 6299, 'token_str': 'Capital', 'sequence': 'Paris is the Capital of France'}
{'score': 0.004281804896891117, 'token': 2057, 'token_str': 'center', 'sequence': 'Paris is the center of France'}
{'score': 0.002848200500011444, 'token': 2642, 'token_str': 'centre', 'sequence': 'Paris is the centre of France'}
{'score': 0.0022805952467024326, 'token': 1331, 'token_str': 'city', 'sequence': 'Paris is the city of France'}

=====	 I want to eat a cold [MASK] this afternoon
{'score': 0.19168031215667725, 'token': 13473, 'token_str': 'pizza', 'sequence': 'I want to eat a cold pizza this afternoon'}
{'score': 0.14800849556922913, 'token': 25138, 'token_str': 'turkey', 'sequence': 'I want to eat a cold turkey this afternoon'}
{'score': 0.14620967209339142, 'token': 14327, 'token_str': 'sandwich', 'sequence': 'I want to eat a cold sandwich this afternoon'}
{'score': 0.09997560828924179, 'token': 5953, 'token_str': 'lunch', 'sequence': 'I want to eat a cold lunch this afternoon'}
{'score': 0.06001955270767212, 'token': 4014, 'token_str': 'dinner', 'sequence': 'I want to eat a cold dinner this afternoon'}

=====	 Maria [MASK] Groningen
{'score': 0.24399833381175995, 'token': 117, 'token_str': ',', 'sequence': 'Maria, Groningen'}
{'score': 0.12300779670476913, 'token': 1104, 'token_str': 'of', 'sequence': 'Maria of Groningen'}
{'score': 0.11991506069898605, 'token': 1107, 'token_str': 'in', 'sequence': 'Maria in Groningen'}
{'score': 0.07722211629152298, 'token': 1306, 'token_str': '##m', 'sequence': 'Mariam Groningen'}
{'score': 0.0632941722869873, 'token': 118, 'token_str': '-', 'sequence': 'Maria - Groningen'}
</code></pre>
<p>When we call the <code>nlp</code> pipeline, requesting to return the
<code>top_k</code> most likely suggestions to complete the provided
sentences (in this case <code>k=5</code>). The pipeline returns a list
of outputs as python dictionaries. Depending on the task, the fields of
the dictionary will differ. In this case, the <code>fill-mask</code>
task returns a score (between 0 and 1, the higher the score the more
likely the token is), a tokenId, and its corresponding string, as well
as the full “unmasked” sequence.</p>
<p>In the list of outputs we can observe: the first example shows
correctly that the missing token in the first sentence is
<em>capital</em>, the second example is a bit more ambiguous, but the
model at least uses the context to correctly predict a series of items
that can be eaten (unfortunately, none of its suggestions sound very
tasty); finally, the third example gives almost no useful context so the
model plays it safe and only suggests prepositions or punctuation. This
already shows some of the weaknesses of the approach.</p>
<p>We will next see the case of combining BERT with a classifier on
top.</p>
</div>
<div class="section level1">
<h1 id="bert-for-text-classification">BERT for Text Classification<a class="anchor" aria-label="anchor" href="#bert-for-text-classification"></a>
</h1>
<p>The task of text classification is assigning a label to a whole
sequence of tokens, for example a sentence. With the parameter
<code>task="text_classification"</code> the <code>pipeline()</code>
function will load the base model and automatically add a linear layer
with a softmax on top. This layer can be fine-tuned with our own labeled
data or we can also directly load the fully pre-trained text
classification models that are already available in HuggingFace.</p>
<figure><img src="../fig/bert4.png" alt="BERT as an Emotion Classifier" class="figure mx-auto d-block"><div class="figcaption">BERT as an Emotion Classifier</div>
</figure><p>Let’s see the example of a ready pre-trained emotion classifier based
on <code>RoBERTa</code> model. This model was fine-tuned in the Go
emotions <a href="https://huggingface.co/datasets/google-research-datasets/go_emotions" class="external-link">dataset</a>,
taken from English Reddit and labeled for 28 different emotions at the
sentence level. The fine-tuned model is called <a href="https://huggingface.co/SamLowe/roberta-base-go_emotions" class="external-link">roberta-base-go_emotions</a>.
This model takes a sentence as input and ouputs a probability
distribution over the 28 possible emotions that might be conveyed in the
text. For example:</p>
<div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a></span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a>classifier <span class="op">=</span> pipeline(task<span class="op">=</span><span class="st">"text-classification"</span>, model<span class="op">=</span><span class="st">"SamLowe/roberta-base-go_emotions"</span>, top_k<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb15-3"><a href="#cb15-3" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" tabindex="-1"></a>sentences <span class="op">=</span> [<span class="st">"I am not having a great day"</span>, <span class="st">"This is a lovely and innocent sentence"</span>, <span class="st">"Maria loves Groningen"</span>]</span>
<span id="cb15-5"><a href="#cb15-5" tabindex="-1"></a>model_outputs <span class="op">=</span> classifier(sentences)</span>
<span id="cb15-6"><a href="#cb15-6" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" tabindex="-1"></a>pretty_print_outputs(sentences, model_outputs)</span></code></pre>
</div>
<pre><code>=====	 I am not having a great day
{'label': 'disappointment', 'score': 0.46669483184814453}
{'label': 'sadness', 'score': 0.39849498867988586}
{'label': 'annoyance', 'score': 0.06806594133377075}

=====	 This is a lovely and innocent sentence
{'label': 'admiration', 'score': 0.6457845568656921}
{'label': 'approval', 'score': 0.5112180113792419}
{'label': 'love', 'score': 0.09214121848344803}

=====	 Maria loves Groningen
{'label': 'love', 'score': 0.8922032117843628}
{'label': 'neutral', 'score': 0.10132959485054016}
{'label': 'approval', 'score': 0.02525361441075802}</code></pre>
<p>This code outputs again a list of dictionaries with the
<code>top-k</code> (<code>k=3</code>) emotions that each of the two
sentences convey. In this case, the first sentence evokes (in order of
likelihood) <em>dissapointment</em>, <em>sadness</em> and
<em>annoyance</em>; whereas the second sentence evokes <em>love</em>,
<em>neutral</em> and <em>approval</em>. Note however that the likelihood
of each prediction decreases dramatically below the top choice, so
perhaps this specific classifier is only useful for the top emotion.</p>
<div id="callout4" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div class="callout-inner">
<div class="callout-content">
<p>Finetunning BERT is very cheap, because we only need to train the
<em>classifier</em> layer, a very small neural network, that can learn
to choose between the classes (labels) for your custom classification
problem, without needing a big amount of annotated data. This classifier
is just a one-layer neural layer with a softmax that assigns a score
that can be translated to the probability over a set of labels, given
the input features provided by BERT, which <em>encodes</em> the meaning
of the entire sequence in its hidden states.</p>
</div>
</div>
</div>
<figure><img src="../fig/bert4b.png" alt="BERT as an Emotion Classifier" class="figure mx-auto d-block"><div class="figcaption">BERT as an Emotion Classifier</div>
</figure>
</div>
<div class="section level1">
<h1 id="bert-for-token-classification">BERT for Token Classification<a class="anchor" aria-label="anchor" href="#bert-for-token-classification"></a>
</h1>
<p>Just as we plugged in a trainable text classifier layer, we can add a
token-level classifier that assigns a class to each of the tokens
encoded by a transformer (as opposed to one label for the whole
sequence). A specific example of this task is Named Entity Recognition,
but you can basically define any task that requires to
<em>highlight</em> sub-strings of text and classify them using this
technique.</p>
<div class="section level2">
<h2 id="named-entity-recognition">Named Entity Recognition<a class="anchor" aria-label="anchor" href="#named-entity-recognition"></a>
</h2>
<p>Named Entity Recognition (NER) is the task of recognizing mentions of
real-world entities inside a text. The concept of
<strong>Entity</strong> includes proper names that unequivocally
identify a unique individual (PER), place (LOC), organization (ORG), or
other object/name (MISC). Depending on the domain, the concept can
expanded to recognize other unique (and more conceptual) entities such
as DATE, MONEY, WORK_OF_ART, DISEASE, PROTEIN_TYPE, etcetera…</p>
<p>In terms of NLP, this boils down to classifying each token into a
series of labels (<code>PER</code>, <code>LOC</code>, <code>ORG</code>,
<code>O</code>[no-entity] ). Since a single entity can be expressed with
multiple words (e.g. New York) the usual notation used for labeling the
text is IOB (<strong>I</strong>nner <strong>O</strong>ut
<strong>B</strong>eginnig of entity) notations which identifies the
limits of each entity tokens. For example:</p>
<figure><img src="../fig/bert5.png" alt="BERT as an NER Classifier" class="figure mx-auto d-block"><div class="figcaption">BERT as an NER Classifier</div>
</figure><p>This is a typical sequence classification problem where an imput
sequence must be fully mapped into an output sequence of labels with
global constraints (for example, there can’t be an inner I-LOC label
before a beginning B-LOC label). Since the labels of the tokens are
context dependent, a language model with attention mechanism such as
BERT is very beneficial for a task like NER.</p>
<p>Because this is one of the core tasks in NLP, there are dozens of
pre-trained NER classifiers in HuggingFace that you can use right away.
We use once again the <code>pipeline()</code> to run the model for
predictions in your custom data, in this case with
<code>task="ner"</code>. For example:</p>
<div class="codewrapper sourceCode" id="cb17">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer, AutoModelForTokenClassification</span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb17-3"><a href="#cb17-3" tabindex="-1"></a></span>
<span id="cb17-4"><a href="#cb17-4" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(<span class="st">"dslim/bert-base-NER"</span>)</span>
<span id="cb17-5"><a href="#cb17-5" tabindex="-1"></a>model <span class="op">=</span> AutoModelForTokenClassification.from_pretrained(<span class="st">"dslim/bert-base-NER"</span>)</span>
<span id="cb17-6"><a href="#cb17-6" tabindex="-1"></a></span>
<span id="cb17-7"><a href="#cb17-7" tabindex="-1"></a>ner_classifier <span class="op">=</span> pipeline(<span class="st">"token-classification"</span>, model<span class="op">=</span>model, tokenizer<span class="op">=</span>tokenizer)</span>
<span id="cb17-8"><a href="#cb17-8" tabindex="-1"></a>example <span class="op">=</span> <span class="st">"My name is Wolfgang Schmid and I live in Berlin"</span></span>
<span id="cb17-9"><a href="#cb17-9" tabindex="-1"></a></span>
<span id="cb17-10"><a href="#cb17-10" tabindex="-1"></a>ner_results <span class="op">=</span> ner_classifier(example)</span>
<span id="cb17-11"><a href="#cb17-11" tabindex="-1"></a><span class="cf">for</span> nr <span class="kw">in</span> ner_results:</span>
<span id="cb17-12"><a href="#cb17-12" tabindex="-1"></a>    <span class="bu">print</span>(nr)</span></code></pre>
</div>
<p>The code prints the following:</p>
<pre><code>{'entity': 'B-PER', 'score': 0.9996068, 'index': 4, 'word': 'Wolfgang', 'start': 11, 'end': 19}
{'entity': 'I-PER', 'score': 0.999582, 'index': 5, 'word': 'Sc', 'start': 20, 'end': 22}
{'entity': 'I-PER', 'score': 0.9990482, 'index': 6, 'word': '##hm', 'start': 22, 'end': 24}
{'entity': 'I-PER', 'score': 0.9951691, 'index': 7, 'word': '##id', 'start': 24, 'end': 26}
{'entity': 'B-LOC', 'score': 0.99956733, 'index': 12, 'word': 'Berlin', 'start': 41, 'end': 47}</code></pre>
<p>In this case the output of the pipeline is a list of dictionaries,
each one representing only entity <code>IOB</code> labels at the BERT
token level. IMPORTANT: this list is per wordPiece and NOT per <em>human
word</em> even if the provided text is pre-tokenized. You can assume all
of the tokens that don’t appear in the output were labeled as no-entity,
that is <code>"O"</code>. To recover the full-word entities you can
initialize the pipeline with
<code>aggregation_strategy="first"</code>:</p>
<div class="codewrapper sourceCode" id="cb19">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" tabindex="-1"></a>ner_classifier <span class="op">=</span> pipeline(<span class="st">"token-classification"</span>, model<span class="op">=</span>model, tokenizer<span class="op">=</span>tokenizer, aggregation_strategy<span class="op">=</span><span class="st">"first"</span>)</span>
<span id="cb19-2"><a href="#cb19-2" tabindex="-1"></a>example <span class="op">=</span> <span class="st">"My name is Wolfgang Schmid and I live in Berlin"</span></span>
<span id="cb19-3"><a href="#cb19-3" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" tabindex="-1"></a>ner_results <span class="op">=</span> ner_classifier(example)</span>
<span id="cb19-5"><a href="#cb19-5" tabindex="-1"></a><span class="cf">for</span> nr <span class="kw">in</span> ner_results:</span>
<span id="cb19-6"><a href="#cb19-6" tabindex="-1"></a>    <span class="bu">print</span>(nr)</span></code></pre>
</div>
<p>The code now prints the following:</p>
<pre><code>{'entity_group': 'PER', 'score': 0.9995944, 'word': 'Wolfgang Schmid', 'start': 11, 'end': 26}
{'entity_group': 'LOC', 'score': 0.99956733, 'word': 'Berlin', 'start': 41, 'end': 47}</code></pre>
<p>As you can see, entities aggregated at the Span Leven (instead of the
Token Level). Word pieces are merged back into <em>human words</em> and
also multiword entities are assigned a single entity label unifying the
<code>IOB</code> labels into one. Depending on your use case you can
request the pipeline to give different
<code>aggregation_strateg[ies]</code>. More info about the pipeline can
be found <a href="https://huggingface.co/docs/transformers/main_classes/pipelines" class="external-link">here</a>.</p>
<p>The next step is crucial: evaluate how does the pre-trained model
actually performs in <strong>your dataset</strong>. This is important
since the fine-tuned model could be overfitted to other custom
benchmarks that do not share the characteristics of your dataset.</p>
<p>To observe this, we can first see the performance on the test portion
of the dataset in which this classifier was trained, and then evaluate
the same pre-trained classifier on a NER dataset form a different
domain.</p>
</div>
<div class="section level2">
<h2 id="model-evaluation">Model Evaluation<a class="anchor" aria-label="anchor" href="#model-evaluation"></a>
</h2>
<p>To perform evaluation in your data you can use again the
<code>seqeval</code> package:</p>
<div class="codewrapper sourceCode" id="cb21">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" tabindex="-1"></a></span>
<span id="cb21-2"><a href="#cb21-2" tabindex="-1"></a><span class="im">from</span> seqeval.metrics <span class="im">import</span> classification_report</span>
<span id="cb21-3"><a href="#cb21-3" tabindex="-1"></a><span class="bu">print</span>(classification_report(gold_labels, model_predictions))</span></code></pre>
</div>
<p>Since we took a classifier that was not trained for the book domain,
the performance is quite poor. But this example shows us that
classifiers performing very well on their own domain most of the times
transfer poorly to other apparently similar datasets.</p>
<p>The solution in this case is to use another of the great
characteristics of BERT: fine-tuning for domain adaptation. It is
possible to train your own classifier with relatively small data (given
that a lot of linguistic knowledge was already provided during the
language modeling pre-training). In the following section we will see
how to train your own NER model and use it for predictions.</p>
</div>
</div></section><section id="aio-04-LargeLanguageModels"><p>Content from <a href="04-LargeLanguageModels.html">Episode 3: Using large language models</a></p>
<hr>
<p>Last updated on 2025-09-18 |

        <a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/edit/main/episodes/04-LargeLanguageModels.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 60 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What is a Large Language Model (LLM)?</li>
<li>How do LLMs differ from traditional NLP models?</li>
<li>What is the Transformer architecture, and why is it important for
LLMs?</li>
<li>How does prompt engineering influence LLM outputs?</li>
<li>What are some real-world applications of LLMs?</li>
<li>How can LLMs generate, classify, and summarise text?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<p>After following this lesson, learners will be able to:</p>
<ul>
<li>Understand what Large Language Models (LLMs) are and their role in
NLP</li>
<li>Explain the Transformer architecture and why it is foundational for
LLMs</li>
<li>Use prompt engineering to generate high-quality text responses</li>
<li>Apply LLMs to real-world tasks like news article analysis</li>
<li>Explore pre-trained LLMs and experiment with custom tasks</li>
<li>Set up a chatbot and a simple RAG</li>
<li>Understand the impact of LLMs in modern AI and language
processing</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<p>Large Language Models (LLMs) are a hot and a big topic these days,
and are continuously in the news. Everybody heard of ChatGPT, many tried
it out for a variety of purposes, or even incorporated these tools in
their daily work. But what are these models exactly, how do they work
‘under the hood’, and how can you make use of them in the best possible
way?</p>
<p>In this episode, we will:</p>
<ul>
<li>Explore LLMs, which represent a significant advancement in natural
language processing (NLP). We will begin by defining what LLMs are and
touching on their foundational architecture, particularly the
Transformer model, which allows the LLM to understand and generate
human-like text.</li>
<li>Through practical examples, you will discover how to work with these
models for specific tasks with content generation.</li>
<li>We’ll also discuss real-world applications of LLMs, such as
analyzing news articles and generating insights from large amounts of
text data.</li>
</ul>
<p>This episode aims to equip you with both theoretical knowledge and
practical skills, preparing you to harness the power of LLMs in your own
projects and applications.</p>
<section><h2 class="section-heading" id="what-are-large-language-models">What are Large Language Models?<a class="anchor" aria-label="anchor" href="#what-are-large-language-models"></a>
</h2>
<hr class="half-width">
<p>Large language models (LLMs) represent a significant advancement in
artificial intelligence, designed to process and interpret large-scale
natural language data to generate responses to user queries. By being
trained on extensive datasets through advanced machine learning
algorithms, these models learn the intricate patterns, structures and
nuances of human language. This enables them to produce coherent and
natural-sounding language outputs across various inputs. As a result,
large language models are becoming increasingly essential in a range of
tasks such as text generation, text summarisation, rewriting, question
answering, and language translation.</p>
<p>The emergence of <em>ChatGPT</em>, powered by OpenAI’s advanced LLMs,
has brought these capabilities into the mainstream. With ChatGPT, users
interact through natural language, enabling seamless conversations and
performing complex tasks across various sectors, like customer service,
education, and content creation. Models like GPT-4, BERT, and LLaMA are
also used across various applications; from chatbots, virtual
assistants, text analysis, translation, summarisation, and more.</p>
<p>Notably, the success of ChatGPT and other LLM-driven applications
highlights their versatility and potential to transform how humans
interact with digital systems. These models continue to push the
boundaries of what’s possible in human-computer interaction, offering a
glimpse into a future where machines and humans communicate more
naturally and effectively.</p>
<div class="section level3">
<h3 id="timeline">Timeline<a class="anchor" aria-label="anchor" href="#timeline"></a>
</h3>
<p>The journey of large language models over the past decade highlights
the rapid evolution and growing impact of AI in language processing:</p>
<ul>
<li><p><strong>2012</strong>: Significant advancements in deep learning
architectures and access to larger datasets lead to the creation of the
GPT (Generative Pre-trained Transformer) framework, marking a pivotal
point in AI development.</p></li>
<li><p><strong>2018</strong>: Google unveils BERT (Bidirectional Encoder
Representations from Transformers), introducing a groundbreaking
architecture that sets the stage for the next generation of large
language models.</p></li>
<li><p><strong>2020</strong>: OpenAI releases GPT-3, which, with its
remarkable 175 billion parameters, establishes a new standard for
performance in language-related tasks.</p></li>
<li><p><strong>2022</strong>: The launch of ChatGPT marks a turning
point by making GPT-3 and similar models easily accessible to the public
via a web interface, significantly raising public awareness and interest
in LLMs and generative AI.</p></li>
<li><p><strong>2023</strong>: Open-source LLMs begin to deliver
impressive outcomes with the introduction of models like Dolly 2.0,
LLaMA, Alpaca, and Vicuna. Meanwhile, the release of GPT-4 pushes the
envelope by setting new records for both parameter scale and overall
performance.</p></li>
</ul>
</div>
<div class="section level3">
<h3 id="starting-up-a-simple-chat">Starting up a simple chat<a class="anchor" aria-label="anchor" href="#starting-up-a-simple-chat"></a>
</h3>
<p>Let’s explore how these LLMs work by doing a simple chat
demonstration.</p>
<p>Imagine asking a computer a question and getting a human-like
response. An LLM, like GPT-4, can generate responses that feel natural,
informative, and tailored to your question by analyzing the context and
meaning of your input. In the demonstration below, you’ll see how an LLM
responds to a prompt and how it builds upon previous information to
maintain the conversation.</p>
</div>
<div class="section level3">
<h3 id="example-chat">Example Chat<a class="anchor" aria-label="anchor" href="#example-chat"></a>
</h3>
<ul>
<li><p><em>User</em>: “What’s the tallest mountain in the
world?”</p></li>
<li><p><em>LLM</em>: “The tallest mountain in the world is Mount
Everest, which stands at 8,848 meters (29,029 feet).”</p></li>
<li><p><em>User</em>: “How long does it take to climb it?”</p></li>
<li><p><em>LLM</em>: “Climbing Mount Everest typically takes around two
months, due to the acclimatization process needed to adjust to high
altitudes. Expeditions often start from Base Camp in Nepal, where
climbers spend several weeks preparing.”</p></li>
<li><p><em>User</em>: “Could someone with no climbing experience try
it?”</p></li>
<li><p><em>LLM</em>: “Climbing Mount Everest is extremely challenging
and requires significant training and experience due to the harsh
conditions, high altitudes, and technical aspects of the climb. Most
climbers spend years training and gaining experience on other mountains
before attempting Everest.”</p></li>
</ul>
<p>This example shows how LLMs can maintain context across multiple
questions. Notice how the model connects information across each
question, keeping the conversation relevant and informed.</p>
<p>This quick chat example introduces you to the conversational
abilities of LLMs, demonstrating their ability to respond contextually,
provide coherent answers, and follow up on earlier parts of the
conversation.</p>
</div>
</section><section><h2 class="section-heading" id="how-are-llms-different-from-other-nlp-techniques">How are LLMs different from other NLP techniques?<a class="anchor" aria-label="anchor" href="#how-are-llms-different-from-other-nlp-techniques"></a>
</h2>
<hr class="half-width">
<p>LLMs stand apart from traditional NLP techniques due to their scale,
versatility, and reliance on deep learning models, especially
Transformers. Here’s a breakdown of how they differ:</p>
<div class="section level3">
<h3 id="scale-of-training-data-and-parameters">Scale of training data and parameters<a class="anchor" aria-label="anchor" href="#scale-of-training-data-and-parameters"></a>
</h3>
<ul>
<li>LLMs: Trained on massive datasets (trillions of words) and use
billions of parameters, allowing them to learn language patterns, facts,
and relationships at an unprecedented depth.</li>
<li>Traditional NLP: Techniques like rule-based systems or smaller
machine learning models rely on much smaller datasets, often requiring
domain-specific training for each task (e.g., sentiment analysis or
named entity recognition).</li>
</ul>
</div>
<div class="section level3">
<h3 id="model-architecture">Model architecture<a class="anchor" aria-label="anchor" href="#model-architecture"></a>
</h3>
<ul>
<li>LLMs: Use the Transformer architecture, particularly self-attention,
to analyze relationships between words regardless of position. This
allows them to capture long-range dependencies and context better than
traditional models.</li>
<li>Traditional NLP: Often use simpler models like bag-of-words, TF-IDF
(term frequency-inverse document frequency), RNNs (recurrent neural
networks), and LSTMs (long-short-term memory models), which treat words
independently or consider only local context, missing the complex,
global relationships.</li>
</ul>
</div>
<div class="section level3">
<h3 id="generalization-vs--task-specific-models">Generalization vs. task-specific models<a class="anchor" aria-label="anchor" href="#generalization-vs--task-specific-models"></a>
</h3>
<ul>
<li>LLMs: Can be applied across a wide range of tasks—summarization,
translation, question answering, etc.—without the need for separate
models for each. Fine-tuning makes them even more adaptable to specific
needs.</li>
<li>Traditional NLP: Typically requires developing or training a
separate model for each task. For example, separate models for sentiment
analysis, translation, and entity recognition.</li>
</ul>
</div>
<div class="section level3">
<h3 id="learning-from-unlabeled-data">Learning from unlabeled data<a class="anchor" aria-label="anchor" href="#learning-from-unlabeled-data"></a>
</h3>
<ul>
<li>LLMs: Leverage unsupervised or self-supervised learning during
pretraining, enabling them to learn language patterns from raw text
without human-labeled data.</li>
<li>Traditional NLP: are often supervised models, relying on labeled
data for training (e.g., labeled sentiment or part-of-speech tags),
which can be costly and time-consuming to create at scale.</li>
</ul>
</div>
<div class="section level3">
<h3 id="context-and-language-nuance">Context and language nuance<a class="anchor" aria-label="anchor" href="#context-and-language-nuance"></a>
</h3>
<ul>
<li>LLMs: Excel at understanding context, tone, and nuance, due to their
ability to weigh word relationships dynamically. This enables better
handling of idioms, sarcasm, and ambiguous phrases.</li>
<li>Traditional NLP: Struggles with complex language nuances, often
producing more rigid or literal interpretations. Contextual
understanding is limited, especially for longer sentences or
paragraphs.</li>
</ul>
</div>
<div class="section level3">
<h3 id="adaptability-and-fine-tuning">Adaptability and fine-tuning<a class="anchor" aria-label="anchor" href="#adaptability-and-fine-tuning"></a>
</h3>
<ul>
<li>LLMs: Easily adaptable to new tasks or domains with fine-tuning,
making them versatile across different applications.</li>
<li>Traditional NLP: Less flexible, often requiring retraining from
scratch or heavy feature engineering to adapt to new domains or
tasks.</li>
</ul>
<p>In short, LLMs represent a leap forward by combining scale,
flexibility, and deep learning power, allowing for more accurate,
nuanced, and versatile language processing compared to traditional NLP
techniques.</p>
</div>
</section><section><h2 class="section-heading" id="what-llms-are-good-at">What LLMs are good at<a class="anchor" aria-label="anchor" href="#what-llms-are-good-at"></a>
</h2>
<hr class="half-width">
<ul>
<li>
<em>Language generation</em>: Creating coherent and contextually
appropriate text, making them ideal for creative writing, chatbots, and
automated responses.</li>
<li>
<em>Summarization and translation</em>: Quickly summarizing
articles, books, and translating text between languages with reasonable
accuracy.</li>
<li>
<em>Information retrieval and answering questions</em>: LLMs can
recall and apply general knowledge from their training data to answer
questions, though they don’t actually “know” facts.</li>
<li>
<em>Sentiment and text classification</em>: LLMs can classify text
for tasks like sentiment analysis, spam detection, and topic
categorization.</li>
</ul></section><section><h2 class="section-heading" id="what-llms-struggle-with">What LLMs struggle with<a class="anchor" aria-label="anchor" href="#what-llms-struggle-with"></a>
</h2>
<hr class="half-width">
<ul>
<li>
<em>Fact-based accuracy</em>: Since LLMs don’t “know” facts, they
may generate incorrect or outdated information and are prone to
hallucinations (making up facts).</li>
<li>
<em>Understanding context over long passages</em>: LLMs can struggle
with context over very long texts and may lose track of earlier details,
affecting coherence.</li>
<li>
<em>Mathematical reasoning and logic</em>: Though improving, LLMs
often find complex problem-solving and detailed logical reasoning
challenging without direct guidance.</li>
<li>
<em>Ethical and sensitive issues</em>: LLMs may produce biased or
offensive text based on biases present in the training data, making
content moderation necessary in sensitive applications.</li>
</ul></section><section><h2 class="section-heading" id="how-do-llms-work">How do LLMs work?<a class="anchor" aria-label="anchor" href="#how-do-llms-work"></a>
</h2>
<hr class="half-width">
<p>So, how is it that you can chat with a model and receive responses
that seem almost human? The answer lies in the architecture and training
of Large Language Models (LLMs), which are powered by advanced neural
networks that understand, generate, and even translate human language
with surprising accuracy.</p>
<p>At the core of LLMs lies a framework known as the
<strong>transformer</strong>; a concept already encountered in the
previous episode. Transformers allow these models to process vast
amounts of text and learn the structure and nuances of language. This
setup enables LLMs not only to answer questions but also to predict,
complete, and even generate coherent text based on the patterns they’ve
learned.</p>
<p>LLMs are trained on large text datasets and later fine-tuned on
specific tasks, which helps them adapt to a wide range of applications,
from conversation to text classification. The result? A model that can
chat, summarize, translate, and much more—all by leveraging these core
mechanisms. LLM’s rely on the following key concepts:</p>
<ul>
<li>
<em>Transformers and self-attention</em>: The transformer
architecture, especially the self-attention mechanism, is at the heart
of LLMs. Self-attention enables these models to understand the
importance of each word in relation to others in a sequence, regardless
of their position.</li>
<li>
<em>Pretraining and fine-tuning</em>: LLMs are first pre-trained on
large text datasets using tasks like predicting the next word in a
sentence, learning language patterns. They are then fine-tuned on
specific tasks (e.g., translation, summarization) to enhance performance
for targeted applications.</li>
<li>
<em>Generative vs. discriminative models</em>: LLMs can be applied
to both generative tasks (e.g., text generation) and discriminative
tasks (e.g., classification).</li>
</ul>
<p>In practice, this attention mechanism helps LLMs produce coherent
responses by establishing relationships between words as each new token
is generated. Here’s how it works:</p>
<ul>
<li><p><em>Understanding word relationships</em>. Self-attention enables
the model to weigh the importance of each word in a sentence, no matter
where it appears, to make sense of the sentence as a whole.</p></li>
<li><p><em>Predicting next words based on context</em>. With these
relationships mapped out, the model can predict the next word in a
sequence. For example, in “The fox,” self-attention allows the model to
anticipate that “jumps” or “runs” might come next rather than something
unrelated like “table.”</p></li>
<li><p><em>Structuring responses</em>. As each word is generated, the
model assesses how each new token impacts the entire sentence, ensuring
that responses are relevant, logically sound, and grammatically correct.
This ability to “structure” language is why LLMs can produce responses
that are contextually meaningful and well-organized.</p></li>
</ul></section><section><h2 class="section-heading" id="a-zoo-of-large-language-models">A zoo of Large Language Models<a class="anchor" aria-label="anchor" href="#a-zoo-of-large-language-models"></a>
</h2>
<hr class="half-width">
<p>The era of Large Language Models gained momentum in 2018 with the
release of Google’s BERT. Since then, many companies have rapidly
developed newer and more powerful models. Among these are GPT (OpenAI),
Llama (Meta), Mistral (Mistral AI), Gemini (Google DeepMind), Claude
(Anthropic), and Grok (xAI). Some are open-source or more transparent
than others, revealing their architectures, parameter counts, or
training data and the collection thereof.</p>
<p>A list of LLMs: - GPT - OpenAI - Llama - Meta - Mistral / Mixtral -
Mistral AI (founded by former engineers from Google DeepMind and Meta) -
Gemini - Google DeepMind - Claude - Anthropic - (founded by former
OpenAI employees) - Grok - xAI (Elon Musk)</p>
<p>Training a large language model is extremely resource intensive. For
example, llama’s model Llama 3.1 405B is a model that has 405 billion
parameters. It was trained on 15 trillion tokens, uses 31 million GPU
hours (H100 gpus), and emitted almost 9000 tons of CO_2 (for the
training process only).</p>
<p>Inference also consumes considerable resources and has a significant
environmental impact. Large models require large memory for storing and
loading the model weights (storing weights alone can require
<em>hundreds</em> of gigabytes), and need high-performance GPUs to
achieve reasonable runtimes. As a result, many models operate on
cloud-based servers, increasing power consumption, especially when
scaled accomodate large numbers of users.</p>
</section><section><h2 class="section-heading" id="which-one-to-chose-when">Which one to chose when?<a class="anchor" aria-label="anchor" href="#which-one-to-chose-when"></a>
</h2>
<hr class="half-width">
<p>With so many available models the question arises “which model you
should use when”? One thing to consider here is whether you want to use
an open source model or not. But another important aspect is that it
depends on the task at hand. There are various leaderboards (for
example: <a href="https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/,%20%5BHELM%5D(https://crfm.stanford.edu/helm/lite/latest/)" class="external-link">HuggingFace</a>
that track which tasks specific models are good at, based on widely used
benchmarks. Also, which language are you using? Most models are fully
trained on English, not many models are trained on Dutch text. So if you
are using Dutch texts, you may want to look for a model that is trained
on or finetuned for Dutch. Additionally, some LLMs are multimodal
models, meaning they can process various forms of input; text, images,
timeseries, audio, videos and so on.</p>
</section><section><h2 class="section-heading" id="building-a-chatbot">Building a chatbot<a class="anchor" aria-label="anchor" href="#building-a-chatbot"></a>
</h2>
<hr class="half-width">
<p>It is time to start using an LLM! We are not going to train our own
LLM, but use Meta’s open source Llama model to set up a chatbot.</p>
<div class="section level4">
<h4 id="starting-ollama">Starting Ollama<a class="anchor" aria-label="anchor" href="#starting-ollama"></a>
</h4>
<p>Ollama is a platform that allows users to run various LLM locally on
your own computer. This is different from for example using chatgpt,
where you log in and use the online api. ChatGPT collects the input you
are providing and uses this to their own benefit. Running an LLM locally
using Ollama thus preserves your privacy. It also allows you to
customize a model, by setting certain parameters, or even by finetuning
a model.</p>
<p>To start Ollama:</p>
<pre><code>ollama serve</code></pre>
<p>Next, download the large language model to be used. In this case use
the smallest open source llama model, which is llama3.1:8b. Here 3.1 is
the version of the model and 8b stands for the number of parameters that
the model has.</p>
<pre><code>!ollama pull llama3.1:8b</code></pre>
<p>In general, a bigger version of the same model (such as Llama3.1:70b)
is better in accuracy, but since it is larger it takes more resources to
run and can hence be too much for a laptop.</p>
<p>Import the packages that will be used:</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="im">from</span> langchain_ollama <span class="im">import</span> ChatOllama</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a><span class="im">from</span> langchain_core.messages <span class="im">import</span> HumanMessage, SystemMessage</span></code></pre>
</div>
</div>
<div class="section level3">
<h3 id="create-a-model-instance">Create a model instance<a class="anchor" aria-label="anchor" href="#create-a-model-instance"></a>
</h3>
<p>Here, <code>model</code> defines the LLM to be used, which is set to
the model just downloaded, and <code>temperature</code> sets the
randomness of the mode, using the value zero ensures that repeating a
question will give the same model output (answer).</p>
<pre><code><span><span class="va">llm</span> <span class="op">=</span> <span class="fu">ChatOllama</span><span class="op">(</span>model<span class="op">=</span><span class="st">"llama3.1:8b"</span>, temperature<span class="op">=</span><span class="fl">0</span><span class="op">)</span></span></code></pre>
<p>Now that the model is set up, it can be invoked - ask it a
question.</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a>question <span class="op">=</span> <span class="st">"When was the moon landing?"</span></span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>chatresult <span class="op">=</span> llm.invoke([HumanMessage(content<span class="op">=</span>question)])</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a><span class="bu">print</span>(chatresult.content)</span></code></pre>
</div>
<div id="challenge1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div class="callout-inner">
<h3 class="callout-title">Challenge</h3>
<div class="callout-content">
<p>Play around with the chat bot by changing the questions. - How is the
quality of the answers? - Is it able to answer general questions, and
very specific questions? - Which limitations can you identify? - How
could you get better answers?</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">

</div>
</div>
</div>
</div>
<div id="discussion1" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<span class="callout-header">Discussion</span>
<div class="callout-inner">
<h3 class="callout-title">Challenge<em> (continued)</em>
</h3>
<div class="callout-content">
<p>This Llama chat bot, just like ChatGPT, is quite generic. It is good
at answering general questions; things that a lot of people know. Going
deeper and asking very specific questions often leads to vague or
inaccurate results.</p>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="use-context">Use context<a class="anchor" aria-label="anchor" href="#use-context"></a>
</h3>
<p>To improve on what to expect the LLM to return, it is also possible
to provide it with some context. For example, add:</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a>context <span class="op">=</span> <span class="st">"You are a highschool history teacher trying to explain societal impact of historic events."</span></span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>messages <span class="op">=</span> [</span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a>    SystemMessage(content<span class="op">=</span>context),</span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a>    HumanMessage(content<span class="op">=</span>question),</span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a>]</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a>chatresult <span class="op">=</span> llm.invoke(messages)</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a><span class="bu">print</span>(chatresult.content)</span></code></pre>
</div>
<p>The benefit here is that your answer will be phrased in a way that
fits your context, without having to specify this for every
question.</p>
</div>
<div class="section level3">
<h3 id="use-the-chat-history">Use the chat history<a class="anchor" aria-label="anchor" href="#use-the-chat-history"></a>
</h3>
<p>With this chatbot the LLM can be invoked to generate output based on
the provided input and context. However, what is not possible in this
state, is to ask followup questions. This can be useful to refine the
output that it generates. The next step is therefore to implement
message persistence in the workflow.</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="im">from</span> langgraph.checkpoint.memory <span class="im">import</span> MemorySaver</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a><span class="im">from</span> langgraph.graph <span class="im">import</span> START, MessagesState, StateGraph</span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> Image, display</span></code></pre>
</div>
<p>The package LangGraph is a library that is designed to build LLM
agents using workflows represented as graphs. The workflows you create
consist of connected components, which allows you to build multi-step
processes. The workflow graphs can be easily visualised which makes them
quite insightful. LangGraph also has a build-in persistence layer,
exactly what we want right now!</p>
<p>First, define an empty workflow graph with the StateGraph class with
the MessageState schema (a simple schema with messages as only key)</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a>workflow <span class="op">=</span> StateGraph(state_schema<span class="op">=</span>MessagesState)</span></code></pre>
</div>
<p>Then define a function to invoke the llm with a message</p>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a><span class="kw">def</span> call_llm(state: MessagesState):</span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a>    response <span class="op">=</span> llm.invoke(state[<span class="st">"messages"</span>])</span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a>    <span class="cf">return</span> {<span class="st">"messages"</span>: response}</span></code></pre>
</div>
<p>Then add the call_llm function as a node to the graph and connect it
with an edge to the start point of the graph. This start node sends the
user input to the graph, which in this case only contains the LLM
element.</p>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a>workflow.add_node(<span class="st">"LLM"</span>, call_llm)</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a>workflow.add_edge(START, <span class="st">"LLM"</span>)</span></code></pre>
</div>
<p>Initialise a memory that will preserve the messages state in a
dictionary while going though the graph multiple times asking followup
questions.</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a>memory <span class="op">=</span> MemorySaver()</span></code></pre>
</div>
<p>Then compile and visualise the graph with the memory as
checkpoint.</p>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a>graph <span class="op">=</span> workflow.<span class="bu">compile</span>(checkpointer<span class="op">=</span>memory)</span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a>display(Image(graph.get_graph().draw_mermaid_png()))</span></code></pre>
</div>
<p><img src=".././fig/workflow_llm.png" alt="workflow" class="figure"> Define a memory
id for the current conservation.</p>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a>config <span class="op">=</span> {<span class="st">"configurable"</span>: {<span class="st">"thread_id"</span>: <span class="st">"moonconversation"</span>}}</span></code></pre>
</div>
<p>Then call the workflow with memory we created with the original
question</p>
<div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a>question <span class="op">=</span> <span class="st">'Who landed on the Moon?'</span></span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a>messages <span class="op">=</span> [HumanMessage(question)]</span>
<span id="cb15-3"><a href="#cb15-3" tabindex="-1"></a>output <span class="op">=</span> graph.invoke({<span class="st">"messages"</span>: messages}, config)</span>
<span id="cb15-4"><a href="#cb15-4" tabindex="-1"></a></span>
<span id="cb15-5"><a href="#cb15-5" tabindex="-1"></a>output[<span class="st">"messages"</span>][<span class="op">-</span><span class="dv">1</span>].pretty_print()</span></code></pre>
</div>
<p>The question and answer are now saved in the graph state with this
config, and followup questions and answers with the same config will be
added to it.</p>
<p>Everything that is saved can be found in the config state</p>
<div class="codewrapper sourceCode" id="cb16">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a>graph.get_state(config)</span></code></pre>
</div>
<p>The workflow can now be used to ask followup questions without having
to repeat the original question, and based on the previous generated
answer.</p>
<div class="codewrapper sourceCode" id="cb17">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a><span class="co"># Followup</span></span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a>followup <span class="op">=</span> <span class="st">"Shorten the answer to 20 words"</span></span>
<span id="cb17-3"><a href="#cb17-3" tabindex="-1"></a>input_messages <span class="op">=</span> [HumanMessage(followup)]</span>
<span id="cb17-4"><a href="#cb17-4" tabindex="-1"></a>output <span class="op">=</span> graph.invoke({<span class="st">"messages"</span>: input_messages}, config)</span>
<span id="cb17-5"><a href="#cb17-5" tabindex="-1"></a></span>
<span id="cb17-6"><a href="#cb17-6" tabindex="-1"></a><span class="co"># print the last output</span></span>
<span id="cb17-7"><a href="#cb17-7" tabindex="-1"></a>output[<span class="st">"messages"</span>][<span class="op">-</span><span class="dv">1</span>].pretty_print()</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb18">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" tabindex="-1"></a><span class="co"># Followup instruction</span></span>
<span id="cb18-2"><a href="#cb18-2" tabindex="-1"></a>followup2 <span class="op">=</span> <span class="st">"Translate the answer to Dutch"</span></span>
<span id="cb18-3"><a href="#cb18-3" tabindex="-1"></a>input_messages <span class="op">=</span> [HumanMessage(followup2)]</span>
<span id="cb18-4"><a href="#cb18-4" tabindex="-1"></a>output <span class="op">=</span> graph.invoke({<span class="st">"messages"</span>: input_messages}, config)</span>
<span id="cb18-5"><a href="#cb18-5" tabindex="-1"></a></span>
<span id="cb18-6"><a href="#cb18-6" tabindex="-1"></a><span class="co"># print the last output</span></span>
<span id="cb18-7"><a href="#cb18-7" tabindex="-1"></a>output[<span class="st">"messages"</span>][<span class="op">-</span><span class="dv">1</span>].pretty_print()</span></code></pre>
</div>
</div>
</section><section><h2 class="section-heading" id="retrieval-augmented-generation---build-a-rag">Retrieval Augmented Generation - Build a RAG<a class="anchor" aria-label="anchor" href="#retrieval-augmented-generation---build-a-rag"></a>
</h2>
<hr class="half-width">
<p>A chatbot tends to give quite generic answers. A more specific
chatbot can be made by building a Retrieval Augmented Generation agent.
This is an information that you yourself provide with a knowledge base:
a large number of documents. When prompted with a questions, the agent
first retrieves relevant sections of the data that is in the knowledge
base, and then generates and answer based on that data. In this way you
can build an agent with very specific knowledge.</p>
<p>The simplest form of a rag consists of two parts, a retriever and a
generator. The retriever part will collect data from the provided data,
so first a knowledge base has to be created for the retriever.</p>
<p>To generate text in the RAG the trained Llama model will be used,
which works well for English text. Because this model was not trained on
Dutch text, the RAG will work better for an English knowledge base.</p>
<p>Three newspaper pages will be used for the example RAG, these are
pages from a Curacao newspaper. This is a Dutch newspaper with an
additional page in English. The text versions of the newspapers can be
downloaded to only get these specific English pages. Save them in a
folder called “rag_data” for further processing: - <a href="https://www.delpher.nl/nl/kranten/view?query=the+moon&amp;coll=ddd&amp;identifier=ddd:010460545:mpeg21:p012&amp;resultsidentifier=ddd:010460545:mpeg21:a0134&amp;rowid=4" class="external-link">page1</a>
- <a href="https://www.delpher.nl/nl/kranten/view?query=moon+landing&amp;coll=ddd&amp;page=1&amp;facets%5Bspatial%5D%5B%5D=Nederlandse+Antillen&amp;identifier=ddd:010460616:mpeg21:a0146&amp;resultsidentifier=ddd:010460616:mpeg21:a0146&amp;rowid=1" class="external-link">page2</a>
- <a href="https://www.delpher.nl/nl/kranten/view?query=moon+landing&amp;coll=ddd&amp;page=1&amp;facets%5Bspatial%5D%5B%5D=Nederlandse+Antillen&amp;identifier=ddd:010460520:mpeg21:a0167&amp;resultsidentifier=ddd:010460520:mpeg21:a0167&amp;rowid=7" class="external-link">page3</a></p>
<div class="section level4">
<h4 id="the-knowledge-base---a-vector-store">The knowledge base - a vector store<a class="anchor" aria-label="anchor" href="#the-knowledge-base---a-vector-store"></a>
</h4>
<p>Language models all work with vectors - embedded text. Instead of
saving text, a the data has to be stored in embedded versions in a
vector store, where the retriever can shop around for the relevant
text.</p>
<p>There a number of packages to be used in this section to build the
RAG.</p>
<div class="codewrapper sourceCode" id="cb19">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb19-2"><a href="#cb19-2" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> Image, display</span>
<span id="cb19-3"><a href="#cb19-3" tabindex="-1"></a><span class="im">from</span> typing_extensions <span class="im">import</span> List, TypedDict</span>
<span id="cb19-4"><a href="#cb19-4" tabindex="-1"></a></span>
<span id="cb19-5"><a href="#cb19-5" tabindex="-1"></a><span class="im">from</span> langchain_ollama <span class="im">import</span> ChatOllama, OllamaEmbeddings</span>
<span id="cb19-6"><a href="#cb19-6" tabindex="-1"></a><span class="im">from</span> langchain_core.messages <span class="im">import</span> HumanMessage</span>
<span id="cb19-7"><a href="#cb19-7" tabindex="-1"></a><span class="im">from</span> langchain_core.documents <span class="im">import</span> Document</span>
<span id="cb19-8"><a href="#cb19-8" tabindex="-1"></a><span class="im">from</span> langchain_core.vectorstores <span class="im">import</span> InMemoryVectorStore</span>
<span id="cb19-9"><a href="#cb19-9" tabindex="-1"></a><span class="im">from</span> langchain_text_splitters <span class="im">import</span> RecursiveCharacterTextSplitter</span>
<span id="cb19-10"><a href="#cb19-10" tabindex="-1"></a><span class="im">from</span> langgraph.graph <span class="im">import</span> START, StateGraph</span>
<span id="cb19-11"><a href="#cb19-11" tabindex="-1"></a><span class="im">from</span> langchain_nomic.embeddings <span class="im">import</span> NomicEmbeddings</span></code></pre>
</div>
<p>Define the large language model to be used to generate an answer
based on provided context:</p>
<div class="codewrapper sourceCode" id="cb20">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" tabindex="-1"></a>llm <span class="op">=</span> ChatOllama(model<span class="op">=</span><span class="st">"llama3.1:8b"</span>, temperature<span class="op">=</span><span class="dv">0</span>)</span></code></pre>
</div>
<p>Define the embeddings model, this is the model to convert our
knowledge base texts into vector embeddings and will be used for the
retrieval part of the RAG:</p>
<div class="codewrapper sourceCode" id="cb21">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" tabindex="-1"></a>embeddings<span class="op">=</span>NomicEmbeddings(model<span class="op">=</span><span class="st">"nomic-embed-text-v1.5"</span>, inference_mode<span class="op">=</span><span class="st">"local"</span>)</span></code></pre>
</div>
<div id="accordionInstructor1" class="accordion instructor-note accordion-flush">
<div class="accordion-item">
<button class="accordion-button instructor-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseInstructor1" aria-expanded="false" aria-controls="collapseInstructor1">
  <h3 class="accordion-header" id="headingInstructor1">  <div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="edit-2"></i></div> Instructor Note </h3>
</button>
<div id="collapseInstructor1" class="accordion-collapse collapse" aria-labelledby="headingInstructor1" data-bs-parent="#accordionInstructor1">
<div class="accordion-body">
<p>For more on Nomic embeddings see: <a href="https://python.langchain.com/api_reference/nomic/embeddings/langchain_nomic.embeddings.NomicEmbeddings.html" class="external-link uri">https://python.langchain.com/api_reference/nomic/embeddings/langchain_nomic.embeddings.NomicEmbeddings.html</a></p>
<p>using inference_model=“local” uses (Embed4All)[<a href="https://docs.gpt4all.io/old/gpt4all_python_embedding.html" class="external-link uri">https://docs.gpt4all.io/old/gpt4all_python_embedding.html</a>]</p>
</div>
</div>
</div>
</div>
<p>In the text files, the articles are split by ‘—’. This information
can be used to store the individual articles into a list. Store the
filename of the articles in a list as well, so that one can find easily
in from which file a text snippet was taken.</p>
<div class="codewrapper sourceCode" id="cb22">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" tabindex="-1"></a><span class="bu">dir</span> <span class="op">=</span> <span class="st">"./rag_data"</span></span>
<span id="cb22-2"><a href="#cb22-2" tabindex="-1"></a>articles <span class="op">=</span> []</span>
<span id="cb22-3"><a href="#cb22-3" tabindex="-1"></a>metadata <span class="op">=</span> []</span>
<span id="cb22-4"><a href="#cb22-4" tabindex="-1"></a></span>
<span id="cb22-5"><a href="#cb22-5" tabindex="-1"></a><span class="co"># Iterate over files and add individual articles and corresponding filenames to lists</span></span>
<span id="cb22-6"><a href="#cb22-6" tabindex="-1"></a><span class="cf">for</span> <span class="bu">file</span> <span class="kw">in</span> os.listdir(<span class="bu">dir</span>):</span>
<span id="cb22-7"><a href="#cb22-7" tabindex="-1"></a>    file_path <span class="op">=</span> os.path.join(<span class="bu">dir</span>, <span class="bu">file</span>)</span>
<span id="cb22-8"><a href="#cb22-8" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(file_path, <span class="st">"r"</span>) <span class="im">as</span> f:</span>
<span id="cb22-9"><a href="#cb22-9" tabindex="-1"></a>        content <span class="op">=</span> f.read().split(<span class="st">'---'</span>)</span>
<span id="cb22-10"><a href="#cb22-10" tabindex="-1"></a>        articles.extend(content)</span>
<span id="cb22-11"><a href="#cb22-11" tabindex="-1"></a>        metadata.extend([file_path] <span class="op">*</span> <span class="bu">len</span>(content))</span></code></pre>
</div>
<p>The generator will in the end provide an answer based on the text
snippet that is retrieved from the knowledge base. If the fragment is
very long, it may contain a lot of irrelevant information, which will
blur the generated answer. Therefor it is better to split the data into
smaller parts, so that the retriever can collect very specific pieces of
text to generate an answer from. It is useful to keep some overlap
between the splits, so that information does not get lost because of for
example splits in the middle of a sentence.</p>
<div class="codewrapper sourceCode" id="cb23">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" tabindex="-1"></a>text_splitter <span class="op">=</span> RecursiveCharacterTextSplitter(</span>
<span id="cb23-2"><a href="#cb23-2" tabindex="-1"></a>    chunk_size<span class="op">=</span><span class="dv">500</span>, chunk_overlap<span class="op">=</span><span class="dv">50</span></span>
<span id="cb23-3"><a href="#cb23-3" tabindex="-1"></a>)</span>
<span id="cb23-4"><a href="#cb23-4" tabindex="-1"></a></span>
<span id="cb23-5"><a href="#cb23-5" tabindex="-1"></a>documents <span class="op">=</span> text_splitter.create_documents(articles, metadatas<span class="op">=</span>[{<span class="st">'filename'</span>: <span class="bu">file</span>} <span class="cf">for</span> <span class="bu">file</span> <span class="kw">in</span> files])</span>
<span id="cb23-6"><a href="#cb23-6" tabindex="-1"></a></span>
<span id="cb23-7"><a href="#cb23-7" tabindex="-1"></a><span class="bu">print</span>(documents)</span></code></pre>
</div>
<p>This text splitter splits text based on the defined character chunk
size, but also takes into account spaces and newlines to slit in “smart”
chunks, so the chunks will not be exactly of length 500.</p>
<p>Finally, convert each text split into a vector, and save all vectors
in a vector store. The text is converted into embeddings using the
earlier defined embeddings model.</p>
<div class="codewrapper sourceCode" id="cb24">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" tabindex="-1"></a>vectorstore <span class="op">=</span> InMemoryVectorStore.from_texts(</span>
<span id="cb24-2"><a href="#cb24-2" tabindex="-1"></a>    [doc.page_content <span class="cf">for</span> doc <span class="kw">in</span> documents],</span>
<span id="cb24-3"><a href="#cb24-3" tabindex="-1"></a>    embedding<span class="op">=</span>embeddings,</span>
<span id="cb24-4"><a href="#cb24-4" tabindex="-1"></a>)</span></code></pre>
</div>
<p>The contents of the vectorstore can be printed as</p>
<div class="codewrapper sourceCode" id="cb25">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" tabindex="-1"></a><span class="bu">print</span>(vectorstore.store)</span></code></pre>
</div>
<p>It shows that for each text fragment that was given, a vector is
created and it is saved in the vectorstore together with the original
text.</p>
</div>
<div class="section level4">
<h4 id="setting-up-the-retriever-and-generator">Setting up the retriever and generator<a class="anchor" aria-label="anchor" href="#setting-up-the-retriever-and-generator"></a>
</h4>
<p>Define the structure of a dictionary with the keys
<code>question</code>, <code>context</code>, and
<code>answer</code>.</p>
<div class="codewrapper sourceCode" id="cb26">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" tabindex="-1"></a><span class="kw">class</span> State(TypedDict):</span>
<span id="cb26-2"><a href="#cb26-2" tabindex="-1"></a>    question: <span class="bu">str</span></span>
<span id="cb26-3"><a href="#cb26-3" tabindex="-1"></a>    context: List[Document]</span>
<span id="cb26-4"><a href="#cb26-4" tabindex="-1"></a>    answer: <span class="bu">str</span></span></code></pre>
</div>
<p>Define the retriever function of the RAG. It takes in the question
and does a similarity search in the created vectorstore and returns the
text snippets that were found to be similar. The similarity search
converts the question into an embeddings vector and uses the cosine
similarity to determine the similarity between the question and
snippets. It then returns the top 4 snippets with the highest cosine
similarity score. The snippets are returned in the original text form,
i.e. the retrieved vectors are transformed back into text.</p>
<div class="codewrapper sourceCode" id="cb27">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" tabindex="-1"></a><span class="kw">def</span> retrieve(state: State):</span>
<span id="cb27-2"><a href="#cb27-2" tabindex="-1"></a>    <span class="co">"Retrieve documents that are similar to the question."</span></span>
<span id="cb27-3"><a href="#cb27-3" tabindex="-1"></a>    retrieved_docs <span class="op">=</span> vectorstore.similarity_search(state[<span class="st">"question"</span>], k<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb27-4"><a href="#cb27-4" tabindex="-1"></a>    <span class="cf">return</span> {<span class="st">"context"</span>: retrieved_docs}</span></code></pre>
</div>
<p>Define the generator function of the RAG. In this function a prompt
is defined for the RAG using the context and question. The large
language model (the Llama model, defined above) is then invoked with
this question and generates an answer for the provided prompt, which is
returned as the answer key of the dictionary.</p>
<div class="codewrapper sourceCode" id="cb28">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" tabindex="-1"></a><span class="kw">def</span> generate(state: State):</span>
<span id="cb28-2"><a href="#cb28-2" tabindex="-1"></a>    docs_content <span class="op">=</span> <span class="st">"</span><span class="ch">\n\n</span><span class="st">"</span>.join(doc.page_content <span class="cf">for</span> doc <span class="kw">in</span> state[<span class="st">"context"</span>])</span>
<span id="cb28-3"><a href="#cb28-3" tabindex="-1"></a>    </span>
<span id="cb28-4"><a href="#cb28-4" tabindex="-1"></a>    rag_prompt <span class="op">=</span> <span class="st">"""You are an assistant for question-answering tasks.</span></span>
<span id="cb28-5"><a href="#cb28-5" tabindex="-1"></a><span class="st">    Here is the context to use to answer the question:</span></span>
<span id="cb28-6"><a href="#cb28-6" tabindex="-1"></a><span class="st">    </span><span class="sc">{context}</span></span>
<span id="cb28-7"><a href="#cb28-7" tabindex="-1"></a><span class="st">    Think carefully about the above context.</span></span>
<span id="cb28-8"><a href="#cb28-8" tabindex="-1"></a><span class="st">    Now, review the user question:</span></span>
<span id="cb28-9"><a href="#cb28-9" tabindex="-1"></a><span class="st">    </span><span class="sc">{question}</span></span>
<span id="cb28-10"><a href="#cb28-10" tabindex="-1"></a><span class="st">    Provide an answer to this questions using only the above context.</span></span>
<span id="cb28-11"><a href="#cb28-11" tabindex="-1"></a><span class="st">    Use 10 sentences maximum and keep the answer concise.</span></span>
<span id="cb28-12"><a href="#cb28-12" tabindex="-1"></a><span class="st">    Answer:"""</span></span>
<span id="cb28-13"><a href="#cb28-13" tabindex="-1"></a></span>
<span id="cb28-14"><a href="#cb28-14" tabindex="-1"></a>    rag_prompt_formatted <span class="op">=</span> rag_prompt.<span class="bu">format</span>(context<span class="op">=</span>docs_content, question<span class="op">=</span>State[<span class="st">"question"</span>])</span>
<span id="cb28-15"><a href="#cb28-15" tabindex="-1"></a>    </span>
<span id="cb28-16"><a href="#cb28-16" tabindex="-1"></a>    generate <span class="op">=</span> llm.invoke([HumanMessage(content<span class="op">=</span>rag_prompt_formatted)])</span>
<span id="cb28-17"><a href="#cb28-17" tabindex="-1"></a>    <span class="cf">return</span> {<span class="st">"answer"</span>: generate.content}</span></code></pre>
</div>
</div>
<div class="section level4">
<h4 id="build-the-workflow">Build the workflow<a class="anchor" aria-label="anchor" href="#build-the-workflow"></a>
</h4>
<p>The retriever and generator are combined into a workflow graph. The
workflow is defined as a StateGraph that uses the dictionary structure
(with the keys <code>question</code>, <code>context</code>, and
<code>answer</code>) defined above. The retriever and generator are
added as nodes, and the two are connected via the edge. The retrieve is
set as the start point of the workflow, and finally the graph is
compiled into an executable.</p>
<pre><code><span><span class="va">workflow</span> <span class="op">=</span> <span class="fu">StateGraph</span><span class="op">(</span><span class="va">State</span><span class="op">)</span></span>
<span><span class="fu">workflow.add_node</span><span class="op">(</span><span class="st">"retrieve"</span>, <span class="va">retrieve</span><span class="op">)</span></span>
<span><span class="fu">workflow.add_node</span><span class="op">(</span><span class="st">"generate"</span>, <span class="va">generate</span><span class="op">)</span></span>
<span><span class="fu">workflow.add_edge</span><span class="op">(</span><span class="st">"retrieve"</span>, <span class="st">"generate"</span><span class="op">)</span></span>
<span><span class="fu">workflow.set_entry_point</span><span class="op">(</span><span class="st">"retrieve"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">graph</span> <span class="op">=</span> <span class="fu">workflow.compile</span><span class="op">(</span><span class="op">)</span></span></code></pre>
<div class="codewrapper sourceCode" id="cb30">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" tabindex="-1"></a>display(Image(graph.get_graph().draw_mermaid_png()))https:<span class="op">//</span>scikit<span class="op">-</span>learn.org<span class="op">/</span><span class="fl">1.5</span><span class="op">/</span>modules<span class="op">/</span>grid_search.html</span></code></pre>
</div>
<figure><img src=".././fig/workflow_rag.png" alt="workflow" class="figure mx-auto d-block"><div class="figcaption">workflow</div>
</figure><p>That’s it! The RAG can now be asked questions. Let’s see what it can
tell about the moon landing:</p>
<div class="codewrapper sourceCode" id="cb31">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" tabindex="-1"></a>response <span class="op">=</span> graph.invoke({<span class="st">"question"</span>: <span class="st">"Who landed on the Moon?"</span>})</span>
<span id="cb31-2"><a href="#cb31-2" tabindex="-1"></a><span class="bu">print</span>(response[<span class="st">"answer"</span>])</span></code></pre>
</div>
<p>This is quite a specific answer. It can be seen why by looking at the
text snippets that were used:</p>
<pre><code><span><span class="fu">print</span><span class="op">(</span><span class="va">response</span><span class="op">[</span><span class="st">"context"</span><span class="op">]</span><span class="op">)</span></span></code></pre>
<p>While a general chatbot uses all the information in the material that
it was trained on, the RAG only uses the information that was stored in
the vectorstore to generate the answer.</p>
<div id="challenge2" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div class="callout-inner">
<h3 class="callout-title">Challenge</h3>
<div class="callout-content">
<p>Try generating more answers with the RAG based on other questions,
perhaps also looking at the newspaper texts that are used. What stands
out?</p>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2"> Show me the solution </h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" aria-labelledby="headingSolution2" data-bs-parent="#accordionSolution2">
<div class="accordion-body">
<p>For example: - The RAG returns in some cases that no answer can be
generated on the context it was provided with - For some questions, the
LLM returns that it cannot provide an answer because of safety
precautions that are inherent to the LLM used, such as information about
violent acts.</p>
</div>
</div>
</div>
</div>
<p>This is the simplest form of a RAG, with a retriever and a generator.
However, one can make the RAG more complex by adding more components and
options to the workflow, for example one to check the relevance of the
retrieved documents, removing those that turn out to be irrelevant to be
used for the answer generation, or having a component that can
reformulate the question. Another example is to add a hallucination
checker step after the generator that checks if the generated answer can
actually be found in the provided context.</p>
</div>
</section><div class="section level1">
<h1 id="pitfalls-limitations-privacy">Pitfalls, limitations, privacy<a class="anchor" aria-label="anchor" href="#pitfalls-limitations-privacy"></a>
</h1>
<p>While LLMs are very powerful and provide us with many great
possibilities and opportunities, they also have limitations.</p>
<ul>
<li>Training data: LLMs are trained on large data sets. These are often
collected from the internet and books, but these come with downsides
<ul>
<li>They have a date cutoff; LLMs are trained on a static data set,
meaning they are trained on information up to a certain date. They
therefor do not have the latest information. They are definitely not
useful for recent news events (often they mention this), but also lack
behind in for example technological advancements. Always consider how
old the model is that you are using.s</li>
<li>There is no fact checking involved in the training data. If the
training data contains a lot of incorrect information or fake news, this
will affect the answers generated.</li>
<li>The training data is often biased, including social and cultural
biases. This can lead to harmful responses that are stereotyping or
racist. Training LLMs does involve a human review for fine-tuning the
models, such that they are prevented from answering questions on illegal
activities, political advice, advice on harming yourself or others, or
generating violent or sexual content. When for example prompted with
questions about politics it does provide generic factual information,
but will also say that it will not give advice or opinionated
answers.</li>
<li>For GPT-4, there is no exact information provided as to which data
it is trained on, meaning that the data might be breaking privacy laws
or copyright infringement.</li>
<li>The data an LLM is trained on is generic, resulting in that it is
not good at generating answers for specialised questions. There are
however already a lot of models that are finetuned for specific
fields.</li>
<li>Language: LLMs are primarily trained on data collected from the
internet, resulting in that they are ‘best’ in the most spoken
languages. ChatGPT is trained on many languages, but languages that are
less widely spoken will automatically have smaller data to train on,
which makes the LLM less accurate in these languages.</li>
</ul>
</li>
<li>Multi-step thinking: LLMs are generally not good at multi-step
thinking. They are very good at providing bullet point lists of
information, but reasoning like humans do, drawing a conclusion from
combined logic is something they are not good at (yet).</li>
<li>Hallucinations: LLMs tend to hallucinate. When it ‘does not know the
answer’, it will often still try to provide an answer. You should
therefore not blindly use the answers from an LLM, but still check the
given information yourself.</li>
<li>Privacy: when using a language model locally, such as done above
with Llama, your privacy is preserved. The model is only on your laptop,
and the data you provide is not uploaded to any server. But when you for
example use ChatGPT via the web interface, there is no privacy. Any
information you provide, questions, provided context and so on will be
used by ChatGPT. It will be used (ao) for improving the model, which may
be considered a good thing, but other things it is used for are not
necessarily known. This means that you should be careful in what you
provide to the LLM. Never provide sensitive information or private data
to an LLM that you do not run fully locally.</li>
</ul>
<div id="key-points-to-remember" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<span class="callout-header">Key Points</span>
<div id="key-points-to-remember" class="callout-inner">
<h3 class="callout-title">Key points to remember</h3>
<div class="callout-content">
<ul>
<li>
<em>Transformer models power LLMs</em>: The Transformer architecture
and its self-attention mechanism allow LLMs to handle long-range
dependencies in text effectively.</li>
<li>
<em>LLMs excel at multiple tasks</em>: From text generation to
classification and summarisation, LLMs like Llama are versatile and
perform well on various NLP tasks.</li>
<li>
<em>Prompt engineering is crucial</em>: Designing effective prompts
can significantly improve the quality of outputs from LLMs, especially
for creative tasks like text generation and translation.</li>
<li>
<em>Real-world use cases</em>: LLMs can be applied to real-world
problems like news classification, summarisation, and headline
generation, improving efficiency in content management and
delivery.</li>
</ul>
</div>
</div>
</div>
</div></section>
</div>
    </main>
</div>
<!-- END  : inst/pkgdown/templates/content-extra.html -->

      </div>
<!--/div.row-->
      		<footer class="row footer mx-md-3"><hr>
<div class="col-md-6">
        <p>This lesson is subject to the <a href="CODE_OF_CONDUCT.html">Code of Conduct</a></p>
        <p>

        <a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/edit/main/README.md" class="external-link">Edit on GitHub</a>

	
        | <a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/blob/main/CONTRIBUTING.md" class="external-link">Contributing</a>
        | <a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/" class="external-link">Source</a></p>
				<p><a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/blob/main/CITATION.cff" class="external-link">Cite</a> | <a href="mailto:team@carpentries.org">Contact</a> | <a href="https://carpentries.org/about/" class="external-link">About</a></p>
			</div>
			<div class="col-md-6">

        <p>Materials licensed under <a href="LICENSE.html">CC-BY 4.0</a> by the authors</p>

        <p>Template licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/" class="external-link">CC-BY 4.0</a> by <a href="https://carpentries.org/" class="external-link">The Carpentries</a></p>
        <p>Built with <a href="https://github.com/carpentries/sandpaper/tree/0.17.1" class="external-link">sandpaper (0.17.1)</a>, <a href="https://github.com/carpentries/pegboard/tree/0.7.9" class="external-link">pegboard (0.7.9)</a>, and <a href="https://github.com/carpentries/varnish/tree/1.0.7" class="external-link">varnish (1.0.7)</a></p>
			</div>
		</footer>
</div> <!-- / div.container -->
	<div id="to-top">
		<a href="#top">
      <i class="search-icon" data-feather="arrow-up" role="img" aria-label="Back To Top"></i><br><!-- <span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top --><span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top
		</a>
	</div>
  <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "LearningResource",
  "@id": "https://esciencecenter-digital-skills.github.io/Natural-language-processing/instructor/aio.html",
  "inLanguage": "en",
  "dct:conformsTo": "https://bioschemas.org/profiles/LearningResource/1.0-RELEASE",
  "description": "A Carpentries Lesson teaching foundational data and coding skills to researchers worldwide",
  "keywords": "software, data, lesson, The Carpentries, NLP, English, social sciences, pre-alpha",
  "name": "All in One View",
  "creativeWorkStatus": "active",
  "url": "https://esciencecenter-digital-skills.github.io/Natural-language-processing/instructor/aio.html",
  "identifier": "https://esciencecenter-digital-skills.github.io/Natural-language-processing/instructor/aio.html",
  "dateCreated": "2024-04-24",
  "dateModified": "2025-09-23",
  "datePublished": "2025-09-23"
}

  </script><script>
		feather.replace();
	</script>
</body>
</html><!-- END:   inst/pkgdown/templates/layout.html-->

