<!DOCTYPE html>
<!-- START: inst/pkgdown/templates/layout.html --><!-- Generated by pkgdown: do not edit by hand --><html lang="en" data-bs-theme="auto">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<title>Fundamentals of Natural Language Processing (NLP) in Python: All in One View</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<script src="assets/themetoggle.js"></script><link rel="stylesheet" type="text/css" href="assets/styles.css">
<script src="assets/scripts.js" type="text/javascript"></script><!-- mathjax --><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      config: ["MMLorHTML.js"],
      jax: ["input/TeX","input/MathML","output/HTML-CSS","output/NativeMML", "output/PreviewHTML"],
      extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
      TeX: {
        extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
      },
      tex2jax: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [ ['$$','$$'], ['\\[', '\\]'] ],
        processEscapes: true
      }
    });
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><!-- Responsive Favicon for The Carpentries --><link rel="apple-touch-icon" sizes="180x180" href="favicons/incubator/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicons/incubator/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicons/incubator/favicon-16x16.png">
<link rel="manifest" href="favicons/incubator/site.webmanifest">
<link rel="mask-icon" href="favicons/incubator/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" media="(prefers-color-scheme: light)" content="white">
<meta name="theme-color" media="(prefers-color-scheme: dark)" content="black">
</head>
<body>
    <header id="top" class="navbar navbar-expand-md top-nav incubator"><svg xmlns="http://www.w3.org/2000/svg" class="d-none"><symbol id="check2" viewbox="0 0 16 16"><path d="M13.854 3.646a.5.5 0 0 1 0 .708l-7 7a.5.5 0 0 1-.708 0l-3.5-3.5a.5.5 0 1 1 .708-.708L6.5 10.293l6.646-6.647a.5.5 0 0 1 .708 0z"></path></symbol><symbol id="circle-half" viewbox="0 0 16 16"><path d="M8 15A7 7 0 1 0 8 1v14zm0 1A8 8 0 1 1 8 0a8 8 0 0 1 0 16z"></path></symbol><symbol id="moon-stars-fill" viewbox="0 0 16 16"><path d="M6 .278a.768.768 0 0 1 .08.858 7.208 7.208 0 0 0-.878 3.46c0 4.021 3.278 7.277 7.318 7.277.527 0 1.04-.055 1.533-.16a.787.787 0 0 1 .81.316.733.733 0 0 1-.031.893A8.349 8.349 0 0 1 8.344 16C3.734 16 0 12.286 0 7.71 0 4.266 2.114 1.312 5.124.06A.752.752 0 0 1 6 .278z"></path><path d="M10.794 3.148a.217.217 0 0 1 .412 0l.387 1.162c.173.518.579.924 1.097 1.097l1.162.387a.217.217 0 0 1 0 .412l-1.162.387a1.734 1.734 0 0 0-1.097 1.097l-.387 1.162a.217.217 0 0 1-.412 0l-.387-1.162A1.734 1.734 0 0 0 9.31 6.593l-1.162-.387a.217.217 0 0 1 0-.412l1.162-.387a1.734 1.734 0 0 0 1.097-1.097l.387-1.162zM13.863.099a.145.145 0 0 1 .274 0l.258.774c.115.346.386.617.732.732l.774.258a.145.145 0 0 1 0 .274l-.774.258a1.156 1.156 0 0 0-.732.732l-.258.774a.145.145 0 0 1-.274 0l-.258-.774a1.156 1.156 0 0 0-.732-.732l-.774-.258a.145.145 0 0 1 0-.274l.774-.258c.346-.115.617-.386.732-.732L13.863.1z"></path></symbol><symbol id="sun-fill" viewbox="0 0 16 16"><path d="M8 12a4 4 0 1 0 0-8 4 4 0 0 0 0 8zM8 0a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 0zm0 13a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 13zm8-5a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2a.5.5 0 0 1 .5.5zM3 8a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2A.5.5 0 0 1 3 8zm10.657-5.657a.5.5 0 0 1 0 .707l-1.414 1.415a.5.5 0 1 1-.707-.708l1.414-1.414a.5.5 0 0 1 .707 0zm-9.193 9.193a.5.5 0 0 1 0 .707L3.05 13.657a.5.5 0 0 1-.707-.707l1.414-1.414a.5.5 0 0 1 .707 0zm9.193 2.121a.5.5 0 0 1-.707 0l-1.414-1.414a.5.5 0 0 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .707zM4.464 4.465a.5.5 0 0 1-.707 0L2.343 3.05a.5.5 0 1 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .708z"></path></symbol></svg><a class="visually-hidden-focusable skip-link" href="#main-content">Skip to main content</a>
  <div class="container-fluid top-nav-container">
    <div class="col-md-8">
      <div class="large-logo">
        <img id="incubator-logo" alt="Carpentries Incubator" src="assets/images/incubator-logo.svg"><span class="badge text-bg-danger">
          <abbr title="This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught.">
            <a href="https://docs.carpentries.org/resources/curriculum/lesson-life-cycle.html" class="external-link alert-link">
              <i aria-hidden="true" class="icon" data-feather="alert-octagon" style="border-radius: 5px"></i>
              Pre-Alpha
            </a>
            <span class="visually-hidden">This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught.</span>
          </abbr>
        </span>

      </div>
    </div>
    <div class="selector-container">
      <div id="theme-selector">
        <li class="nav-item dropdown" id="theme-button-list">
          <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
            <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="bd-theme-text">
<li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                Light
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                Dark
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                Auto
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
          </ul>
</li>
      </div>

      <div class="dropdown" id="instructor-dropdown">
        <button class="btn btn-secondary dropdown-toggle bordered-button" type="button" id="dropdownMenu1" data-bs-toggle="dropdown" aria-expanded="false">
          <i aria-hidden="true" class="icon" data-feather="eye"></i> Learner View <i data-feather="chevron-down"></i>
        </button>
        <ul class="dropdown-menu" aria-labelledby="dropdownMenu1">
<li><button class="dropdown-item" type="button" onclick="window.location.href='instructor/aio.html';">Instructor View</button></li>
        </ul>
</div>
    </div>
  </div>
  <hr></header><nav class="navbar navbar-expand-xl bottom-nav incubator" aria-label="Main Navigation"><div class="container-fluid nav-container">
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle Navigation">
      <span class="navbar-toggler-icon"></span>
      <span class="menu-title">Menu</span>
    </button>
    <div class="nav-logo">
      <img class="small-logo" alt="Carpentries Incubator" src="assets/images/incubator-logo-sm.svg">
</div>
    <div class="lesson-title-md">
      Fundamentals of Natural Language Processing (NLP) in Python
    </div>
    <div class="search-icon-sm">
      <!-- TODO: do not show until we have search
        <i role="img" aria-label="Search the All In One page" data-feather="search"></i>
      -->
    </div>
    <div class="desktop-nav">
      <ul class="navbar-nav me-auto mb-2 mb-lg-0">
<li class="nav-item">
          <span class="lesson-title">
            Fundamentals of Natural Language Processing (NLP) in Python
          </span>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="key-points.html">Key Points</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="reference.html#glossary">Glossary</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="profiles.html">Learner Profiles</a>
        </li>
        <li class="nav-item dropdown">
          <button class="nav-link dropdown-toggle" id="navbarDropdown" data-bs-toggle="dropdown" aria-expanded="false">
            More <i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu" aria-labelledby="navbarDropdown"></ul>
</li>
      </ul>
</div>
    <!--
    <form class="d-flex col-md-2 search-form">
      <fieldset disabled>
      <input class="form-control me-2 searchbox" type="search" placeholder="" aria-label="">
        <button class="btn btn-outline-success tablet-search-button"  type="submit">
          <i class="search-icon" data-feather="search" role="img" aria-label="Search the All In One page"></i>
        </button>
      </fieldset>
    </form>
    -->
    <a id="search-button" class="btn btn-primary" href="aio.html" role="button" aria-label="Search the All In One page">Search the All In One page</a>
  </div>
<!--/div.container-fluid -->
</nav><div class="col-md-12 mobile-title">
  Fundamentals of Natural Language Processing (NLP) in Python
</div>

<aside class="col-md-12 lesson-progress"><div style="width: %" class="percentage">
    %
  </div>
  <div class="progress incubator">
    <div class="progress-bar incubator" role="progressbar" style="width: %" aria-valuenow="" aria-label="Lesson Progress" aria-valuemin="0" aria-valuemax="100">
    </div>
  </div>
</aside><div class="container">
      <div class="row">
        <!-- START: inst/pkgdown/templates/navbar.html -->
<div id="sidebar-col" class="col-lg-4">
  <div id="sidebar" class="sidebar">
      <nav aria-labelledby="flush-headingEleven"><button role="button" aria-label="close menu" alt="close menu" aria-expanded="true" aria-controls="sidebar" class="collapse-toggle" data-collapse="Collapse " data-episodes="Episodes ">
          <i class="search-icon" data-feather="x" role="img"></i>
        </button>
        <div class="sidebar-inner">
          <div class="row mobile-row" id="theme-row-mobile">
            <div class="col" id="theme-selector">
              <li class="nav-item dropdown" id="theme-button-list">
                <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
                  <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><span class="d-lg-none ms-1" id="bd-theme-text">Toggle Theme</span>
                </button>
                <ul class="dropdown-menu dropdown-menu-right" aria-labelledby="bd-theme-text">
<li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                      Light
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                      Dark
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                      <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                      Auto
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                </ul>
</li>
            </div>
          </div>
          <div class="row mobile-row">
            <div class="col">
              <div class="sidenav-view-selector">
                <div class="accordion accordion-flush" id="accordionFlush9">
                  <div class="accordion-item">
                    <h2 class="accordion-header" id="flush-headingNine">
                      <button class="accordion-button collapsed" id="instructor" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseNine" aria-expanded="false" aria-controls="flush-collapseNine">
                        <i id="eye" aria-hidden="true" class="icon" data-feather="eye"></i> Learner View
                      </button>
                    </h2>
                    <div id="flush-collapseNine" class="accordion-collapse collapse" aria-labelledby="flush-headingNine" data-bs-parent="#accordionFlush2">
                      <div class="accordion-body">
                        <a href="instructor/aio.html">Instructor View</a>
                      </div>
                    </div>
                  </div>
<!--/div.accordion-item-->
                </div>
<!--/div.accordion-flush-->
              </div>
<!--div.sidenav-view-selector -->
            </div>
<!--/div.col -->

            <hr>
</div>
<!--/div.mobile-row -->

          <div class="accordion accordion-flush" id="accordionFlush11">
            <div class="accordion-item">

              <button id="chapters" class="accordion-button show" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseEleven" aria-expanded="false" aria-controls="flush-collapseEleven">
                <h2 class="accordion-header chapters" id="flush-headingEleven">
                  EPISODES
                </h2>
              </button>
              <div id="flush-collapseEleven" class="accordion-collapse show collapse" aria-labelledby="flush-headingEleven" data-bs-parent="#accordionFlush11">

                <div class="accordion-body">
                  <div class="accordion accordion-flush" id="accordionFlush1">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading1">
        <a href="index.html">Summary and Setup</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush2">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading2">
        <a href="01-introduction.html">1. Introduction</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush3">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading3">
        <a href="02-word_representations.html">2. From words to vectors</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush4">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading4">
        <a href="03-transformers.html">3. Transformers: BERT and Beyond</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush5">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading5">
        <a href="04-LargeLanguageModels.html">4. Using large language models</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

                </div>
              </div>
            </div>

            <hr class="half-width">
<div class="accordion accordion-flush lesson-resources" id="accordionFlush12">
              <div class="accordion-item">
                <h2 class="accordion-header" id="flush-headingTwelve">
                  <button class="accordion-button collapsed" id="lesson-resources" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseTwelve" aria-expanded="false" aria-controls="flush-collapseTwelve">
                    RESOURCES
                  </button>
                </h2>
                <div id="flush-collapseTwelve" class="accordion-collapse collapse" aria-labelledby="flush-headingTwelve" data-bs-parent="#accordionFlush12">
                  <div class="accordion-body">
                    <ul>
<li>
                        <a href="key-points.html">Key Points</a>
                      </li>
                      <li>
                        <a href="reference.html#glossary">Glossary</a>
                      </li>
                      <li>
                        <a href="profiles.html">Learner Profiles</a>
                      </li>

                    </ul>
</div>
                </div>
              </div>
            </div>
            <hr class="half-width lesson-resources">
<a href="aio.html">See all in one page</a>


            <hr class="d-none d-sm-block d-md-none">
<div class="d-grid gap-1">

            </div>
          </div>
<!-- /div.accordion -->
        </div>
<!-- /div.sidebar-inner -->
      </nav>
</div>
<!-- /div.sidebar -->
  </div>
<!-- /div.sidebar-col -->
<!-- END:   inst/pkgdown/templates/navbar.html-->

        <!-- START: inst/pkgdown/templates/content-extra.html -->
  <div class="col-xl-8 col-lg-12 primary-content">
    <main id="main-content" class="main-content"><div class="container lesson-content">
        
        
<section id="aio-01-introduction"><p>Content from <a href="01-introduction.html">Introduction</a></p>
<hr>
<p>Last updated on 2025-11-27 |

        <a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/edit/main/episodes/01-introduction.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What is Natural Language Processing?</li>
<li>What are some common applications of NLP?</li>
<li>What makes text different from other data?</li>
<li>Why not just learn Large Language Models?</li>
<li>What linguistic properties should we consider when dealing with
texts?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Define Natural Language Processing</li>
<li>Show the most relevant NLP tasks and applications in practice</li>
<li>Learn how to handle Linguistic Data and how is Linguistics relevant
to NLP</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="what-is-nlp">What is NLP?<a class="anchor" aria-label="anchor" href="#what-is-nlp"></a>
</h2>
<hr class="half-width">
<p>Natural language processing (NLP) is an area of research and
application that focuses on making human languages processable for
computers, so that they can perform useful tasks. It is therefore not a
single method, but a collection of techniques that help us deal with
linguistic inputs. The range of techniques spans simple word counts, to
Machine Learning (ML) methods, all the way up to complex Deep Learning
(DL) architectures.</p>
<p>We use the term “natural language”, as opposed to “artificial
language” such as programming languages, which are by design constructed
to be easily formalized into machine-readable instructions. In contrast
to programming languages, natural languages are complex, ambiguous, and
heavily context-dependent, making them challenging for computers to
process. To complicate matters, there is not only a single <em>human
language</em>. More than 7000 languages are spoken around the world,
each with its own grammar, vocabulary, and cultural context.</p>
<p>In this course we will mainly focus on written language, specifically
written English, we leave out audio and speech, as they require a
different kind of input processing. But consider that we use English
only as a convenience so we can address the technical aspects of
processing textual data. While ideally most of the concepts from NLP
apply to most languages, one should always be aware that certain
languages require different approaches to solve seemingly similar
problems. We would like to encourage the usage of NLP in other less
widely known languages, especially if it is a minority language. You can
read more about this topic in this <a href="https://www.ruder.io/nlp-beyond-english/" class="external-link">blogpost</a>.</p>
<div id="nlp-in-the-real-world" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="nlp-in-the-real-world" class="callout-inner">
<h3 class="callout-title">NLP in the real world</h3>
<div class="callout-content">
<p>Name three to five tools/products that you use on a daily basis and
that you think leverage NLP techniques. To do this exercise you may make
use of the Web.</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<p>These are some of the most popular NLP-based products that we use on
a daily basis:</p>
<ul>
<li>Agentic Chatbots (ChatGPT, Perplexity)</li>
<li>Voice-based assistants (e.g., Alexa, Siri, Cortana)</li>
<li>Machine translation (e.g., Google translate, DeepL, Amazon
translate)</li>
<li>Search engines (e.g., Google, Bing, DuckDuckGo)</li>
<li>Keyboard autocompletion on smartphones</li>
<li>Spam filtering</li>
<li>Spell and grammar checking apps</li>
<li>Customer care chatbots</li>
<li>Text summarization tools (e.g., news aggregators)</li>
<li>Sentiment analysis tools (e.g., social media monitoring)</li>
</ul>
</div>
</div>
</div>
</div>
<p>We can already find differences between languages in the most basic
step for processing text. Take the problem of segmenting text into
meaningful units, most of the times these units are words, in NLP we
call this task <strong>tokenization</strong>. A naive approach is to
obtain individual words by splitting text by spaces, as it seems obvious
that we always separate words with spaces. Just as human beings break up
sentences into words, phrases and other units in order to learn about
grammar and other structures of a language, NLP techniques achieve a
similar goal through tokenization. Let’s see how can we segment or
<strong>tokenize</strong> a sentence in English:</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a>english_sentence <span class="op">=</span> <span class="st">"Tokenization isn't always trivial."</span></span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a>english_words <span class="op">=</span> english_sentence.split(<span class="st">" "</span>)</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="bu">print</span>(english_words)</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(english_words))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>['Tokenization', "isn't", 'always', 'trivial.']
4</code></pre>
</div>
<p>The words are mostly well separated, however we do not get fully
formed words (we have punctuation with the period after “trivial” and
also special cases such as the abbreviation of “is not” into “isn’t”).
But at least we get a rough count of the number of words present in the
sentence. Let’s now look at the same example in Chinese:</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="co"># Chinese Translation of "Tokenization is not always trivial"</span></span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>chinese_sentence <span class="op">=</span> <span class="st">"标记化并不总是那么简单"</span> </span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a>chinese_words <span class="op">=</span> chinese_sentence.split(<span class="st">" "</span>)</span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a><span class="bu">print</span>(chinese_words)</span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(chinese_words))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>['标记化并不总是那么简单']
1</code></pre>
</div>
<p>The same example however did not work in Chinese, because Chinese
does not use spaces to separate words. This is an example of how the
idiosyncrasies of human language affects how we can process them with
computers. We therefore need to use a tokenizer specifically designed
for Chinese to obtain the list of well-formed words in the text. Here we
use a “pre-trained” tokenizer called <strong>MicroTokenizer</strong>,
which uses a dictionary-based approach to correctly identify the
distinct words:</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="im">import</span> MicroTokenizer  <span class="co"># A popular Chinese text segmentation library</span></span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>chinese_sentence <span class="op">=</span> <span class="st">"标记化并不总是那么简单"</span></span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a>chinese_words <span class="op">=</span> MicroTokenizer.cut(chinese_sentence)</span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a><span class="bu">print</span>(chinese_words)</span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a><span class="co"># ['mark', 'transform', 'and', 'no', 'always', 'so', 'simple']</span></span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(chinese_words))  <span class="co"># Output: 7</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>['标记', '化', '并', '不', '总是', '那么', '简单']
7</code></pre>
</div>
<p>We can trust that the output is valid because we are using a verified
library - jieba, even though we don’t speak Chinese. Another interesting
aspect is that the Chinese sentence has more words than the English one,
even though they convey the same meaning. This shows the complexity of
dealing with more than one language at a time, as is the case in task
such as <strong>Machine Translation</strong> (using computers to
translate speech or text from one human language to another).</p>
<p>Natural Language Processing deals with the challenges of correctly
processing and generating text in any language. This can be as simple as
counting word frequencies to detect different writing styles, using
statistical methods to classify texts into different categories, or
using <strong>deep neural networks</strong> to generate human-like text
by exploiting word co-occurrences in large amounts of texts.</p>
</section><section><h2 class="section-heading" id="why-should-we-learn-nlp-fundamentals">Why should we learn NLP Fundamentals?<a class="anchor" aria-label="anchor" href="#why-should-we-learn-nlp-fundamentals"></a>
</h2>
<hr class="half-width">
<p>In the past decade, NLP has evolved significantly, especially in the
field of deep learning, to the point that it has become embedded in our
daily lives, one just needs to look at the term Large Language Models
(LLMs), the latest generation of NLP models, which is now ubiquitous in
news media and tech products we use on a daily basis.</p>
<p>The term LLM now is often (and wrongly) used as a synonym of
Artificial Intelligence. We could therefore think that today we just
need to learn how to manipulate LLMs in order to fulfill our research
goals involving textual data. The truth is that Language Modeling has
always been part of the core tasks of NLP, therefore, by learning NLP
you will understand better where are the main ideas behind LLMs coming
from.</p>
<figure><img src="fig/intro0_cs_nlp.png" alt="NLP is an interdisciplinary field, and LLMs are just a subset of it" class="figure mx-auto d-block"><div class="figcaption">NLP is an interdisciplinary field, and LLMs are
just a subset of it</div>
</figure><p>LLM is a blanket term for an assembly of large neural networks that
are trained on vast amounts of text data with the objective of
optimizing for language modeling. Once they are trained, they are used
to generate human-like text or fine-tunned to perform much more advanced
tasks. Indeed, the surprising and fascinating properties that emerge
from training models at this scale allows us to solve different complex
tasks such as answer elaborate questions, translate languages, solve
complex problems, generate narratives that emulate reasoning, and many
more, all of this with a single tool.</p>
<p>It is important, however, to pay attention to what is happening
behind the scenes in order to be able <strong>trace sources of errors
and biases</strong> that get hidden in the complexity of these models.
The purpose of this course is precisely to take a step back, and
understand that:</p>
<ul>
<li>There are a wide variety of tools available, beyond LLMs, that do
not require so much computing power</li>
<li>Sometimes a much simpler method than an LLM is available that can
solve our problem at hand</li>
<li>If we learn how previous approaches to solve linguistic problems
were designed, we can better understand the limitations of LLMs and how
to use them effectively</li>
<li>LLMs excel at confidently delivering information, without any
regards for correctness. This calls for a careful design of
<strong>evaluation metrics</strong> that give us a better understanding
of the quality of the generated content.</li>
</ul>
<p>Let’s go back to our problem of segmenting text and see what ChatGPT
has to say about tokenizing Chinese text:</p>
<figure><img src="fig/intro1.png" alt="ChatGPT Just Works! Does it…?" class="figure mx-auto d-block"><div class="figcaption">ChatGPT Just Works! Does it…?</div>
</figure><p>We got what sounds like a straightforward confident answer. However,
it is not clear how the model arrived at this solution. Second, we do
not know whether the solution is correct or not. In this case ChatGPT
made some assumptions for us, such as choosing a specific kind of
tokenizer to give the answer, and since we do not speak the language, we
do not know if this is indeed the best approach to tokenize Chinese
text. If we understand the concept of Token (which we will today!), then
we can be more informed about the quality of the answer, whether it is
useful to us, and therefore make a better use of the model.</p>
<p>And by the way, ChatGPT was <strong>almost</strong> correct, in the
specific case of the gpt-4 tokenizer, the model will return 12 tokens
(not 11!) for the given Chinese sentence.</p>
<figure><img src="fig/intro1b.png" alt="GPT-4 Tokenization Example" class="figure mx-auto d-block"><div class="figcaption">GPT-4 Tokenization Example</div>
</figure><p>We can also argue if the statement “Chinese is generally tokenized
character by character” is an overstatement or not. In any case, the
real question here is: Are we ok with <em>almost correct answers</em>?
Please note that this is not a call to avoid using LLM’s but a call for
a careful consideration of usage and more importantly, an attempt to
explain the mechanisms behind via NLP concepts.</p>
</section><section><h2 class="section-heading" id="language-as-data">Language as Data<a class="anchor" aria-label="anchor" href="#language-as-data"></a>
</h2>
<hr class="half-width">
<p>From a more technical perspective, NLP focuses on applying advanced
statistical techniques to linguistic data. This is a key factor, since
we need a structured dataset with a well defined set of features in
order to manipulate it numerically. Your first task as an NLP
practitioner is to <strong>understand what aspects of textual data are
relevant for your application</strong> and apply techniques to
systematically extract meaningful features from unstructured data (if
using statistics or Machine Learning) or choose an appropriate neural
architecture (if using Deep Learning) that can help solve our problem at
hand.</p>
<div class="section level3">
<h3 id="what-is-a-word">What is a word?<a class="anchor" aria-label="anchor" href="#what-is-a-word"></a>
</h3>
<p>When dealing with language our basic data unit is usually a word. We
deal with sequences of words and with how they relate to each other to
generate meaning in text pieces. Thus, our first step will be to load a
text file and provide it with structure by splitting it into valid words
(tokenization)!</p>
<div id="token-vs-word" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="token-vs-word" class="callout-inner">
<h3 class="callout-title">Token vs Word</h3>
<div class="callout-content">
<p>For simplicity, in the rest of the course we will use the terms
“word” and “token” interchangeably, but as we just saw they do not
always have the same granularity. Originally the concept of token
comprised dictionary words, numeric symbols and punctuation. Nowadays,
tokenization has also evolved and became an optimization task on its own
(How can we segment text in a way that neural networks learn optimally
from text?). Tokenizers allow one to reconstruct or revert back to the
original pre-tokenized form of tokens or words, hence we can afford to
use <em>token</em> and <em>word</em> as synonyms. If you are curious,
you can visualize how different state-of-the-art tokenizers split text
<a href="https://tiktokenizer.vercel.app/" class="external-link">in this WebApp</a></p>
</div>
</div>
</div>
<p>Let’s open a file, read it into a string and split it by spaces. We
will print the original text and the list of “words” to see how they
look:</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">"data/84_frankenstein_clean.txt"</span>) <span class="im">as</span> f:</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>  text <span class="op">=</span> f.read()</span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a><span class="bu">print</span>(text[:<span class="dv">100</span>])</span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Length:"</span>, <span class="bu">len</span>(text))</span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a>proto_tokens <span class="op">=</span> text.split()</span>
<span id="cb7-8"><a href="#cb7-8" tabindex="-1"></a><span class="bu">print</span>(proto_tokens[:<span class="dv">40</span>])</span>
<span id="cb7-9"><a href="#cb7-9" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(proto_tokens))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Letter 1 St. Petersburgh, Dec. 11th, 17-- TO Mrs. Saville, England You will rejoice to hear that no disaster has accompanied the commencement of an en
Length: 417931

Proto-Tokens:
['Letter', '1', 'St.', 'Petersburgh,', 'Dec.', '11th,', '17--', 'TO', 'Mrs.', 'Saville,', 'England', 'You', 'will', 'rejoice', 'to', 'hear', 'that', 'no', 'disaster', 'has', 'accompanied', 'the', 'commencement', 'of', 'an', 'enterprise', 'which', 'you', 'have', 'regarded', 'with', 'such', 'evil', 'forebodings.', 'I', 'arrived', 'here', 'yesterday,', 'and', 'my']
74942</code></pre>
</div>
<p>Splitting by white space is possible but needs several extra steps to
separate out punctuation appropriately. A more sophisticated approach is
to use the <a href="https://github.com/explosion/spaCy" class="external-link">spaCy</a>
library to segment the text into human-readable tokens. First we will
download the pre-trained model, in this case we only need the small
English version:</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="op">!</span> python <span class="op">-</span>m spacy download en_core_web_sm</span></code></pre>
</div>
<p>This is a model that spaCy already trained for us on a subset of web
English data. Hence, the model already “knows” how to tokenize into
English words. When the model processes a string, it does not only do
the splitting for us but already provides more advanced linguistic
properties of the tokens (such as part-of-speech tags, or named
entities). You can check more languages and models in the <a href="https://spacy.io/models" class="external-link">spacy documentation</a></p>
<div id="pre-trained-models-and-fine-tuning" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="pre-trained-models-and-fine-tuning" class="callout-inner">
<h3 class="callout-title">Pre-trained Models and Fine-tuning</h3>
<div class="callout-content">
<p>These two terms frequently arise in discussions of NLP. The notion of
pre-trained comes from Machine Learning and describes a model that has
already been optimized on relevant data for a given task. Such a model
can typically be loaded and applied directly to new datasets, often
working “out of the box.” without need of further refinement. Ideally,
publicly released pre-trained models have undergone rigorous testing for
both generalization and output quality on different textual data that it
was intended to be used on. Nevertheless, it remains essential to
carefully review the evaluation methods used before relying on them in
practice. It is also recommended that you perform your own evaluation of
the model on text that you intend to use it on.</p>
<p>Sometimes a pre-trained model is of good quality, but it does not fit
the nuances of our specific dataset. For example, the model was trained
on newspaper articles but you are interested in poetry. In this case, it
is common to perform <em>fine-tuning</em>, this means that instead of
training your own model from scratch, you start with the knowledge
obtained in the pre-trained model and adjust it (fine-tune it) to work
optimally with your specific data. If this is done well it leads to
increased performance in the specific task you are trying to solve. The
advantage of fine-tuning is that you often do not need a large amount of
data to improve the results, hence the popularity of the technique.</p>
</div>
</div>
</div>
<p>Let’s now import the model and use it to parse our document:</p>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a><span class="im">import</span> spacy</span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a>nlp <span class="op">=</span> spacy.load(<span class="st">"en_core_web_sm"</span>) <span class="co"># we load the small English model for efficiency</span></span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a>doc <span class="op">=</span> nlp(text) <span class="co"># Doc is a python object with several methods to retrieve linguistic properties</span></span>
<span id="cb10-6"><a href="#cb10-6" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" tabindex="-1"></a><span class="co"># SpaCy-Tokens</span></span>
<span id="cb10-8"><a href="#cb10-8" tabindex="-1"></a>tokens <span class="op">=</span> [token.text <span class="cf">for</span> token <span class="kw">in</span> doc] <span class="co"># Note that spacy tokens are also python objects </span></span>
<span id="cb10-9"><a href="#cb10-9" tabindex="-1"></a><span class="bu">print</span>(tokens[:<span class="dv">40</span>])</span>
<span id="cb10-10"><a href="#cb10-10" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(tokens))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>['Letter', '1', 'St.', 'Petersburgh', ',', 'Dec.', '11th', ',', '17', '-', '-', 'TO', 'Mrs.', 'Saville', ',', 'England', 'You', 'will', 'rejoice', 'to', 'hear', 'that', 'no', 'disaster', 'has', 'accompanied', 'the', 'commencement', 'of', 'an', 'enterprise', 'which', 'you', 'have', 'regarded', 'with', 'such', 'evil', 'forebodings', '.']
85713</code></pre>
</div>
<p>The differences look subtle at the beginning, but if we carefully
inspect the way spaCy splits the text, we can see the advantage of using
a specialized tokenizer. There are also several useful features that
spaCy provides us with. For example, we can choose to extract only
symbols, or only alphanumerical tokens, and more advanced linguistic
properties, for example we can remove punctuation and only keep
alphanumerical tokens:</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a>only_words <span class="op">=</span> [token <span class="cf">for</span> token <span class="kw">in</span> doc <span class="cf">if</span> token.is_alpha]  <span class="co"># Only alphanumerical tokens</span></span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a><span class="bu">print</span>(only_words[:<span class="dv">50</span>])</span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(only_words))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[Letter, Petersburgh, TO, Saville, England, You, will, rejoice, to, hear, that, no, disaster, has, accompanied, the, commencement, of, an, enterprise, which, you, have, regarded, with, such, evil, forebodings, I, arrived, here, yesterday, and, my, first, task, is, to, assure, my, dear, sister, of, my, welfare, and, increasing, confidence, in, the]
75062</code></pre>
</div>
<p>or keep only the verbs from our text:</p>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a>only_verbs <span class="op">=</span> [token <span class="cf">for</span> token <span class="kw">in</span> doc <span class="cf">if</span> token.pos_ <span class="op">==</span> <span class="st">"VERB"</span>]  <span class="co"># Only verbs</span></span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a><span class="bu">print</span>(only_verbs[:<span class="dv">10</span>])</span>
<span id="cb14-3"><a href="#cb14-3" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(only_verbs))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[rejoice, hear, accompanied, regarded, arrived, assure, increasing, walk, feel, braces]
10148</code></pre>
</div>
<p>SpaCy also predicts the sentences under the hood for us. It might
seem trivial to you as a human reader to recognize where a sentence
begins and ends but for a machine, just like finding words, finding
sentences is a task on its own, for which sentence-segmentation models
exist. In the case of Spacy, we can access the sentences like this:</p>
<div class="codewrapper sourceCode" id="cb16">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a>sentences <span class="op">=</span> [sent.text <span class="cf">for</span> sent <span class="kw">in</span> doc.sents] <span class="co"># Sentences are also python objects</span></span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a><span class="bu">print</span>(sentences[:<span class="dv">5</span>])</span>
<span id="cb16-3"><a href="#cb16-3" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(sentences))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>['Letter 1 St. Petersburgh, Dec. 11th, 17-- TO Mrs. Saville, England You will rejoice to hear that no disaster has accompanied the commencement of an enterprise which you have regarded with such evil forebodings.', 'I arrived here yesterday, and my first task is to assure my dear sister of my welfare and increasing confidence in the success of my undertaking.', 'I am already far north of London, and as I walk in the streets of Petersburgh, I feel a cold northern breeze play upon my cheeks, which braces my nerves and fills me with delight.', 'Do you understand this feeling?', 'This breeze, which has traveled from the regions towards which I am advancing, gives me a foretaste of those icy climes.']
3317</code></pre>
</div>
<p>We can also see what named entities the model predicted:</p>
<div class="codewrapper sourceCode" id="cb18">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(doc.ents))</span>
<span id="cb18-2"><a href="#cb18-2" tabindex="-1"></a><span class="cf">for</span> ent <span class="kw">in</span> doc.ents[:<span class="dv">5</span>]:</span>
<span id="cb18-3"><a href="#cb18-3" tabindex="-1"></a>    <span class="bu">print</span>(ent.label_, ent.text)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>1713
DATE Dec. 11th
CARDINAL 17
PERSON Saville
GPE England
DATE yesterday</code></pre>
</div>
<p>These are just basic tests to demonstrate how you can immediately
process the structure of text using existing NLP libraries. The spaCy
models we used are simpler relative to state of the art approaches. So
the more complex the input text and task, the more errors are likely to
appear when using such models. The biggest advantage of using these
existing libraries is that they help you transform unstructured plain
text files into structured data that you can manipulate later for your
own goals such as training language models.</p>
<div id="computing-stats-with-spacy" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="computing-stats-with-spacy" class="callout-inner">
<h3 class="callout-title">Computing stats with spaCy</h3>
<div class="callout-content">
<p>Use the spaCy Doc object to compute an aggregate statistic about the
Frankenstein book. HINT: Use the python <code>set</code>,
<code>dictionary</code> or <code>Counter</code> objects to hold the
accumulative counts. For example:</p>
<ul>
<li>Give the list of the 20 most common verbs in the book</li>
<li>How many different Places are identified in the book? (Label =
GPE)</li>
<li>How many different entity categories are in the book?</li>
<li>Who are the 10 most mentioned PERSONs in the book?</li>
<li>Or any other similar aggregate you want…</li>
</ul>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2"> Show me the solution </h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" data-bs-parent="#accordionSolution2" aria-labelledby="headingSolution2">
<div class="accordion-body">
<p>Let’s describe the solution to obtain all the different entity
categories. For that we should iterate the whole text and keep a python
set with all the seen labels.</p>
<div class="codewrapper sourceCode" id="cb20">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" tabindex="-1"></a>entity_types <span class="op">=</span> <span class="bu">set</span>()</span>
<span id="cb20-2"><a href="#cb20-2" tabindex="-1"></a></span>
<span id="cb20-3"><a href="#cb20-3" tabindex="-1"></a><span class="cf">for</span> ent <span class="kw">in</span> doc.ents:</span>
<span id="cb20-4"><a href="#cb20-4" tabindex="-1"></a>    entity_types.add(ent.label_)</span>
<span id="cb20-5"><a href="#cb20-5" tabindex="-1"></a></span>
<span id="cb20-6"><a href="#cb20-6" tabindex="-1"></a><span class="bu">print</span>(entity_types)</span>
<span id="cb20-7"><a href="#cb20-7" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(entity_types))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>{'CARDINAL', 'GPE', 'WORK_OF_ART', 'ORDINAL', 'DATE', 'LAW', 'PRODUCT', 'QUANTITY', 'ORG', 'TIME', 'PERSON', 'LOC', 'LANGUAGE', 'FAC', 'NORP'}
15</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="nlp-tasks">NLP tasks<a class="anchor" aria-label="anchor" href="#nlp-tasks"></a>
</h3>
<p>The previous exercise shows that a great deal of NLP techniques are
embedded in our daily life. Indeed NLP is an important component in a
wide range of software applications that we use in our day to day
activities.</p>
<p>There are several ways to describe the tasks that NLP solves. From
the Machine Learning perspective, we have:</p>
<ul>
<li>Unsupervised tasks: exploiting existing patterns from large amounts
of text.</li>
</ul>
<figure><img src="fig/intro_unsupervised.png" width="582" alt="Unsupervised Learning" class="figure mx-auto d-block"><div class="figcaption">Unsupervised Learning</div>
</figure><ul>
<li>Supervised tasks: learning to classify texts given a labeled set of
examples</li>
</ul>
<figure><img src="fig/intro_supervised.png" width="605" alt="Supervised Learning" class="figure mx-auto d-block"><div class="figcaption">Supervised Learning</div>
</figure><p>The Deep Learning perspective usually involves the selection of the
right model among different neural network architectures to tackle an
NLP task, such as:</p>
<ul>
<li><p>Multi-layer Perceptron</p></li>
<li><p>Recurrent Neural Network</p></li>
<li><p>Convolutional Neural Network</p></li>
<li><p>Long-Short Term Memory Networks (LSTMs)</p></li>
<li><p>Transformer (including LLMs!)</p></li>
</ul>
<p>Regardless of the chosen method, below we show one possible taxonomy
of NLP tasks. The tasks are grouped together with some of their most
prominent applications. This is definitely a non-exhaustive list, as in
reality there are hundreds of them, but it is a good start:</p>
<figure><img src="fig/intro_taxonomy.png" width="630" alt="A taxonomy of NLP Tasks" class="figure mx-auto d-block"><div class="figcaption">A taxonomy of NLP Tasks</div>
</figure><ul>
<li>
<p><strong>Text Classification</strong>: Assign one or more labels
to a given piece of text. This text is usually referred to as a
<em>document</em> and in our context this can be a sentence, a
paragraph, a book chapter, etc…</p>
<ul>
<li>
<strong>Language Identification</strong>: determining the language
in which a particular input text is written.</li>
<li>
<strong>Spam Filtering</strong>: classifying emails into spam or not
spam based on their content.</li>
<li>
<strong>Authorship Attribution</strong>: determining the author of a
text based on its style and content (based on the assumption that each
author has a unique writing style).</li>
<li>
<strong>Sentiment Analysis</strong>: classifying text into positive,
negative or neutral sentiment. For example, in the sentence “I love this
product!”, the model would classify it as positive sentiment.</li>
</ul>
</li>
<li>
<p><strong>Token Classification</strong>: The task of individually
assigning one label to each word in a document. This is a one-to-one
mapping; however, because words do not occur in isolation and their
meaning depend on the sequence of words to the left or the right of
them, this is also called Word-In-Context Classification or Sequence
Labeling and usually involves syntactic and semantic analysis.</p>
<ul>
<li>
<strong>Part-Of-Speech Tagging</strong>: is the task of assigning a
part-of-speech label (e.g., noun, verb, adjective) to each word in a
sentence.</li>
<li>
<strong>Chunking</strong>: splitting a running text into “chunks” of
words that together represent a meaningful unit: phrases, sentences,
paragraphs, etc.</li>
<li>
<strong>Word Sense Disambiguation</strong>: based on the context
what does a word mean (think of “book” in “I read a book.” vs “I want to
book a flight.”)</li>
<li>
<strong>Named Entity Recognition</strong>: recognize world entities
in text, e.g. Persons, Locations, Book Titles, or many others. For
example “Mary Shelley” is a person, “Frankenstein or the Modern
Prometheus” is a book, etc.</li>
<li>
<strong>Semantic Role Labeling</strong>: the task of finding out
“Who did what to whom?” in a sentence: information from events such as
agents, participants, circumstances, subject-verb-object triples
etc.</li>
<li>
<strong>Relation Extraction</strong>: the task of identifying named
relationships between entities in a text, e.g. “Apple is based in
California” has the relation (Apple, based_in, California).</li>
<li>
<strong>Co-reference Resolution</strong>: the task of determining
which words refer to the same entity in a text, e.g. “Mary is a doctor.
She works at the hospital.” Here “She” refers to “Mary”.</li>
<li>
<strong>Entity Linking</strong>: the task of disambiguation of named
entities in a text, linking them to their corresponding entries in a
knowledge base, e.g. Mary Shelley’s biography in Wikipedia.</li>
</ul>
</li>
<li>
<p><strong>Language Modeling</strong>: Given a sequence of words,
the model predicts the next word. For example, in the sentence “The
capital of France is _____”, the model should predict “Paris” based on
the context. This task was initially useful for building solutions that
require speech and optical character recognition (even handwriting),
language translation and spelling correction. Nowadays this has scaled
up to the LLMs that we know. A byproduct of pre-trained Language
Modeling is the <strong>vectorized representation</strong> of texts
which allows to perform specific tasks such as:</p>
<ul>
<li>
<strong>Text Similarity</strong>: The task of determining how
similar two pieces of text are.</li>
<li>
<strong>Plagiarism detection</strong>: determining whether a piece
of text, B, is close enough to another known piece of text, A, which
increases the likelihood that it was plagiarized.</li>
<li>
<strong>Document clustering</strong>: grouping similar texts
together based on their content.</li>
<li>
<strong>Topic modelling</strong>: a specific instance of clustering,
here we automatically identify abstract “topics” that occur in a set of
documents, where each topic is represented as a cluster of words that
frequently appear together.</li>
<li>
<strong>Information Retrieval</strong>: this is the task of finding
relevant information or documents from a large collection of
unstructured data based on user’s query, e.g., “What’s the best
restaurant near me?”.</li>
</ul>
</li>
<li>
<p><strong>Text Generation</strong>: the task of generating text
based on a given input. This is usually done by generating the output
word by word, conditioned on both the input and the output so far. The
difference with Language Modeling is that for generation there are
higher-level generation objectives such as:</p>
<ul>
<li>
<strong>Machine Translation</strong>: translating text from one
language to another, e.g., “Hello” in English to “Que tal” in
Spanish.</li>
<li>
<strong>Summarization</strong>: generating a concise summary of a
longer text. It can be abstractive (generating new sentences that
capture the main ideas of the original text) but also extractive
(selecting important sentences from the original text).</li>
<li>
<strong>Paraphrasing</strong>: generating a new sentence that
conveys the same meaning as the original sentence, e.g., “The cat is on
the mat.” to “The mat has a cat on it.”.</li>
<li>
<strong>Question Answering</strong>: given a question and a context,
the model generates an answer. For example, given the question “What is
the capital of France?” and the Wikipedia article about France as the
context, the model should answer “Paris”. This task can be approached as
a text classification problem (where the answer is one of the predefined
options) or as a generative task (where the model generates the answer
from scratch).</li>
<li>
<strong>Conversational Agent (ChatBot)</strong>: Building a system
that interacts with a user via natural language, e.g., “What’s the
weather today, Siri?”. These agents are widely used to improve user
experience in customer service, personal assistance and many other
domains.</li>
</ul>
</li>
</ul>
<p>For the purposes of this episode, we will focus on <strong>supervised
learning</strong> tasks and we will emphasize how the
<strong>Transformer architecture</strong> is used to tackle some of
them.</p>
<div id="inputs-and-outputs" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="inputs-and-outputs" class="callout-inner">
<h3 class="callout-title">Inputs and Outputs</h3>
<div class="callout-content">
<p>Look at the NLP Task taxonomy described above and write down a couple
of examples of (Input, Output) instance pairs that you would need in
order to train a supervised model for your chosen task.</p>
<p>For Example: the task of labeling an E-mail as spam or not-spam</p>
<p>Label_Set: [SPAM, NO-SPAM]</p>
<p>Training Instances:</p>
<p><strong>Input:</strong> “Dear Sir, you’ve been awarded a grant of 10
million Euros and it is only available today. Please contact me ASAP!”
<strong>Output:</strong> SPAM</p>
<p><strong>Input:</strong> “Dear Madam, as agreed by phone here is the
sales report for last month.” <strong>Output:</strong> NO-SPAM</p>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3"> Show me the solution </h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" data-bs-parent="#accordionSolution3" aria-labelledby="headingSolution3">
<div class="accordion-body">
<p>Example B: the task of Conversational agent. Here are 3 instances to
provide supervision for a model:</p>
<p>Label_Set: Output vocabulary. This is: learning to generate token by
token a coherent response that addresses the input question.</p>
<p><strong>Input:</strong> “Hello, how are you?”
<strong>Output:</strong> “I am fine thanks!”</p>
<p><strong>Input:</strong> “Do you know at what time is the World Cup
final today?” <strong>Output:</strong> “Yes, the World Cup final will be
at 6pm CET”</p>
<p><strong>Input:</strong> “What color is my shirt?”
<strong>Output:</strong> “Sorry, I am unable to see what you are
wearing.”</p>
</div>
</div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="a-primer-on-linguistics">A Primer on Linguistics<a class="anchor" aria-label="anchor" href="#a-primer-on-linguistics"></a>
</h2>
<hr class="half-width">
<p>Natural language exhibits a set of properties that make it more
challenging to process than other types of data such as tables,
spreadsheets or time series. <strong>Language is hard to process because
it is compositional, ambiguous, discrete and sparse</strong>.</p>
<div class="section level3">
<h3 id="compositionality">Compositionality<a class="anchor" aria-label="anchor" href="#compositionality"></a>
</h3>
<p>The basic elements of written languages are characters, a sequence of
characters form words, and words in turn denote objects, concepts,
events, actions and ideas (Goldberg, 2016). Subsequently words form
phrases and sentences which are used in communication and depend on the
context in which they are used. We as humans derive the meaning of
utterances from interpreting contextual information that is present at
different levels at the same time:</p>
<figure><img src="fig/intro2_levels_lang.svg" width="573" alt="Levels of Language" class="figure mx-auto d-block"><div class="figcaption">Levels of Language</div>
</figure><p>The first two levels refer to spoken language only, and the other
four levels are present in both speech and text. Because in principle
machines do not have access to the same levels of information that we do
(they can only have independent audio, textual or visual inputs), we
need to come up with clever methods to overcome this significant
limitation. Knowing the levels of language is important so we consider
what kind of problems we are facing when attempting to solve our NLP
task at hand.</p>
</div>
<div class="section level3">
<h3 id="ambiguity">Ambiguity<a class="anchor" aria-label="anchor" href="#ambiguity"></a>
</h3>
<p>The disambiguation of meaning is usually a by-product of the context
in which utterances are expressed and also the historic accumulation of
interactions which are transmitted across generations (think for
instance to idioms – these are usually meaningless phrases that acquire
meaning only if situated within their historical and societal context).
These characteristics make NLP a particularly challenging field to work
in.</p>
<p>We cannot expect a machine to process human language and simply
understand it as it is. We need a systematic, scientific approach to
deal with it. It’s within this premise that the field of NLP is born,
primarily interested in converting the building blocks of human/natural
language into something that a machine can understand.</p>
<p>The image below shows how the levels of language relate to a few NLP
applications:</p>
<figure><img src="fig/intro3_levels_nlp.png" alt="Diagram showing building blocks of language" class="figure mx-auto d-block"><div class="figcaption">Diagram showing building blocks of
language</div>
</figure><div id="levels-of-ambiguity" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="levels-of-ambiguity" class="callout-inner">
<h3 class="callout-title">Levels of ambiguity</h3>
<div class="callout-content">
<p>Discuss what the following sentences mean. What level of ambiguity do
they represent?:</p>
<ul>
<li><p>“The door is unlockable from the inside.” vs “Unfortunately, the
cabinet is unlockable, so we can’t secure it”</p></li>
<li><p>“I saw the <em>cat with the stripes</em>” vs “I saw the cat
<em>with the telescope</em>”</p></li>
<li><p>“Please don’t drive the cat to the vet!” vs “Please don’t drive
the car tomorrow!”</p></li>
<li><p>“I never said she stole my money.” (re-write this sentence
multiple times and each time emphasize a different word in
uppercases).</p></li>
</ul>
</div>
</div>
</div>
<div id="accordionSolution4" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution4" aria-expanded="false" aria-controls="collapseSolution4">
  <h4 class="accordion-header" id="headingSolution4"> Show me the solution </h4>
</button>
<div id="collapseSolution4" class="accordion-collapse collapse" data-bs-parent="#accordionSolution4" aria-labelledby="headingSolution4">
<div class="accordion-body">
<p>This is why the previous statements were difficult:</p>
<ul>
<li>“Un-lockable vs Unlock-able” is a <strong>Morphological</strong>
ambiguity: Same word form, two possible meanings</li>
<li>“I saw the cat with the telescope” has a <strong>Syntactic</strong>
ambiguity: Same sentence structure, different properties</li>
<li>“drive the cat” vs “drive the car” shows a <strong>Semantic</strong>
ambiguity: Syntactically identical sentences that imply quite different
actions.</li>
<li>“I NEVER said she stole MY money.” is a <strong>Pragmatic</strong>
ambiguity: Meaning relies on word emphasis</li>
</ul>
</div>
</div>
</div>
</div>
<p>Whenever you are solving a specific task, you should ask yourself
what kind of ambiguity can affect your results, and to what degrees? At
what level are your assumptions operating when defining your research
questions? Having the answers to this can save you a lot of time when
debugging your models. Sometimes the most innocent assumptions (for
example using the wrong tokenizer) can create enormous performance drops
even when the higher level assumptions were correct.</p>
</div>
<div class="section level3">
<h3 id="sparsity">Sparsity<a class="anchor" aria-label="anchor" href="#sparsity"></a>
</h3>
<p>Another key property of linguistic data is its sparsity. This means
that if we are hunting for a specific phenomenon, we may often realize
it barely occurs inside a vast amount of text. Imagine we have the
following brief text and we are interested in <em>pizzas</em> and
<em>hamburgers</em>:</p>
<div class="codewrapper sourceCode" id="cb22">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" tabindex="-1"></a><span class="co"># A mini-corpus where our target words appear</span></span>
<span id="cb22-2"><a href="#cb22-2" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"""</span></span>
<span id="cb22-3"><a href="#cb22-3" tabindex="-1"></a><span class="st">I am hungry. Should I eat delicious pizza?</span></span>
<span id="cb22-4"><a href="#cb22-4" tabindex="-1"></a><span class="st">Or maybe I should eat a juicy hamburger instead.</span></span>
<span id="cb22-5"><a href="#cb22-5" tabindex="-1"></a><span class="st">Many people like to eat pizza because is tasty, they think pizza is delicious as hell!</span></span>
<span id="cb22-6"><a href="#cb22-6" tabindex="-1"></a><span class="st">My friend prefers to eat a hamburger and I agree with him.</span></span>
<span id="cb22-7"><a href="#cb22-7" tabindex="-1"></a><span class="st">We will drive our car to the restaurant to get the succulent hamburger.</span></span>
<span id="cb22-8"><a href="#cb22-8" tabindex="-1"></a><span class="st">Right now, our cat sleeps on the mat so we won't take him.</span></span>
<span id="cb22-9"><a href="#cb22-9" tabindex="-1"></a><span class="st">I did not wash my car, but at least the car has gasoline.</span></span>
<span id="cb22-10"><a href="#cb22-10" tabindex="-1"></a><span class="st">Perhaps when we come back we will take out the cat for a walk.</span></span>
<span id="cb22-11"><a href="#cb22-11" tabindex="-1"></a><span class="st">The cat will be happy then.</span></span>
<span id="cb22-12"><a href="#cb22-12" tabindex="-1"></a><span class="st">"""</span></span></code></pre>
</div>
<p>We can first use spaCy to tokenize the text and do some direct word
count:</p>
<div class="codewrapper sourceCode" id="cb23">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" tabindex="-1"></a><span class="im">import</span> spacy</span>
<span id="cb23-2"><a href="#cb23-2" tabindex="-1"></a>nlp <span class="op">=</span> spacy.load(<span class="st">"en_core_web_sm"</span>)</span>
<span id="cb23-3"><a href="#cb23-3" tabindex="-1"></a></span>
<span id="cb23-4"><a href="#cb23-4" tabindex="-1"></a>doc <span class="op">=</span> nlp(text)</span>
<span id="cb23-5"><a href="#cb23-5" tabindex="-1"></a>words <span class="op">=</span> [token.lower_ <span class="cf">for</span> token <span class="kw">in</span> doc <span class="cf">if</span> token.is_alpha]  <span class="co"># Filter out punctuation and new lines</span></span>
<span id="cb23-6"><a href="#cb23-6" tabindex="-1"></a><span class="bu">print</span>(words)</span>
<span id="cb23-7"><a href="#cb23-7" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(words))</span></code></pre>
</div>
<p>We have in total 104 words, but we actually want to know how many
times each word appears. For that we use the Python Counter and then we
can visualize it inside a chart with matplotlib:</p>
<div class="codewrapper sourceCode" id="cb24">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> Counter</span>
<span id="cb24-2"><a href="#cb24-2" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb24-3"><a href="#cb24-3" tabindex="-1"></a></span>
<span id="cb24-4"><a href="#cb24-4" tabindex="-1"></a>word_count <span class="op">=</span> Counter(words).most_common()</span>
<span id="cb24-5"><a href="#cb24-5" tabindex="-1"></a>tokens <span class="op">=</span> [item[<span class="dv">0</span>] <span class="cf">for</span> item <span class="kw">in</span> word_count]</span>
<span id="cb24-6"><a href="#cb24-6" tabindex="-1"></a>frequencies <span class="op">=</span> [item[<span class="dv">1</span>] <span class="cf">for</span> item <span class="kw">in</span> word_count]</span>
<span id="cb24-7"><a href="#cb24-7" tabindex="-1"></a></span>
<span id="cb24-8"><a href="#cb24-8" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">18</span>, <span class="dv">6</span>))</span>
<span id="cb24-9"><a href="#cb24-9" tabindex="-1"></a>plt.bar(tokens, frequencies)</span>
<span id="cb24-10"><a href="#cb24-10" tabindex="-1"></a>plt.xticks(rotation<span class="op">=</span><span class="dv">90</span>)</span>
<span id="cb24-11"><a href="#cb24-11" tabindex="-1"></a>plt.show()</span></code></pre>
</div>
<p>This bar chart shows us several things about sparsity, even with such
a small text:</p>
<ul>
<li><p>The most common words are filler words such as “the”, “of”, “not”
etc. These are known as <strong>stopwords</strong> because such words by
themselves generally do not hold a lot of information about the meaning
of the piece of text.</p></li>
<li><p>The two concepts (hamburger and pizza) we are interested in,
appear only 3 times each, out of 104 words (comprising only ~3% of our
corpus). This number only goes lower as the corpus size
increases</p></li>
<li><p>There is a long tail in the distribution, where actually a lot of
meaningful words are located.</p></li>
</ul>
<div id="stop-words" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="stop-words" class="callout-inner">
<h3 class="callout-title">Stop Words</h3>
<div class="callout-content">
<p><strong>Stop words</strong> are extremely frequent syntactic filler
words that do not provide relevant semantic information for our use
case. For some use cases it is better to ignore them in order to fight
the sparsity phenomenon. However, consider that in many other use cases
the syntactic information that stop words provide is crucial to solve
the task.</p>
<p>SpaCy has a pre-defined list of stopwords per language. To explicitly
load the English stop words we can do:</p>
<div class="codewrapper sourceCode" id="cb25">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" tabindex="-1"></a><span class="im">from</span> spacy.lang.en.stop_words <span class="im">import</span> STOP_WORDS</span>
<span id="cb25-2"><a href="#cb25-2" tabindex="-1"></a><span class="bu">print</span>(STOP_WORDS)  <span class="co"># a set of common stopwords</span></span>
<span id="cb25-3"><a href="#cb25-3" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(STOP_WORDS)) <span class="co"># There are 326 words considered in this list</span></span></code></pre>
</div>
<p>You can also manually extend the list of stop words if you are
interested in ignoring other unlisted terms that you encounter in your
data.</p>
<p>Alternatively, you can filter out stop words when iterating your
tokens (remember the spaCy token properties!) like this:</p>
<div class="codewrapper sourceCode" id="cb26">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" tabindex="-1"></a>doc <span class="op">=</span> nlp(text)</span>
<span id="cb26-2"><a href="#cb26-2" tabindex="-1"></a>content_words <span class="op">=</span> [token.text <span class="cf">for</span> token <span class="kw">in</span> doc <span class="cf">if</span> token.is_alpha <span class="kw">and</span> <span class="kw">not</span> token.is_stop]  <span class="co"># Filter out stop words and punctuation</span></span>
<span id="cb26-3"><a href="#cb26-3" tabindex="-1"></a><span class="bu">print</span>(content_words)</span></code></pre>
</div>
</div>
</div>
</div>
<div id="callout4" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div class="callout-inner">
<div class="callout-content">
<p>Sparsity is closely related to what is frequently called
<strong>domain-specific data</strong>. The discourse context in which
language is used varies importantly across disciplines (domains). Take
for example law texts and medical texts which are typically filled with
domain-specific jargon. We should expect the top part of the
distribution to contain mostly the same words as they tend to be stop
words. But once we remove the stop words, the top of the distribution
will contain very different content words.</p>
<p>Also, the meaning of concepts described in each domain might
significantly differ. For example the word “trial” refers to a procedure
for examining evidence in court, but in the medical domain this could
refer to a clinical “trial” which is a procedure to test the efficacy
and safety of treatments on patients. For this reason there are
specialized models and corpora that model language use in specific
domains. The concept of fine-tuning a general purpose model with
domain-specific data is also popular, even when using LLMs.</p>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="discreteness">Discreteness<a class="anchor" aria-label="anchor" href="#discreteness"></a>
</h3>
<p>There is no inherent relationship between the form of a word and its
meaning. For this reason, by syntactic or lexical analysis alone, there
is no automatic way of knowing if two words are similar in meaning or
how they relate semantically to each other. For example, “car” and “cat”
appear to be very closely related at the morphological level, only one
letter needs to change to convert one word into the other. But the two
words represent concepts or entities in the world which are very
different. Conversely, “pizza” and “hamburger” look very different (they
only share one letter in common) but are more closely related
semantically, because they both refer to typical fast foods.</p>
<p>How can we automatically know that “pizza” and “hamburger” share more
semantic properties than “car” and “cat”? One way is by looking at the
<strong>context</strong> (neighboring words) of these words. This idea
is the principle behind <strong>distributional semantics</strong>, and
aims to look at the statistical properties of language, such as word
co-occurrences (what words are typically located nearby a given word in
a given corpus of text), to understand how words relate to each
other.</p>
<p>Let’s keep using the list of words from our mini corpus:</p>
<div class="codewrapper sourceCode" id="cb27">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" tabindex="-1"></a>words <span class="op">=</span> [token.lower_ <span class="cf">for</span> token <span class="kw">in</span> doc <span class="cf">if</span> token.is_alpha]</span></code></pre>
</div>
<p>Now we will create a dictionary where we accumulate the words that
appear around our words of interest. In this case we want to find out,
according to our corpus, the most frequent words that occur around
<em>pizza</em>, <em>hamburger</em>, <em>car</em> and <em>cat</em>:</p>
<div class="codewrapper sourceCode" id="cb28">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" tabindex="-1"></a>target_words <span class="op">=</span> [<span class="st">"pizza"</span>, <span class="st">"hamburger"</span>, <span class="st">"car"</span>, <span class="st">"cat"</span>] <span class="co"># words we want to analyze</span></span>
<span id="cb28-2"><a href="#cb28-2" tabindex="-1"></a>co_occurrence <span class="op">=</span> {word: [] <span class="cf">for</span> word <span class="kw">in</span> target_words}</span>
<span id="cb28-3"><a href="#cb28-3" tabindex="-1"></a>co_occurrence</span></code></pre>
</div>
<p>We iterate over each word in our corpus, collecting its surrounding
words within a defined window. A window consists of a set number of
words to the left and right of the target word, as determined by the
window_size parameter. For example, with <code>window_size = 3</code>, a
word <code>W</code> has a window of six neighboring words—three
preceding and three following—excluding <code>W</code> itself:</p>
<div class="codewrapper sourceCode" id="cb29">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" tabindex="-1"></a>window_size <span class="op">=</span> <span class="dv">3</span> <span class="co"># How many words to look at on each side</span></span>
<span id="cb29-2"><a href="#cb29-2" tabindex="-1"></a><span class="cf">for</span> i, word <span class="kw">in</span> <span class="bu">enumerate</span>(words):</span>
<span id="cb29-3"><a href="#cb29-3" tabindex="-1"></a>    <span class="co"># If the current word is one of our target words...</span></span>
<span id="cb29-4"><a href="#cb29-4" tabindex="-1"></a>    <span class="cf">if</span> word <span class="kw">in</span> target_words:</span>
<span id="cb29-5"><a href="#cb29-5" tabindex="-1"></a>        start <span class="op">=</span> <span class="bu">max</span>(<span class="dv">0</span>, i <span class="op">-</span> window_size) <span class="co"># get the start index of the window</span></span>
<span id="cb29-6"><a href="#cb29-6" tabindex="-1"></a>        end <span class="op">=</span> <span class="bu">min</span>(<span class="bu">len</span>(words), i <span class="op">+</span> <span class="dv">1</span> <span class="op">+</span> window_size) <span class="co"># get the end index of the window</span></span>
<span id="cb29-7"><a href="#cb29-7" tabindex="-1"></a>        context <span class="op">=</span> words[start:i] <span class="op">+</span> words[i<span class="op">+</span><span class="dv">1</span>:end]  <span class="co"># Exclude the target word itself</span></span>
<span id="cb29-8"><a href="#cb29-8" tabindex="-1"></a>        co_occurrence[word].extend(context)</span>
<span id="cb29-9"><a href="#cb29-9" tabindex="-1"></a></span>
<span id="cb29-10"><a href="#cb29-10" tabindex="-1"></a><span class="bu">print</span>(co_occurrence)</span></code></pre>
</div>
<p>We call the words that fall inside this window the
<code>context</code> of a target word. We can already see other
interesting related words in the context of each target word, but a lot
of non interesting stuff is in there. To obtain even nicer results, we
can delete the stop words from the context window before adding it to
the dictionary. You can define your own stop words, here we use the
STOP_WORDS list provided by spaCy:</p>
<div class="codewrapper sourceCode" id="cb30">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" tabindex="-1"></a><span class="im">from</span> spacy.lang.en.stop_words <span class="im">import</span> STOP_WORDS</span>
<span id="cb30-2"><a href="#cb30-2" tabindex="-1"></a></span>
<span id="cb30-3"><a href="#cb30-3" tabindex="-1"></a>co_occurrence <span class="op">=</span> {word: [] <span class="cf">for</span> word <span class="kw">in</span> target_words} <span class="co"># Empty the dictionary</span></span>
<span id="cb30-4"><a href="#cb30-4" tabindex="-1"></a></span>
<span id="cb30-5"><a href="#cb30-5" tabindex="-1"></a>window_size <span class="op">=</span> <span class="dv">3</span> <span class="co"># How many words to look at on each side</span></span>
<span id="cb30-6"><a href="#cb30-6" tabindex="-1"></a><span class="cf">for</span> i, word <span class="kw">in</span> <span class="bu">enumerate</span>(words):</span>
<span id="cb30-7"><a href="#cb30-7" tabindex="-1"></a>    <span class="co"># If the current word is one of our target words...</span></span>
<span id="cb30-8"><a href="#cb30-8" tabindex="-1"></a>    <span class="cf">if</span> word <span class="kw">in</span> target_words:</span>
<span id="cb30-9"><a href="#cb30-9" tabindex="-1"></a>        start <span class="op">=</span> <span class="bu">max</span>(<span class="dv">0</span>, i <span class="op">-</span> window_size) <span class="co"># get the start index of the window</span></span>
<span id="cb30-10"><a href="#cb30-10" tabindex="-1"></a>        end <span class="op">=</span> <span class="bu">min</span>(<span class="bu">len</span>(words), i <span class="op">+</span> <span class="dv">1</span> <span class="op">+</span> window_size) <span class="co"># get the end index of the window</span></span>
<span id="cb30-11"><a href="#cb30-11" tabindex="-1"></a>        context <span class="op">=</span> words[start:i] <span class="op">+</span> words[i<span class="op">+</span><span class="dv">1</span>:end]  <span class="co"># Exclude the target word itself</span></span>
<span id="cb30-12"><a href="#cb30-12" tabindex="-1"></a>        context <span class="op">=</span> [w <span class="cf">for</span> w <span class="kw">in</span> context <span class="cf">if</span> w <span class="kw">not</span> <span class="kw">in</span> STOP_WORDS] <span class="co"># Filter out stop words</span></span>
<span id="cb30-13"><a href="#cb30-13" tabindex="-1"></a>        co_occurrence[word].extend(context)</span>
<span id="cb30-14"><a href="#cb30-14" tabindex="-1"></a></span>
<span id="cb30-15"><a href="#cb30-15" tabindex="-1"></a><span class="bu">print</span>(co_occurrence)</span></code></pre>
</div>
<p>Our dictionary keys represent each word of interest, and the values
are a list of the words that occur within <em>window_size</em> distance
of the word. Now we use a Counter to get the most common items:</p>
<div class="codewrapper sourceCode" id="cb31">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" tabindex="-1"></a><span class="co"># Print the most common context words for each target word</span></span>
<span id="cb31-2"><a href="#cb31-2" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Contextual Fingerprints:</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb31-3"><a href="#cb31-3" tabindex="-1"></a><span class="cf">for</span> word, context_list <span class="kw">in</span> co_occurrence.items():</span>
<span id="cb31-4"><a href="#cb31-4" tabindex="-1"></a>    <span class="co"># We use Counter to get a frequency count of context words</span></span>
<span id="cb31-5"><a href="#cb31-5" tabindex="-1"></a>    fingerprint <span class="op">=</span> Counter(context_list).most_common(<span class="dv">5</span>)</span>
<span id="cb31-6"><a href="#cb31-6" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"'</span><span class="sc">{</span>word<span class="sc">}</span><span class="ss">': </span><span class="sc">{</span>fingerprint<span class="sc">}</span><span class="ss">"</span>)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Contextual Fingerprints:

'pizza': [('eat', 2), ('delicious', 2), ('tasty', 2), ('maybe', 1), ('like', 1)]
'hamburger': [('eat', 2), ('juicy', 1), ('instead', 1), ('people', 1), ('agree', 1)]
'car': [('drive', 1), ('restaurant', 1), ('wash', 1), ('gasoline', 1)]
'cat': [('walk', 2), ('right', 1), ('sleeps', 1), ('happy', 1)]</code></pre>
</div>
<p>As our mini experiment demonstrates, discreteness can be combatted
with statistical co-occurrence: words with similar meaning will occur
around similar concepts, giving us an idea of similarity that has
nothing to do with syntactic or lexical form of words. This is the core
idea behind most modern semantic representation models in NLP.</p>
<div id="nlp-libraries" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="nlp-libraries" class="callout-inner">
<h3 class="callout-title">NLP Libraries</h3>
<div class="callout-content">
<p>Related to the need of shaping our problems into a known task, there
are several existing NLP libraries which provide a wide range of models
that we can use out-of-the-box (without further need of modification).
We already saw simple examples using SpaCy for English and jieba for
Chinese. Again, as a non-exhaustive list, we mention some widely used
NLP libraries in Python:</p>
<ul>
<li><a href="https://github.com/nltk/nltk" class="external-link">NLTK</a></li>
<li><a href="https://github.com/explosion/spaCy" class="external-link">spaCy</a></li>
<li><a href="https://github.com/RaRe-Technologies/gensim" class="external-link">Gensim</a></li>
<li><a href="https://github.com/stanfordnlp/stanza" class="external-link">Stanza</a></li>
<li><a href="https://github.com/flairNLP/flair" class="external-link">Flair</a></li>
<li><a href="https://github.com/facebookresearch/fastText" class="external-link">FastText</a></li>
<li><a href="https://github.com/huggingface/transformers" class="external-link">HuggingFace
Transformers</a></li>
</ul>
</div>
</div>
</div>
<div id="linguistic-resources" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="linguistic-resources" class="callout-inner">
<h3 class="callout-title">Linguistic Resources</h3>
<div class="callout-content">
<p>There are also several curated resources (textual data) that can help
solve your NLP-related tasks, specifically when you need highly
specialized definitions. An exhaustive list would be impossible as there
are thousands of them, and also them being language and domain
dependent. Below we mention some of the most prominent, just to give you
an idea of the kind of resources you can find, so you don’t need to
reinvent the wheel every time you start a project:</p>
<ul>
<li>
<a href="https://huggingface.co/datasets" class="external-link">HuggingFace Datasets</a>:
A large collection of datasets for NLP tasks, including text
classification, question answering, and language modeling.</li>
<li>
<a href="https://wordnet.princeton.edu/" class="external-link">WordNet</a>: A large
lexical database of English, where words are grouped into sets of
synonyms (synsets) and linked by semantic relations.</li>
<li>
<a href="https://www.europarl.europa.eu/ep-search/search.do?language=en" class="external-link">Europarl</a>:
A parallel corpus of the proceedings of the European Parliament,
available in 21 languages, which can be used for machine translation and
cross-lingual NLP tasks.</li>
<li>
<a href="https://universaldependencies.org/" class="external-link">Universal
Dependencies</a>: A collection of syntactically annotated treebanks
across 100+ languages, providing a consistent annotation scheme for
syntactic and morphological properties of words, which can be used for
cross-lingual NLP tasks.</li>
<li>
<a href="https://propbank.github.io/" class="external-link">PropBank</a>: A corpus of
texts annotated with information about basic semantic propositions,
which can be used for English semantic tasks.</li>
<li>
<a href="https://framenet.icsi.berkeley.edu/fndrupal/" class="external-link">FrameNet</a>:
A lexical resource that provides information about the semantic frames
that underlie the meanings of words (mainly verbs and nouns), including
their roles and relations.</li>
<li>
<a href="https://babelnet.org/" class="external-link">BabelNet</a>: A multilingual lexical
resource that combines WordNet and Wikipedia, providing a large number
of concepts and their relations in multiple languages.</li>
<li>
<a href="https://www.wikidata.org/" class="external-link">Wikidata</a>: A free and open
knowledge base initially derived from Wikipedia, that contains
structured data about entities, their properties and relations, which
can be used to enrich NLP applications.</li>
<li>
<a href="https://github.com/allenai/dolma" class="external-link">Dolma</a>: An open
dataset of 3 trillion tokens from a diverse mix of clean web content,
academic publications, code, books, and encyclopedic materials, used to
train English large language models.</li>
</ul>
</div>
</div>
</div>
<p>What did we learn in this lesson?</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<span class="callout-header">Key Points</span>
<div class="callout-inner">
<div class="callout-content">
<ul>
<li><p>NLP is a subfield of Artificial Intelligence (AI) that, using the
help of Linguistics, deals with approaches to process, understand and
generate natural language</p></li>
<li><p>Linguistic Data has special properties that we should consider
when modeling our solutions</p></li>
<li><p>Key tasks include language modeling, text classification, token
classification and text generation</p></li>
<li><p>Deep learning has significantly advanced NLP, but the challenge
remains in processing the discrete and ambiguous nature of
language</p></li>
<li><p>The ultimate goal of NLP is to enable machines to understand and
process language as humans do</p></li>
</ul>
</div>
</div>
</div>
</div>
</section></section><section id="aio-02-word_representations"><p>Content from <a href="02-word_representations.html">From words to vectors</a></p>
<hr>
<p>Last updated on 2025-11-27 |

        <a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/edit/main/episodes/02-word_representations.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>Why do we need to preprocess text in NLP?</li>
<li>What are the most common preprocessing operations and in which
contexts should each be used?</li>
<li>What properties do word embeddings have?</li>
<li>What is a word2vec model?</li>
<li>What insights can I get from word embeddings?</li>
<li>How do I train my own word2vec model?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<p>After following this lesson, learners will be able to:</p>
<ul>
<li>Perform basic NLP preprocessing operations</li>
<li>Implement a basic NLP Pipeline</li>
<li>Explain the motivation for vectorization in modern NLP</li>
<li>Train a custom Word2Vec model using the <a href="https://radimrehurek.com/gensim/" class="external-link">Gensim</a> library</li>
<li>Apply a Word2Vec model to interpret and analyze semantics of text
(either a pre-trained model or custom model)</li>
<li>Describe the kinds of semantic relationships captured by Word2Vec,
and identify NLP tasks it is suited to support</li>
<li>Explain, with examples, what the limitations are for the Word2Vec
representation</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<hr class="half-width">
<p>In this episode, we will learn about the importance of preprocessing
text in NLP, and how to apply common preprocessing operations to text
files. We will also learn more about <em>NLP Pipelines</em>, learn about
their basic components and how to construct such pipelines.</p>
<p>We will then address the transition from rule-based NLP to
distributional semantics approaches which encode text into numerical
representations based on statistical relationships between tokens. We
will introduce one particular algorithm for this kind of encoding called
Word2Vec proposed in 2013 by <a href="https://arxiv.org/pdf/1301.3781" class="external-link">Mikolov et al</a>. We will show
what kind of useful semantic relationships these representations encode
in text, and how we can use them to solve specific NLP tasks. We will
also discuss some of the limitations of Word2Vec which are addressed in
the next lesson on transformers before concluding with a summary of what
we covered in this lesson.</p>
</section><section><h2 class="section-heading" id="preprocessing-operations">Preprocessing Operations<a class="anchor" aria-label="anchor" href="#preprocessing-operations"></a>
</h2>
<hr class="half-width">
<p>As is common in data science and machine learning, raw textual data
often comes in a form that is not readily suitable for downstream NLP
tasks. Preprocessing operations in NLP are analogous to the data
cleaning and sanitation steps in traditional non-NLP Machine Learning
tasks. Sometimes you are extracting text from PDF files which contain
line breaks, headers, tables etc. that are not relevant to NLP tasks and
which need to be removed. You may need to remove punctuation and special
characters, or lowercase text for some NLP tasks etc. Whether you need
to perform certain preprocessing operations, and the order in which you
should perform them, will depend on the NLP task at hand.</p>
<p>Also note that preprocessing can differ significantly if you work
with different languages. This is both in terms of which steps to apply,
but also which methods to use for a specific step.</p>
<p>Here we will analyze with more detail the most common pre-processing
steps when dealing with unstructured English text data:</p>
<div class="section level3">
<h3 id="data-formatting">Data Formatting<a class="anchor" aria-label="anchor" href="#data-formatting"></a>
</h3>
<p>Text comes from various sources and are available in different
formats (e.g., Microsoft Word documents, PDF documents, ePub files,
plain text files, Web pages etc…). The first step is to obtain a clean
text representation that can be transferred into python UTF-8 strings
that our scripts can manipulate.</p>
<p>Take a look at the
<code>data/84_frankenstein_or_the_modern_prometheus.txt</code> file:</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a>filename <span class="op">=</span> <span class="st">"data/84_frankenstein_or_the_modern_prometheus.txt"</span></span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(filename, <span class="st">'r'</span>, encoding<span class="op">=</span><span class="st">'utf-8'</span>) <span class="im">as</span> <span class="bu">file</span>:</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a>    text <span class="op">=</span> <span class="bu">file</span>.read()</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="bu">print</span>(text[:<span class="dv">300</span>]) <span class="co"># print the first 300 characters</span></span></code></pre>
</div>
<p>Our file is already in plain text so it might seem we do not need to
do anything; however, if we look closer we see new line characters
separating not only paragraphs but breaking the lines in the middle of
sentences. While this is useful to keep the text in a narrow space to
help the human reader, it introduces artificial breaks that can confuse
any automatic analysis (for example to identify where sentences start
and end).</p>
<p>One straightforward way is to replace the new lines with spaces so
all the text is in a single line:</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>text_flat <span class="op">=</span> text.replace(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span>, <span class="st">" "</span>)</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a><span class="bu">print</span>(text_flat[:<span class="dv">300</span>]) <span class="co"># print the first 300 characters</span></span></code></pre>
</div>
<p>Other data formatting operations might include: - Removal of special
or noisy characters. For example:</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">SH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode sh" tabindex="0"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="ex">-</span> Random symbols: <span class="st">"The total cost is </span><span class="va">$1</span><span class="st">20.00#"</span> → remove <span class="co">#</span></span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a><span class="ex">-</span> Incorrectly recognized letters or numbers: 1 misread as l, 0 as O, etc. Example: <span class="st">"l0ve"</span> → should be <span class="st">"love"</span></span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a><span class="ex">-</span> Control or formatting characters: <span class="dt">\n</span>, <span class="dt">\t</span>, <span class="dt">\r</span> appearing in the middle of sentences. Example: <span class="st">"Please\nsubmit\tyour form."</span> → <span class="st">"Please submit your form."</span></span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a><span class="ex">-</span> Non-standard Unicode characters: �, �, or other placeholder symbols where OCR failed. Example: <span class="st">"Th� quick brown fox"</span> → <span class="st">"The quick brown fox"</span></span></code></pre>
</div>
<ul>
<li>Remove HTML tags (e.g., if you are extracting text from Web
pages)</li>
<li>Strip non-meaningful punctuation (e.g., “The quick brown fox jumps
over the lazy dog and con- tinues to run across the field.)</li>
<li>Strip footnotes, headers, tables, images etc.</li>
<li>Remove URLs or phone numbers</li>
</ul>
<div id="callout1" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div class="callout-inner">
<div class="callout-content">
<p>What if I need to extract text from MS Word docs or PDF files or Web
pages There are various Python libraries for helping you extract and
manipulate text from these kinds of sources.</p>
<ul>
<li>For MS Word documents <a href="https://python-docx.readthedocs.io/en/latest/" class="external-link">python-docx</a> is
popular.</li>
<li>For (text-based) PDF files <a href="https://pypi.org/project/PyPDF2/" class="external-link">PyPDF2</a> and <a href="https://pymupdf.readthedocs.io/en/latest/" class="external-link">PyMuPDF</a> are widely
used. Note that some PDF files are encoded as images (pixels) and not
text. If the text in these files is digital (as opposed to scanned
handwriting), you can use OCR (Optical Character Recognition) libraries
such as <a href="https://pypi.org/project/pytesseract/" class="external-link">pytesseract</a>
to convert the image to machine-readable text.</li>
<li>For scraping text from websites, <a href="https://pypi.org/project/beautifulsoup4/" class="external-link">BeautifulSoup</a> and <a href="https://docs.scrapy.org/en/latest/" class="external-link">Scrapy</a> are some common
options.</li>
<li>LLMs also have something to offer here, and the field is moving
pretty fast. There are some interesting open source LLM-based document
parsers and OCR-like extractors such as <a href="https://github.com/datalab-to/marker" class="external-link">Marker</a>, or <a href="https://github.com/pymupdf/PyMuPDF4LLM" class="external-link">PyMuPDF4LLM</a>, just to
mention a couple.</li>
</ul>
</div>
</div>
</div>
<div id="callout2" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div class="callout-inner">
<div class="callout-content">
<p>Another important choice at the data formatting level is to decide at
what granularity do you need to perform the NLP task:</p>
<ul>
<li>Are you analyzing phenomena at the <strong>word level</strong>? For
example, detecting abusive language (based on a known vocabulary).</li>
<li>Do you need to first extract sentences from the text and do analysis
at the <strong>sentence level</strong>? For example, extracting entities
in each sentence.</li>
<li>Do you need full <strong>chunks of text</strong>? (e.g. paragraphs
or chapters?) For example, summarizing each paragraph in a
document.</li>
<li>Or perhaps you want to extract patterns at the <strong>document
level</strong>? For example each full book should have one genre tag
(Romance, History, Poetry).</li>
</ul>
<p>Sometimes your data will be already available at the desired
granularity level. If this is not the case, then during the tokenization
step you will need to figure out how to obtain the desired granularity
level.</p>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="tokenization">Tokenization<a class="anchor" aria-label="anchor" href="#tokenization"></a>
</h3>
<p>Tokenization is a foundational operation in NLP, as it helps to
create structure from raw text. This structure is a basic requirement
and input for modern NLP algorithms to attribute and interpret meaning
from text. This operation involves the segmentation of the text into
smaller units referred to as <code>tokens</code>. Tokens can be
sentences (e.g. <code>'the happy cat'</code>), words
(<code>'the', 'happy', 'cat'</code>), subwords
(<code>'un', 'happiness'</code>) or characters
(<code>'c','a', 't'</code>). Different NLP algorithms may require
different choices for the token unit. And different languages may
require different approaches to identify or segment these tokens.</p>
<p>Python strings are by definition sequences of characters, thus we can
iterate through the string character by character:</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">type</span>(text_flat))  <span class="co"># Should be &lt;class 'str'&gt;</span></span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a><span class="cf">for</span> ch <span class="kw">in</span> text_flat:</span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>    <span class="bu">print</span>(ch)</span></code></pre>
</div>
<p>However, it is often more advantageous if our atomic units are words.
As we saw in Lesson 1, the task of extracting word tokens from texts is
not trivial, therefore pre-trained models such as spaCy can help with
this step. In this case we will use the small English model that was
trained on a web corpus:</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="im">import</span> spacy</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>nlp <span class="op">=</span> spacy.load(<span class="st">"en_core_web_sm"</span>)</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a>doc <span class="op">=</span> nlp(text)</span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">type</span>(doc))  <span class="co"># Should be &lt;class 'spacy.tokens.doc.Doc'&gt;</span></span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(doc))</span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a><span class="bu">print</span>(doc)</span></code></pre>
</div>
<div id="callout3" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div class="callout-inner">
<div class="callout-content">
<p>A good word tokenizer for example, does not simply break up a text
based on spaces and punctuation, but it should be able to
distinguish:</p>
<ul>
<li>abbreviations that include points (e.g.: <em>e.g.</em>)</li>
<li>times (<em>11:15</em>) and dates written in various formats
(<em>01/01/2024</em> or <em>01-01-2024</em>)</li>
<li>word contractions such as <em>don’t</em>, these should be split into
<em>do</em> and <em>n’t</em>
</li>
<li>URLs</li>
</ul>
<p>Many older tokenizers are rule-based, meaning that they iterate over
a number of predefined rules to split the text into tokens, which is
useful for splitting text into word tokens for example. Modern large
language models use subword tokenization, which learn to break text into
pieces that are statistically convenient, this makes them more flexible
but less human-readable.</p>
</div>
</div>
</div>
<p>We can access the tokens by iterating the document and getting its
<code>.text</code> property:</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a>tokens_txt <span class="op">=</span> [token.text <span class="cf">for</span> token <span class="kw">in</span> doc]</span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a><span class="bu">print</span>(tokens_txt[:<span class="dv">15</span>])</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>['Letter', '1', '\n\n\n', 'St.', 'Petersburgh', ',', 'Dec.', '11th', ',', '17', '-', '-', '\n\n', 'TO', 'Mrs.']</code></pre>
</div>
<p>This shows us the individual tokens, including new lines and
punctuation (in case we didn’t run the previous cleaning step). spaCy
allows us to filter based on token properties. For example, assuming we
are not interested in the newlines, punctuation nor in numeric tokens,
in one single step we can keep only the token objects that contain
alphabetical characters:</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a>tokens <span class="op">=</span> [token <span class="cf">for</span> token <span class="kw">in</span> doc <span class="cf">if</span> token.is_alpha]</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a><span class="bu">print</span>(tokens[:<span class="dv">15</span>])</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[Letter, Petersburgh, TO, Saville, England, You, will, rejoice, to, hear, that, no, disaster, has, accompanied]</code></pre>
</div>
<p>We do not have to depend necessarily on the <code>Doc</code> and
<code>Token</code> spaCy objects. Once we tokenized the text with the
spaCy model, we can extract the list of words as a list of strings and
continue our text analysis:</p>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a>words <span class="op">=</span> [token.text <span class="cf">for</span> token <span class="kw">in</span> doc]</span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a><span class="bu">print</span>(words[:<span class="dv">20</span>])</span></code></pre>
</div>
<p>Again, it all depends on what your requirements are. For example,
sometimes it is more useful if our atomic units are sentences. Think of
the NLP task of classifying each whole sentence inside a text as
Positive/Negative/Neutral in terms of sentiment (e.g., within movie
reviews). spaCy also helps with this using a sentence segmentation
model:</p>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a>sentences <span class="op">=</span> [sent.text <span class="cf">for</span> sent <span class="kw">in</span> doc.sents]</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a>[<span class="bu">print</span>(s) <span class="cf">for</span> s <span class="kw">in</span> sentences[:<span class="dv">5</span>]]</span></code></pre>
</div>
<p>Note that in this case each sentence is a python object, and the
property <code>.text</code> returns an untokenized string (in terms of
words). But we can still access the list of word tokens inside each
sentence object if we want:</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a>sents_sample <span class="op">=</span> <span class="bu">list</span>(doc.sents)[:<span class="dv">10</span>]</span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a><span class="cf">for</span> sent <span class="kw">in</span> sents_sample:</span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Sentence:"</span>, sent.text)</span>
<span id="cb12-4"><a href="#cb12-4" tabindex="-1"></a>    <span class="cf">for</span> token <span class="kw">in</span> sent:</span>
<span id="cb12-5"><a href="#cb12-5" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"</span><span class="ch">\t</span><span class="st">Token:"</span>, token.text)</span></code></pre>
</div>
<p>This will give us enough flexibility to work at the sentence and word
level at the same time. In terms of what we can do with these sentences
once spaCy has identified them, we could ask humans to label each
sentence as either Positive/Negative/Neutral and train a supervised
model for sentiment classification on the set of sentences. Or if we
have a pre-trained model for sentiment classification on sentences, we
could load this model and classify each of our input sentences as either
Positive/Negative/Neutral.</p>
</div>
<div class="section level3">
<h3 id="lowercasing">Lowercasing<a class="anchor" aria-label="anchor" href="#lowercasing"></a>
</h3>
<p>Removing uppercases to e.g. avoid treating “Dog” and “dog” as two
different words is also a common step, for example to train word vector
representations, we want to merge both occurrences as they represent
exactly the same concept. Lowercasing can be done with Python directly
as:</p>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a>lower_text <span class="op">=</span> text_flat.lower()</span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a>lower_text[:<span class="dv">100</span>] <span class="co"># Beware that this is a python string operation</span></span></code></pre>
</div>
<p>Beware that lowercasing the whole string as a first step might affect
the tokenizer behavior since tokenization benefits from information
provided by case-sensitive strings. We can therefore tokenize first
using spaCy and then obtain the lowercase strings of each token using
the <code>.lower_</code> property:</p>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a>lower_text <span class="op">=</span> [token.lower_ <span class="cf">for</span> token <span class="kw">in</span> doc]</span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a>lower_text[:<span class="dv">10</span>] <span class="co"># Beware that this is a list of strings now!</span></span></code></pre>
</div>
<p>In other tasks, such as Named Entity Recognition (NER), lowercasing
before training can actually lower the performance of your model. This
is because words that start with an uppercase (not preceded by a period)
can represent proper nouns that map into Entities, for example:</p>
<div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a><span class="co"># Preserving uppercase characters increases the likelihood that an NER model</span></span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a><span class="co"># will correctly identify Apple and Will as a company (ORG) and a person (PER)</span></span>
<span id="cb15-3"><a href="#cb15-3" tabindex="-1"></a><span class="co"># respectively.</span></span>
<span id="cb15-4"><a href="#cb15-4" tabindex="-1"></a>str1 <span class="op">=</span> <span class="st">"My next laptop will be from Apple, Will said."</span> </span>
<span id="cb15-5"><a href="#cb15-5" tabindex="-1"></a><span class="co"># Lowercasing can reduce the likelihood of accurate labeling</span></span>
<span id="cb15-6"><a href="#cb15-6" tabindex="-1"></a>str2 <span class="op">=</span> <span class="st">"my next laptop will be from apple, will said."</span></span></code></pre>
</div>
</div>
<div class="section level3">
<h3 id="lemmatization">Lemmatization<a class="anchor" aria-label="anchor" href="#lemmatization"></a>
</h3>
<p>Although it has become less widely used in modern NLP approaches,
normalizing words into their <em>dictionary form</em> can help to focus
on relevant aspects of text. Consider how “eating”, “ate”, “eaten” are
all variations of the root verb “eat”. Each variation is sometimes known
as an <em>inflection</em> of the root word. Conversely, we say that the
word “eat” is the <em>lemma</em> for the words “eating”, “eats”,
“eaten”, “ate” etc. Lemmatization is therefore the process of rewriting
each token or word in a given input text as its lemma.</p>
<p>Lemmatization is not only a possible preprocessing step in NLP but
also an NLP task on its own, with different algorithms for it. Therefore
we also tend to use pre-trained models to perform lemmatization. Using
spaCy we can access the lemmmatized version of each token with the
<code>lemma_</code> property (notice the underscore!):</p>
<div class="codewrapper sourceCode" id="cb16">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a>lemmas <span class="op">=</span> [token.lemma_ <span class="cf">for</span> token <span class="kw">in</span> doc]</span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a><span class="bu">print</span>(lemmas)</span></code></pre>
</div>
<p>Note that the list of lemmas is now a list of strings.</p>
<p>Having a lemmatized text allows us to merge the different surface
occurrences of the same concept into a single token. This can be very
useful for count-based NLP methods such as topic modelling approaches
which count the frequency of certain words to see how prevalent a given
topic is within a document. If you condense “eat”, “eating”, “ate”,
“eaten” to the same token “eat” then you can count four occurrences of
the same “topic” in a text, instead of treating these four tokens as
distinct or unrelated topics just because they are spelled differently.
You can also use lemmatization for generating word embeddings. For
example, you can have a single vector for <code>eat</code> instead of
one vector per verb tense.</p>
<p>As with each preprocessing operation, this step is optional.
Consider, for example, the cases where the differences of verb usage
according to tense is informative, or the difference between singular
and plural usage of nouns, in those cases lemmatizing will get rid of
important information for your task. For example, if your chosen NLP
task is to detect past tense verbs from a document, then lemmatizing
“eaten” into “eat” loses crucial information about tense that your model
requires.</p>
</div>
<div class="section level3">
<h3 id="stop-word-removal">Stop Word Removal<a class="anchor" aria-label="anchor" href="#stop-word-removal"></a>
</h3>
<p>The most frequent words in texts are those which contribute little
semantic value on their own: articles (‘the’, ‘a’, ‘an’), conjunctions
(‘and’, ‘or’, ‘but’), prepositions (‘on’, ‘by’), auxiliary verbs (‘is’,
‘am’), pronouns (‘he’, ‘which’), or any highly frequent word that might
not be of interest in several <em>content-only</em> related tasks. Let’s
define a small list of stop words for this specific case:</p>
<div class="codewrapper sourceCode" id="cb17">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a>STOP_WORDS <span class="op">=</span> [<span class="st">"the"</span>, <span class="st">"you"</span>, <span class="st">"will"</span>] <span class="co"># This list can be customized to your needs...</span></span></code></pre>
</div>
<p>Using Python directly, we need to manually define a list of what we
consider to be stop words and directly filter the tokens that match
this. Notice that lemmatization was a crucial step to get more coverage
with the stop word filtering:</p>
<div class="codewrapper sourceCode" id="cb18">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" tabindex="-1"></a>lemmas <span class="op">=</span> [token.lemma_ <span class="cf">for</span> token <span class="kw">in</span> doc]</span>
<span id="cb18-2"><a href="#cb18-2" tabindex="-1"></a>content_words <span class="op">=</span> []</span>
<span id="cb18-3"><a href="#cb18-3" tabindex="-1"></a><span class="cf">for</span> lemma <span class="kw">in</span> lemmas:</span>
<span id="cb18-4"><a href="#cb18-4" tabindex="-1"></a>    <span class="cf">if</span> lemma <span class="kw">not</span> <span class="kw">in</span> STOP_WORDS:</span>
<span id="cb18-5"><a href="#cb18-5" tabindex="-1"></a>        content_words.append(lemma)</span>
<span id="cb18-6"><a href="#cb18-6" tabindex="-1"></a><span class="bu">print</span>(content_words[:<span class="dv">20</span>])</span></code></pre>
</div>
<p>Using spaCy we can filter the stop words based on the token
properties:</p>
<div class="codewrapper sourceCode" id="cb19">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" tabindex="-1"></a>tokens_nostop <span class="op">=</span> [token <span class="cf">for</span> token <span class="kw">in</span> tokens <span class="cf">if</span> <span class="kw">not</span> token.is_stop]</span>
<span id="cb19-2"><a href="#cb19-2" tabindex="-1"></a><span class="bu">print</span>(tokens[:<span class="dv">15</span>])</span></code></pre>
</div>
<p>There is no canonical definition of stop words because what you
consider to be a stop word is directly linked to the objective of your
task at hand. For example, pronouns are usually considered stopwords,
but if you want to do gender bias analysis then pronouns are actually a
key element of your text processing pipeline. Similarly, removing
articles such as prepositions from text is obviously not advised if you
are doing <em>dependency parsing</em> (the task of identifying the parts
of speech in a given text).</p>
<p>Another special case is the word ‘not’ which may encode the semantic
notion of <em>negation</em>. Removing such tokens can drastically change
the meaning of sentences and therefore affect the accuracy of models for
which negation is important to preserve (e.g., sentiment classification
“this movie was NOT great” vs. “this movie was great”).</p>
</div>
</section><section><h2 class="section-heading" id="nlp-pipeline">NLP Pipeline<a class="anchor" aria-label="anchor" href="#nlp-pipeline"></a>
</h2>
<hr class="half-width">
<p>The concept of NLP pipeline refers to the sequence of operations that
we apply to our data in order to go from the original data
(e.g. original raw documents) to the expected outputs of our NLP Task at
hand. The components of the pipeline refer to any manipulation we apply
to the text, and do not necessarily need to be complex models, they
involve preprocessing operations, application of rules or machine
learning models, as well as formatting the outputs in a desired way.</p>
<div class="section level3">
<h3 id="a-simple-rule-based-classifier">A simple rule-based classifier<a class="anchor" aria-label="anchor" href="#a-simple-rule-based-classifier"></a>
</h3>
<p>Imagine we want to build a very lightweight sentiment classifier. A
basic approach is to design the following pipeline:</p>
<ol style="list-style-type: decimal">
<li>Clean the original text file (as we saw in the Data Formatting
section)</li>
<li>Apply a sentence segmentation or tokenization model</li>
<li>Define a set of positive and negative words (a hard coded
dictionary)</li>
<li>For each sentence:
<ul>
<li>If it contains one or more of the positive words, classify as
<code>POSITIVE</code>
</li>
<li>If it contains one or more of the negative words, classify as
<code>NEGATIVE</code>
</li>
<li>Otherwise classify as <code>NEUTRAL</code>
</li>
</ul>
</li>
<li>Output a table with the original sentence and the assigned
label</li>
</ol>
<p>This is implemented with the following code:</p>
<ol style="list-style-type: decimal">
<li>Read the text and normalize it into a single line</li>
</ol>
<div class="codewrapper sourceCode" id="cb20">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" tabindex="-1"></a><span class="im">import</span> spacy</span>
<span id="cb20-2"><a href="#cb20-2" tabindex="-1"></a>nlp <span class="op">=</span> spacy.load(<span class="st">"en_core_web_sm"</span>)</span>
<span id="cb20-3"><a href="#cb20-3" tabindex="-1"></a></span>
<span id="cb20-4"><a href="#cb20-4" tabindex="-1"></a>filename <span class="op">=</span> <span class="st">"data/84_frankenstein_or_the_modern_prometheus.txt"</span></span>
<span id="cb20-5"><a href="#cb20-5" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(filename, <span class="st">'r'</span>, encoding<span class="op">=</span><span class="st">'utf-8'</span>) <span class="im">as</span> <span class="bu">file</span>:</span>
<span id="cb20-6"><a href="#cb20-6" tabindex="-1"></a>    text <span class="op">=</span> <span class="bu">file</span>.read()</span>
<span id="cb20-7"><a href="#cb20-7" tabindex="-1"></a></span>
<span id="cb20-8"><a href="#cb20-8" tabindex="-1"></a>text <span class="op">=</span> text.replace(<span class="st">"</span><span class="ch">\n</span><span class="st">"</span>, <span class="st">" "</span>) <span class="co"># some cleaning by removing new line characters</span></span></code></pre>
</div>
<ol start="2" style="list-style-type: decimal">
<li>Apply Sentence segmentation</li>
</ol>
<div class="codewrapper sourceCode" id="cb21">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" tabindex="-1"></a>doc <span class="op">=</span> nlp(text)</span>
<span id="cb21-2"><a href="#cb21-2" tabindex="-1"></a>sentences <span class="op">=</span> [sent.text <span class="cf">for</span> sent <span class="kw">in</span> doc.sents]</span></code></pre>
</div>
<ol start="3" style="list-style-type: decimal">
<li>Define the positive and negative words you care about:</li>
</ol>
<div class="codewrapper sourceCode" id="cb22">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" tabindex="-1"></a>positive_words <span class="op">=</span> [<span class="st">"happy"</span>, <span class="st">"excited"</span>, <span class="st">"delighted"</span>, <span class="st">"content"</span>, <span class="st">"love"</span>, <span class="st">"enjoyment"</span>]</span>
<span id="cb22-2"><a href="#cb22-2" tabindex="-1"></a>negative_words <span class="op">=</span> [<span class="st">"unhappy"</span>, <span class="st">"sad"</span>, <span class="st">"anxious"</span>, <span class="st">"miserable"</span>, <span class="st">"fear"</span>, <span class="st">"horror"</span>]</span></code></pre>
</div>
<ol start="4" style="list-style-type: decimal">
<li>Apply the rules to each sentence and collect the labels</li>
</ol>
<div class="codewrapper sourceCode" id="cb23">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" tabindex="-1"></a>classified_sentences <span class="op">=</span> []</span>
<span id="cb23-2"><a href="#cb23-2" tabindex="-1"></a></span>
<span id="cb23-3"><a href="#cb23-3" tabindex="-1"></a><span class="cf">for</span> sent <span class="kw">in</span> sentences:</span>
<span id="cb23-4"><a href="#cb23-4" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">any</span>(word <span class="kw">in</span> sent.lower() <span class="cf">for</span> word <span class="kw">in</span> positive_words):</span>
<span id="cb23-5"><a href="#cb23-5" tabindex="-1"></a>        classified_sentences.append((sent, <span class="st">'POSITIVE'</span>))</span>
<span id="cb23-6"><a href="#cb23-6" tabindex="-1"></a>    <span class="cf">elif</span> <span class="bu">any</span>(word <span class="kw">in</span> sent.lower() <span class="cf">for</span> word <span class="kw">in</span> negative_words):</span>
<span id="cb23-7"><a href="#cb23-7" tabindex="-1"></a>        classified_sentences.append((sent, <span class="st">'NEGATIVE'</span>))</span>
<span id="cb23-8"><a href="#cb23-8" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb23-9"><a href="#cb23-9" tabindex="-1"></a>        classified_sentences.append((sent, <span class="st">'NEUTRAL'</span>))</span></code></pre>
</div>
<ol start="5" style="list-style-type: decimal">
<li>Save the classified data</li>
</ol>
<div class="codewrapper sourceCode" id="cb24">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb24-2"><a href="#cb24-2" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(classified_sentences, columns<span class="op">=</span>[<span class="st">'sentence'</span>, <span class="st">'label'</span>])</span>
<span id="cb24-3"><a href="#cb24-3" tabindex="-1"></a>df.to_csv(<span class="st">'results_naive_rule_classifier.csv'</span>, sep<span class="op">=</span><span class="st">'</span><span class="ch">\t</span><span class="st">'</span>)</span></code></pre>
</div>
<div id="challenge1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div class="callout-inner">
<h3 class="callout-title">Challenge</h3>
<div class="callout-content">
<p>Discuss the pros and cons of the proposed NLP pipeline:</p>
<ol style="list-style-type: decimal">
<li>Do you think it will give accurate results?</li>
<li>What do you think about the coverage of this approach? What cases
will it miss?</li>
<li>Think of possible drawbacks of chaining components in a
pipeline.</li>
</ol>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<ol style="list-style-type: decimal">
<li>This classifier only considers the presence of one word to apply a
label. It does not analyze sentence semantics or even syntax.</li>
<li>Given how the rules are defined, if both positive and negative words
are present in the same sentence it will assign the
<code>POSITIVE</code> label. It will generate a lot of false positives
because of the simplistic rules</li>
<li>The errors from previous steps get carried over to the next steps
increasing the likelihood of noisy outputs.</li>
</ol>
</div>
</div>
</div>
</div>
<p>So far we’ve seen how to format and segment the text to have atomic
data at the word level or sentence level. We then apply operations to
the word and sentence strings. This approach still depends on counting
and exact keyword matching. And as we have already seen it has several
limitations. The method cannot interpret words outside the dictionary
defined for example.</p>
<p>One way to combat this is by transforming each word into numeric
representation and study statistical patterns in how these words are
distributed in text. For example, what words tend to occur “close” to a
given word in my data? For example, if we analyze restaurant menus we
find that “cheese”, “mozzarella”, “base” etc. frequently occur near the
token “pizza”. We can then exploit these statistical patterns to inform
various NLP tasks. This concept is commonly known as <a href="https://arxiv.org/pdf/1905.01896" class="external-link">distributional semantics</a>. It
is based on the assumption “words that appear in similar contexts have
similar meanings.”</p>
<p>This concept is powerful for enabling, for example, the measurement
of semantic similarity of words, sentences, phrases etc. in text. And
this, in turn, can help with other downstream NLP tasks, as we shall see
in the next section on word embeddings.</p>
</div>
</section><section><h2 class="section-heading" id="word-embeddings">Word Embeddings<a class="anchor" aria-label="anchor" href="#word-embeddings"></a>
</h2>
<hr class="half-width">
<div class="section level3">
<h3 id="reminder-neural-networks">Reminder: Neural Networks<a class="anchor" aria-label="anchor" href="#reminder-neural-networks"></a>
</h3>
<p>Understanding how neural networks work is out of the scope of this
course. For our purposes we will simplify the explanation in order to
conceptually understand how Neural Network works. A Neural Network (NN)
is a pattern-finding machine with layers (a <em>deep</em> neural network
is the same concept but scaled to dozens or even hundreds of layers). In
a neural network, each layer has several interconnected
<em>neurons</em>, each one corresponding to a random number initially.
The deeper the network is, the more complex patterns it can learn. As
the neural netork gets trained (that is, as it sees several labeled
examples that we provide), each neuron value will be updated in order to
maximize the probability of getting the answers right. A well trained
neural network will be able to predict the right labels on completely
new data with certain accuracy.</p>
<figure><img src="fig/emb_neuralnet.png" alt="After seeing thousands of examples, each layer represents different “features” that maximize the success of the task, but they are not human-readable. The last layer acts as a classifier and outputs the most likely label given the input" class="figure mx-auto d-block"><div class="figcaption">After seeing thousands of examples, each layer
represents different “features” that maximize the success of the task,
but they are not human-readable. The last layer acts as a classifier and
outputs the most likely label given the input</div>
</figure><p>The main difference with traditional machine learning models is that
we do not need to design explicitly any features, rather the network
will <em>adjust itself</em> by looking at the data alone and executing
the back-propagation algorithm. The main job when using NNs is to encode
our data properly so it can be fed into the network.</p>
</div>
<div class="section level3">
<h3 id="rationale-behind-embeddings">Rationale behind Embeddings<a class="anchor" aria-label="anchor" href="#rationale-behind-embeddings"></a>
</h3>
<p><strong>A word embedding is a numeric vector that represents a
word</strong>. Word2Vec exploits the “feature agnostic” power of neural
networks to transform word strings into trained word numeric
representations. Hence we still use words as features but instead of
using the string directly, we transform that string into its
corresponding vector in the pre-trained Word2Vec model. And because both
the network input and output are the words themselves in text, we
basically have billions of <em>labeled</em> training datapoints for
free.</p>
<figure><img src="fig/emb_embeddings.png" class="figure mx-auto d-block"></figure><p>To obtained the word embeddings, a shallow neural network is
optimized with the task of language modeling and the final hidden layer
inside the trained network holds the fixed size vectors whose values can
be mapped into linguistic properties (since the training objective was
language modeling). Since similar words occur in similar contexts, or
have same characteristics, a properly trained model will learn to assign
similar vectors to similar words.</p>
<p>By representing words with vectors, we can mathematically manipulate
them through vector arithmetic and express semantic similarity in terms
of vector distance. Because the size of the learned vectors is not
proportional to the amount of documents we can learn the representations
from larger collections of texts, obtaining more robust representations,
that are less corpus-dependent.</p>
<p>There are two main algorithms for training Word2Vec:</p>
<ul>
<li>Continuous Bag-of-Words (CBOW): Predicts a target word based on its
surrounding context words.</li>
<li>Continuous Skip-Gram: Predicts surrounding context words given a
target word.</li>
</ul>
<figure><img src="fig/emb13.png" class="figure mx-auto d-block"></figure><p>If you want to know more about the technicl aspecs of training
Word2Vec you can visit this <a href="https://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/" class="external-link">tutorial</a></p>
</div>
<div class="section level3">
<h3 id="the-word2vec-vector-space">The Word2Vec Vector Space<a class="anchor" aria-label="anchor" href="#the-word2vec-vector-space"></a>
</h3>
<p>The python module <code>gensim</code> offers a user-friendly
interface to interact with pre-trained Word2vec models and also to train
our own. First we will explore the model from the original Word2Vec
paper, which was trained on a big corpus from Google News (English news
articles). We will see what functionalities are available to explore a
vector space. Then we will prepare our own text step-by-step to train
our own Word2vec models and save them.</p>
</div>
<div class="section level3">
<h3 id="load-the-embeddings-and-inspect-them">Load the embeddings and inspect them<a class="anchor" aria-label="anchor" href="#load-the-embeddings-and-inspect-them"></a>
</h3>
<p>The library <code>gensim</code> has a repository with English
pre-trained models. We can take a look at the models:</p>
<div class="codewrapper sourceCode" id="cb25">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" tabindex="-1"></a><span class="im">import</span> gensim.downloader</span>
<span id="cb25-2"><a href="#cb25-2" tabindex="-1"></a>available_models <span class="op">=</span> gensim.downloader.info()[<span class="st">'models'</span>].keys()</span>
<span id="cb25-3"><a href="#cb25-3" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">list</span>(available_models))</span></code></pre>
</div>
<p>We will download the google News model with:</p>
<div class="codewrapper sourceCode" id="cb26">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" tabindex="-1"></a>w2v_model <span class="op">=</span> gensim.downloader.load(<span class="st">'word2vec-google-news-300'</span>)</span></code></pre>
</div>
<p>We can do some basic checkups such as showing how many words are in
the vocabulary (i.e., for how many words do we have an available
vector), what is the total number of dimensions in each vector, and
print the components of a vector for a given word:</p>
<div class="codewrapper sourceCode" id="cb27">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(w2v_model.key_to_index.keys())) <span class="co"># 3 million words</span></span>
<span id="cb27-2"><a href="#cb27-2" tabindex="-1"></a><span class="bu">print</span>(w2v_model.vector_size) <span class="co"># 300 dimensions. This can be chosen when training your own model</span></span>
<span id="cb27-3"><a href="#cb27-3" tabindex="-1"></a><span class="bu">print</span>(w2v_model[<span class="st">'car'</span>][:<span class="dv">10</span>]) <span class="co"># The first 10 dimensions of the vector representing 'car'.</span></span>
<span id="cb27-4"><a href="#cb27-4" tabindex="-1"></a><span class="bu">print</span>(w2v_model[<span class="st">'cat'</span>][:<span class="dv">10</span>]) <span class="co"># The first 10 dimensions of the vector representing 'cat'.</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>3000000
300
[ 0.13085938  0.00842285  0.03344727 -0.05883789  0.04003906 -0.14257812
  0.04931641 -0.16894531  0.20898438  0.11962891]
[ 0.0123291   0.20410156 -0.28515625  0.21679688  0.11816406  0.08300781
  0.04980469 -0.00952148  0.22070312 -0.12597656]</code></pre>
</div>
<p>As we can see, this is a very large model with 3 million words and
the dimensionality chosen at training time was 300, thus each word will
have a 300-dimension vector associated with it.</p>
<p>Even with such a big vocabulary we can always find a word that won’t
be in there:</p>
<div class="codewrapper sourceCode" id="cb29">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" tabindex="-1"></a><span class="bu">print</span>(w2v_model[<span class="st">'bazzinga'</span>][:<span class="dv">10</span>])</span></code></pre>
</div>
<p>This will throw a <code>KeyError</code> as the model does not know
that word. Unfortunately this is a limitation of Word2vec - unseen words
(words that were not included in the training data) cannot be
interpreted by the model.</p>
<p>Now let’s talk about the vectors themselves. They are not easy to
interpret as they are a bunch of floating point numbers. These are the
weights that the network learned when optimizing for language modelling.
As the vectors are hard to interpret, we rely on a mathematical method
to compute how similar two vectors are. Generally speaking, the
recommended metric for measuring similarity between two high-dimensional
vectors is <a href="https://en.wikipedia.org/wiki/Cosine_similarity" class="external-link">cosine
similarity</a> .</p>
<div id="callout4" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div class="callout-inner">
<div class="callout-content">
<p><a href="https://en.wikipedia.org/wiki/Cosine_similarity" class="external-link">cosine
similarity</a> ranges between [<code>-1</code> and <code>1</code>]. It
is the cosine of the angle between two vectors, divided by the product
of their length. Mathematically speaking, when two vectors point in
exactly the same direction their cosine will be 1, and when they point
in the opposite direction their cosine will be -1. In python we can use
Numpy to compute the cosine similarity of vectors.</p>
<figure><img src="fig/emb12.png" alt="" class="figure mx-auto d-block"></figure>
</div>
</div>
</div>
<p>We can use <code>sklearn</code> learn to measure any pair of
high-dimensional vectors:</p>
<div class="codewrapper sourceCode" id="cb30">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" tabindex="-1"></a><span class="im">from</span> sklearn.metrics.pairwise <span class="im">import</span> cosine_similarity</span>
<span id="cb30-2"><a href="#cb30-2" tabindex="-1"></a></span>
<span id="cb30-3"><a href="#cb30-3" tabindex="-1"></a>car_vector <span class="op">=</span> w2v_model[<span class="st">'car'</span>]</span>
<span id="cb30-4"><a href="#cb30-4" tabindex="-1"></a>cat_vector <span class="op">=</span> w2v_model[<span class="st">'cat'</span>]</span>
<span id="cb30-5"><a href="#cb30-5" tabindex="-1"></a></span>
<span id="cb30-6"><a href="#cb30-6" tabindex="-1"></a>similarity <span class="op">=</span> cosine_similarity([car_vector], [cat_vector])</span>
<span id="cb30-7"><a href="#cb30-7" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Cosine similarity between 'car' and 'cat': </span><span class="sc">{</span>similarity[<span class="dv">0</span>][<span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb30-8"><a href="#cb30-8" tabindex="-1"></a></span>
<span id="cb30-9"><a href="#cb30-9" tabindex="-1"></a>similarity <span class="op">=</span> cosine_similarity([w2v_model[<span class="st">'hamburger'</span>]], [w2v_model[<span class="st">'pizza'</span>]])</span>
<span id="cb30-10"><a href="#cb30-10" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Cosine similarity between 'hamburger' and 'pizza': </span><span class="sc">{</span>similarity[<span class="dv">0</span>][<span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb31">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" tabindex="-1"></a></span>
<span id="cb31-2"><a href="#cb31-2" tabindex="-1"></a>Cosine similarity between <span class="st">'car'</span> <span class="kw">and</span> <span class="st">'cat'</span>: <span class="fl">0.21528185904026031</span></span>
<span id="cb31-3"><a href="#cb31-3" tabindex="-1"></a>Cosine similarity between <span class="st">'hamburger'</span> <span class="kw">and</span> <span class="st">'pizza'</span>: <span class="fl">0.6153676509857178</span></span></code></pre>
</div>
<p>Or you can use directly the
<code>w2v_model.similarity('car', 'cat')</code> function which gives the
same result.</p>
<p>The higher similarity score between the hamburger and pizza indicates
they are more similar based on the contexts where they appear in the
training data. Even though is hard to read all the floating numbers in
the vectors, we can trust this metric to always give us a hint of which
words are semantically closer than others</p>
<div id="challenge2" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div class="callout-inner">
<h3 class="callout-title">Challenge</h3>
<div class="callout-content">
<p>Think of different word pairs and try to guess how close or distant
they will be from each other. Use the similarity measure from the
word2vec module to compute the metric and discuss if this fits your
expectations. If not, can you come up with a reason why this was not the
case?</p>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2"> Show me the solution </h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" data-bs-parent="#accordionSolution2" aria-labelledby="headingSolution2">
<div class="accordion-body">
<p>Some interesting cases include synonyms, antonyms and morphologically
related words:</p>
<div class="codewrapper sourceCode" id="cb32">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" tabindex="-1"></a><span class="bu">print</span>(w2v_model.similarity(<span class="st">'democracy'</span>, <span class="st">'democratic'</span>))</span>
<span id="cb32-2"><a href="#cb32-2" tabindex="-1"></a><span class="bu">print</span>(w2v_model.similarity(<span class="st">'queen'</span>, <span class="st">'princess'</span>))</span>
<span id="cb32-3"><a href="#cb32-3" tabindex="-1"></a><span class="bu">print</span>(w2v_model.similarity(<span class="st">'love'</span>, <span class="st">'hate'</span>)) <span class="co">#!! (think of "I love X" and "I hate X")</span></span>
<span id="cb32-4"><a href="#cb32-4" tabindex="-1"></a><span class="bu">print</span>(w2v_model.similarity(<span class="st">'love'</span>, <span class="st">'lover'</span>))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code><span><span class="fl">0.86444813</span></span>
<span><span class="fl">0.7070532</span></span>
<span><span class="fl">0.6003957</span></span>
<span><span class="fl">0.48608577</span></span></code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="vector-neighborhoods">Vector Neighborhoods<a class="anchor" aria-label="anchor" href="#vector-neighborhoods"></a>
</h3>
<p>Now that we have a metric we can trust, we can retrieve neighborhoods
of vectors that are close to a given word. This is analogous to
retrieving semantically related terms to a target term. Let’s explore
the neighborhood around `pizza` using the <code>most_similar()</code>
method:</p>
<div class="codewrapper sourceCode" id="cb34">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" tabindex="-1"></a><span class="bu">print</span>(w2v_model.most_similar(<span class="st">'pizza'</span>, topn<span class="op">=</span><span class="dv">10</span>))</span></code></pre>
</div>
<p>This returns a list of ranked tuples with the form (word,
similarity_score). The list is already ordered in descent, so the first
element is the closest vector in the vector space, the second element is
the second closest word and so on…</p>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[('pizzas', 0.7863470911979675),
('Domino_pizza', 0.7342829704284668),
('Pizza', 0.6988078355789185),
('pepperoni_pizza', 0.6902607083320618),
('sandwich', 0.6840401887893677),
('burger', 0.6569692492485046),
('sandwiches', 0.6495091319084167),
('takeout_pizza', 0.6491535902023315),
('gourmet_pizza', 0.6400628089904785),
('meatball_sandwich', 0.6377009749412537)]</code></pre>
</div>
<p>Exploring neighborhoods can help us understand why some vectors are
closer (or not so much). Take the case of <em>love</em> and
<em>lover</em>, originally we might think these should be very close to
each other but by looking at their neighborhoods we understand why this
is not the case:</p>
<div class="codewrapper sourceCode" id="cb36">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" tabindex="-1"></a><span class="bu">print</span>(w2v_model.most_similar(<span class="st">'love'</span>, topn<span class="op">=</span><span class="dv">10</span>))</span>
<span id="cb36-2"><a href="#cb36-2" tabindex="-1"></a><span class="bu">print</span>(w2v_model.most_similar(<span class="st">'lover'</span>, topn<span class="op">=</span><span class="dv">10</span>))</span></code></pre>
</div>
<p>This returns a list of ranked tuples with the form (word,
similarity_score). The list is already ordered in descent, so the first
element is the closest vector in the vector space, the second element is
the second closest word and so on…</p>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[('loved', 0.6907791495323181), ('adore', 0.6816874146461487), ('loves', 0.6618633270263672), ('passion', 0.6100709438323975), ('hate', 0.6003956198692322), ('loving', 0.5886634588241577), ('Ilove', 0.5702950954437256), ('affection', 0.5664337873458862), ('undying_love', 0.5547305345535278), ('absolutely_adore', 0.5536840558052063)]

[('paramour', 0.6798686385154724), ('mistress', 0.6387110352516174), ('boyfriend', 0.6375402212142944), ('lovers', 0.6339589953422546), ('girlfriend', 0.6140860915184021), ('beau', 0.609399676322937), ('fiancé', 0.5994566679000854), ('soulmate', 0.5993717312812805), ('hubby', 0.5904166102409363), ('fiancée', 0.5888950228691101)]</code></pre>
</div>
<p>The first word is a noun or a verb (depending on the context) that
denotes affection to someone/something , so it is associated with other
concepts of affection (positive or negative). The case of <em>lover</em>
is used to describe a person, hence the associated concepts are
descriptors of people with whom the lover can be associated.</p>
</div>
<div class="section level3">
<h3 id="word-analogies-with-vectors">Word Analogies with Vectors<a class="anchor" aria-label="anchor" href="#word-analogies-with-vectors"></a>
</h3>
<p>Another powerful property that word embeddings show is that vector
algebra can preserve semantic analogy. An analogy is a comparison
between two different things based on their similar features or
relationships, for example king is to queen as man is to woman. We can
mimic this operations directly on the vectors using the
<code>most_similar()</code> method with the <code>positive</code> and
<code>negative</code> parameters:</p>
<div class="codewrapper sourceCode" id="cb38">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" tabindex="-1"></a><span class="co"># king is to man as what is to woman?</span></span>
<span id="cb38-2"><a href="#cb38-2" tabindex="-1"></a><span class="co"># king + woman - man = queen</span></span>
<span id="cb38-3"><a href="#cb38-3" tabindex="-1"></a>w2v_model.most_similar(positive<span class="op">=</span>[<span class="st">'king'</span>, <span class="st">'woman'</span>], negative<span class="op">=</span>[<span class="st">'man'</span>])</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[('queen', 0.7118192911148071),
 ('monarch', 0.6189674735069275),
 ('princess', 0.5902431011199951),
 ('crown_prince', 0.5499460697174072),
 ('prince', 0.5377321243286133),
 ('kings', 0.5236844420433044),
 ('Queen_Consort', 0.5235945582389832),
 ('queens', 0.5181134343147278),
 ('sultan', 0.5098593235015869),
 ('monarchy', 0.5087411403656006)]</code></pre>
</div>
</div>
</section><section><h2 class="section-heading" id="train-your-own-word2vec">Train your own Word2Vec<a class="anchor" aria-label="anchor" href="#train-your-own-word2vec"></a>
</h2>
<hr class="half-width">
<p>The <code>gensim</code> package has implemented everything for us,
this means we have to focus mostly on obtaining clean data and then
calling the <code>Word2Vec</code> object to train our own model with our
own data. This can be done like follows:</p>
<div class="codewrapper sourceCode" id="cb40">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" tabindex="-1"></a><span class="im">import</span> spacy</span>
<span id="cb40-2"><a href="#cb40-2" tabindex="-1"></a><span class="im">from</span> gensim.models <span class="im">import</span> Word2Vec </span>
<span id="cb40-3"><a href="#cb40-3" tabindex="-1"></a></span>
<span id="cb40-4"><a href="#cb40-4" tabindex="-1"></a><span class="co"># Load and Tokenize the Text using spacy</span></span>
<span id="cb40-5"><a href="#cb40-5" tabindex="-1"></a>spacy_model <span class="op">=</span> spacy.load(<span class="st">"en_core_web_sm"</span>)</span>
<span id="cb40-6"><a href="#cb40-6" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">"data/84_frankenstein_clean.txt"</span>) <span class="im">as</span> f:</span>
<span id="cb40-7"><a href="#cb40-7" tabindex="-1"></a>    book_text <span class="op">=</span> f.read()    </span>
<span id="cb40-8"><a href="#cb40-8" tabindex="-1"></a>book_doc <span class="op">=</span> spacy_model(book_text)</span>
<span id="cb40-9"><a href="#cb40-9" tabindex="-1"></a>clean_tokens <span class="op">=</span> [tok.text.lower() <span class="cf">for</span> tok <span class="kw">in</span> book_doc <span class="cf">if</span> tok.is_alpha <span class="kw">and</span> <span class="kw">not</span> tok.is_stop]</span>
<span id="cb40-10"><a href="#cb40-10" tabindex="-1"></a></span>
<span id="cb40-11"><a href="#cb40-11" tabindex="-1"></a><span class="co"># Call and Train the Word2Vec model</span></span>
<span id="cb40-12"><a href="#cb40-12" tabindex="-1"></a>model <span class="op">=</span> Word2Vec([clean_tokens], sg<span class="op">=</span><span class="dv">0</span> , vector_size<span class="op">=</span><span class="dv">300</span>, window<span class="op">=</span><span class="dv">5</span>, min_count<span class="op">=</span><span class="dv">1</span>, workers<span class="op">=</span><span class="dv">4</span>)</span></code></pre>
</div>
<p>With this line code we are configuring our whole Word2Vec training
schema. We will be using CBOW (<code>sg=0</code> means CBOW,
<code>sg=1</code> means Skip-gram). We are interested in having vectors
with 300 dimensions <code>vector_size=300</code> and a context size of 5
surrounding words <code>window=5</code>. Because we already filtered our
tokens, we include all words present in the filtered corpora, regardless
of their frequency of occurrence <code>min_count=1</code>. The last
parameters tells python to use 4 CPU cores for training.</p>
<p>See the Gensim <a href="https://radimrehurek.com/gensim/models/word2vec.html" class="external-link">documentation</a>
for more training options.</p>
<div class="section level3">
<h3 id="save-and-retrieve-your-model">Save and Retrieve your model<a class="anchor" aria-label="anchor" href="#save-and-retrieve-your-model"></a>
</h3>
<p>Once your model is trained it is useful to save the checkpoint in
order to retrieve it next time instead of having to train it every time.
You can save it with:</p>
<div class="codewrapper sourceCode" id="cb41">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" tabindex="-1"></a>model.save(<span class="st">"word2vec_mini_books.model"</span>)</span></code></pre>
</div>
<p>Let’s put everything together. We have now the following NLP task:
train our own Word2Vec model. We are interested on having vectors for
content words only, so even though our preprocessing will unfortunately
lose a lot of the original information, in exchange we will be able to
manipulate the most relevant conceptual words as individual numeric
representations.</p>
<p>To load back the pre-trained vectors you just created you can use the
following code:</p>
<div class="codewrapper sourceCode" id="cb42">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" tabindex="-1"></a>model <span class="op">=</span> Word2Vec.load(<span class="st">"word2vec_mini_books.model"</span>)</span>
<span id="cb42-2"><a href="#cb42-2" tabindex="-1"></a>w2v <span class="op">=</span> model.wv</span>
<span id="cb42-3"><a href="#cb42-3" tabindex="-1"></a><span class="co"># Test:</span></span>
<span id="cb42-4"><a href="#cb42-4" tabindex="-1"></a>w2v.most_similar(<span class="st">'monster'</span>)</span></code></pre>
</div>
<div id="challenge3" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div class="callout-inner">
<h3 class="callout-title">Challenge</h3>
<div class="callout-content">
<p>Let’s apply this step by step on a longer text. In this case, because
we are learning the process, our corpus will be only one book but in
reality we would like to train a network with thousands of them. We will
use two books: Frankenstein and Dracula to train a model of word
vectors.</p>
<p>Write the code to follow the proposed pipeline and train the word2vec
model. The proposed pipeline for this task is:</p>
<ul>
<li>load the text files</li>
<li>tokenize files</li>
<li>keep only alphanumerical tokens</li>
<li>lemmatize words</li>
<li>Remove stop words</li>
<li>Train a Word2Vec model (feed the clean tokens to the
<code>Word2Vec</code> object) with <code>vector_size=50</code>
</li>
<li>Save the trained model</li>
</ul>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3"> Show me the solution </h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" data-bs-parent="#accordionSolution3" aria-labelledby="headingSolution3">
<div class="accordion-body">
<div class="codewrapper sourceCode" id="cb43">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" tabindex="-1"></a><span class="im">import</span> spacy</span>
<span id="cb43-2"><a href="#cb43-2" tabindex="-1"></a><span class="im">from</span> gensim.models <span class="im">import</span> Word2Vec </span>
<span id="cb43-3"><a href="#cb43-3" tabindex="-1"></a></span>
<span id="cb43-4"><a href="#cb43-4" tabindex="-1"></a><span class="kw">def</span> process_book(book_filename: <span class="bu">str</span>, spacy_model: spacy.lang) <span class="op">-&gt;</span> <span class="bu">list</span>[<span class="bu">str</span>]:</span>
<span id="cb43-5"><a href="#cb43-5" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(book_filename) <span class="im">as</span> f:</span>
<span id="cb43-6"><a href="#cb43-6" tabindex="-1"></a>        book_text <span class="op">=</span> f.read()</span>
<span id="cb43-7"><a href="#cb43-7" tabindex="-1"></a>    </span>
<span id="cb43-8"><a href="#cb43-8" tabindex="-1"></a>    book_doc <span class="op">=</span> spacy_model(book_text)</span>
<span id="cb43-9"><a href="#cb43-9" tabindex="-1"></a>    valid_tokens <span class="op">=</span> [tok <span class="cf">for</span> tok <span class="kw">in</span> book_doc <span class="cf">if</span> tok.is_alpha <span class="kw">and</span> <span class="kw">not</span> tok.is_stop]</span>
<span id="cb43-10"><a href="#cb43-10" tabindex="-1"></a>    lemmas <span class="op">=</span> [tok.lemma_ <span class="cf">for</span> tok <span class="kw">in</span> valid_tokens] </span>
<span id="cb43-11"><a href="#cb43-11" tabindex="-1"></a>    <span class="cf">return</span> lemmas</span>
<span id="cb43-12"><a href="#cb43-12" tabindex="-1"></a>    </span>
<span id="cb43-13"><a href="#cb43-13" tabindex="-1"></a>nlp <span class="op">=</span> spacy.load(<span class="st">"en_core_web_sm"</span>)</span>
<span id="cb43-14"><a href="#cb43-14" tabindex="-1"></a></span>
<span id="cb43-15"><a href="#cb43-15" tabindex="-1"></a><span class="co"># Load the Tokens</span></span>
<span id="cb43-16"><a href="#cb43-16" tabindex="-1"></a>franken_tokens <span class="op">=</span> process_book(<span class="st">"data/84_frankenstein_clean.txt"</span>, nlp)</span>
<span id="cb43-17"><a href="#cb43-17" tabindex="-1"></a>dracula_tokens <span class="op">=</span> process_book(<span class="st">"data/345_dracula_clean.txt"</span>, nlp)</span>
<span id="cb43-18"><a href="#cb43-18" tabindex="-1"></a></span>
<span id="cb43-19"><a href="#cb43-19" tabindex="-1"></a><span class="co"># Train our own model</span></span>
<span id="cb43-20"><a href="#cb43-20" tabindex="-1"></a>spooky_model <span class="op">=</span> Word2Vec([franken_tokens, dracula_tokens], sg<span class="op">=</span><span class="dv">0</span> , vector_size<span class="op">=</span><span class="dv">50</span>, window<span class="op">=</span><span class="dv">5</span>, min_count<span class="op">=</span><span class="dv">1</span>, workers<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb43-21"><a href="#cb43-21" tabindex="-1"></a></span>
<span id="cb43-22"><a href="#cb43-22" tabindex="-1"></a><span class="co"># Test the vectors</span></span>
<span id="cb43-23"><a href="#cb43-23" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(spooky_model.wv[<span class="st">'Frankenstein'</span>]))</span>
<span id="cb43-24"><a href="#cb43-24" tabindex="-1"></a><span class="bu">print</span>(spooky_model.wv[<span class="st">'Frankenstein'</span>][:<span class="dv">30</span>])</span>
<span id="cb43-25"><a href="#cb43-25" tabindex="-1"></a><span class="bu">print</span>(spooky_model.wv.most_similar(<span class="st">"Frankenstein"</span>))</span></code></pre>
</div>
</div>
</div>
</div>
</div>
<div id="dataset-size-in-training" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="dataset-size-in-training" class="callout-inner">
<h3 class="callout-title">Dataset size in training</h3>
<div class="callout-content">
<p>To obtain your own high-quality embeddings, the size/length of the
training dataset plays a crucial role. Generally <a href="https://cs.stanford.edu/~quocle/paragraph_vector.pdf" class="external-link">tens of
thousands of documents</a> are considered a reasonable amount of data
for decent results.</p>
<p>Is there a strict minimum? Not really. It’s important to keep in mind
that <code>vocabulary size</code>, <code>document length</code>, and
<code>desired vector size</code> all interact with each other.
Higher-dimensional vectors (e.g., 200–300 dimensions) provide more
features to capture a word’s meaning, resulting in higher-quality
embeddings that can represent words across a finer-grained and more
diverse set of contexts.</p>
<p>While Word2vec models typically perform better with large datasets
containing millions of words, using a single page is sufficient for
demonstration and learning purposes. This smaller dataset allows us to
train the model quickly and understand how word2vec works without the
need for extensive computational resources.</p>
</div>
</div>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<span class="callout-header">Key Points</span>
<div class="callout-inner">
<div class="callout-content">
<ul>
<li>We can run a preprocessing pipeline to obtain clear words that can
be used as features</li>
<li>We learned how are words converted into vectors of numbers (which
makes them interpretable for machines)</li>
<li>We can easily compute how words are similar to each other with the
cosine similarity</li>
<li>Using gensim we can train our own word2vec models</li>
</ul>
</div>
</div>
</div>
</div>
</section></section><section id="aio-03-transformers"><p>Content from <a href="03-transformers.html">Transformers: BERT and Beyond</a></p>
<hr>
<p>Last updated on 2025-11-27 |

        <a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/edit/main/episodes/03-transformers.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What are some drawbacks of static word embeddings?</li>
<li>What are Transformers?</li>
<li>What is BERT and how does it work?</li>
<li>How can I use BERT to solve supervised NLP tasks?</li>
<li>How should I evaluate my classifiers?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Understand how a Transformer works and recognize their main
components.</li>
<li>Understand how to use pre-trained transformers (Use Case: BERT)</li>
<li>Use BERT to predict words in context.</li>
<li>Use BERT to classify texts.</li>
<li>Learn to evaluate your NLP Classifier.</li>
<li>Understand assumptions and basic evaluation for NLP outputs.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<p>Word embeddings such as Word2Vec can be used to represent words as
unique vectors instead of Python strings. These vector representations
give us numerical “proxy” representations for words. And these allow us
to provide mathematical definitions for attributing semantics or meaning
to words. They also enable metrics and measures for studying linguistic
relationships with words. For example, one can devise metrics for
similarity and semantic closeness of words, by defining a measure of
distance between their corresponding vectors. This has proven to be
quite useful for downstream lexical-related tasks.</p>
<p>However, a big drawback of Word2Vec is that <strong>each word is
represented in isolation</strong>, which means that once we finished
training a model, each word has exactly one vector associated with it,
regardless of the different contexts in which it appears in the corpus
text. This is what is called <em>static word embedding</em>, and
unfortunately that is a serious limitation in expressing finer-grained
complexities in language. Words derive their meaning
<em>dynamically</em>, based on the specific context in which they are
used. Think of syntactic information, which is relevant to understand
the difference between “the dog bit the man” and “the man bit the dog”.
Another case is polysemy, where the same word can have very different
meanings depending on the context, for example, “bit” in “the dog bit
the man” and “in this bit of the book”. Therefore, we would like to have
richer vectors of words that are themselves sensitive to their context
in order to obtain finer-grained representations of word meaning.</p>
<div id="polysemy-in-language" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="polysemy-in-language" class="callout-inner">
<h3 class="callout-title">Polysemy in Language</h3>
<div class="callout-content">
<p>Think of words (at least 2) that can have more than one meaning
depending on the context. Come up with one simple sentence per meaning
and explain what they mean in each context. Discuss: How do you know
which of the possible meanings does the word have when you see it?</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<p>Two possible examples can be the words ‘fine’ and ‘run’</p>
<p>Sentences for ‘fine’: - She has a fine watch (fine == high-quality) -
He had to pay a fine (fine == penalty) - I am feeling fine (fine == not
bad)</p>
<p>Sentences for ‘run’: - I had to run to catch the bus (run == moving
fast) - Stop talking, before you run out of ideas (run (out) ==
exhaust)</p>
<p>Note how in the “run out” example we even have to understand that the
meaning of run is not literal but goes accompanied with a preposition
that changes its meaning.</p>
</div>
</div>
</div>
</div>
<div id="bridging-word-embeddings-and-contextualized-models" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="bridging-word-embeddings-and-contextualized-models" class="callout-inner">
<h3 class="callout-title">Bridging Word Embeddings and Contextualized
Models</h3>
<div class="callout-content">
<p>The limitations of Word2Vec became apparent as researchers tackled
more complex natural language understanding tasks. BERT, the model we
will describe below, was not the first attempt to improve upon the
drawbacks of Word2Vec. Several intermediate models emerged to address
the shortcomings. Some prominent models were:</p>
<ul>
<li><p><strong>FastText</strong> <a href="https://arxiv.org/pdf/1607.01759" class="external-link">(Joulin, et. al., 2016)</a>,
developed by Facebook, extended Word2Vec by representing words as bags
of character n-grams rather than atomic units. This subword tokenization
approach enabled the model to generate embeddings for previously unseen
words by combining learned subword representations—for instance,
understanding “unhappiness” through its components “un-,” “happiness,”
and “-ness.” FastText proved particularly valuable for morphologically
rich languages and handling misspellings or rare word forms.</p></li>
<li><p><strong>ELMo</strong> <a href="https://aclanthology.org/N18-1202.pdf" class="external-link">(Peters, et. al., 2018)</a>
marked a paradigm shift by integrating context into the word
representations. Unlike Word2Vec’s static embeddings, ELMo generated
different representations for the same word based on its surrounding
context using bidirectional LSTM networks. The model was pretrained on
large text corpora using a language modeling objective—predicting words
from both left and right contexts simultaneously—and crucially
introduced effective transfer learning to NLP. BERT would basically
replicate this concept but using a more powerful neural network
architecture: the Transformer, which allowed for the scaling of training
material.</p></li>
<li><p><strong>ULMFiT</strong> <a href="https://aclanthology.org/P18-1031.pdf" class="external-link">(Howard &amp; Ruder,
2018)</a>. Universal Language Model Fine-tuning, also tackled the
problem via transfer learning; that is, <em>re-using the same model</em>
for learning several different tasks and hence enriching word
representations after each task was learned. This idea also enriched
BERT post-training methodologies.</p></li>
</ul>
<p>These intermediate models established several crucial concepts: that
subword tokenization could handle vocabulary limitations, that
context-dependent representations were superior to static embeddings,
that deep bidirectional architectures captured richer linguistic
information, and most importantly, that large-scale pretraining followed
by task-specific fine-tuning could dramatically improve performance
across diverse NLP applications.</p>
</div>
</div>
</div>
<p>In late 2018, the <a href="https://aclanthology.org/N19-1423.pdf" class="external-link">BERT</a> language model was
introduced. Using a novel architecture called <a href="https://arxiv.org/pdf/1706.03762" class="external-link">Transformer</a> (2017), BERT was
specifically designed to scale the amount of training data and integrate
context into word representations. To understand BERT, we will first
look at what a transformer is and we will then directly use some code to
make use of BERT.</p>
<section><h2 class="section-heading" id="transformers">Transformers<a class="anchor" aria-label="anchor" href="#transformers"></a>
</h2>
<hr class="half-width">
<p>The Transformer is a deep neural network architecture proposed by
Google researchers in a paper called <em>Attention is all you Need</em>.
They tackled specifically the NLP task of Machine Translation (MT),
which is stated as: how to generate a sentence (sequence of words) in
target language B given a sentence in source language A? We all know
that translation cannot be done word by word in isolation, therefore
integrating the context from both the source language and the target
language is necessary. In order to translate, first one neural network
needs to <em>encode</em> the whole meaning of the sentence in language A
into a single vector representation, then a second neural network needs
to <em>decode</em> that representation into tokens that are both
coherent with the meaning of language A and understandable in language
B. Therefore we say that translation is modeling language B
<em>conditioned</em> on what language A originally said.</p>
<figure><img src="fig/trans1.png" alt="Transformer Architecture" class="figure mx-auto d-block"><div class="figcaption">Transformer Architecture</div>
</figure><p>As seen in the picture, the original Transformer is an
Encoder-Decoder network that tackles translation. We first need a token
embedder which converts the string of words into a sequence of vectors
that the Transformer network can process. The first component, the
<strong>Encoder</strong>, is optimized for creating <strong>rich
representations</strong> of the source sequence (in this case an English
sentence) while the second one, the <strong>Decoder</strong> is a
<strong>generative network</strong> that is conditioned on the encoded
representation. The third component we see is the infamous attention
mechanism, a third neural network what computes the correlation between
source and target tokens (<em>which word in Dutch should I pay attention
to, to decide a better next English word?</em>) to generate the most
likely token in the target sequence (in this case Dutch words).</p>
<div id="emulate-the-attention-mechanism" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="emulate-the-attention-mechanism" class="callout-inner">
<h3 class="callout-title">Emulate the Attention Mechanism</h3>
<div class="callout-content">
<p>Pair with a person who speaks a language different from English (we
will call it language B). Think of 1 or 2 simple sentences in English
and come up with their translations in the second language. In a piece
of paper write down both sentences (one on top of the other with some
distance in between) and try to:</p>
<ol style="list-style-type: decimal">
<li>Draw a mapping of words or phrases from language B to English. Is it
always possible to do this one-to-one for words?</li>
<li>Think of how this might relate to attention in transformers?</li>
</ol>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2"> Show me the solution </h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" data-bs-parent="#accordionSolution2" aria-labelledby="headingSolution2">
<div class="accordion-body">
<p>Here is an example of a sentence in English and its translation into
Spanish. We can look at the final mapping and observe that:</p>
<ol style="list-style-type: decimal">
<li>Even though they are closely related languages, the translation is
not linear</li>
<li>There is also not a direct word-to-word mapping between the
sentences</li>
<li>Some words present in the source are not present in the target (and
vice versa)</li>
<li>Some words are not translations of each other but they are still
very relevant to understand the context</li>
</ol>
<figure><img src="fig/trans_attention_mapping.png" class="figure mx-auto d-block"></figure>
</div>
</div>
</div>
</div>
<p>Next, we will see how BERT exploits the idea of a <strong>Transformer
Encoder</strong> to perform the NLP Task we are interested in:
generating powerful word representations.</p>
</section><section><h2 class="section-heading" id="bert">BERT<a class="anchor" aria-label="anchor" href="#bert"></a>
</h2>
<hr class="half-width">
<p><a href="https://aclanthology.org/N19-1423.pdf" class="external-link">BERT</a> is an
acronym that stands for <strong>B</strong>idirectional
<strong>E</strong>ncoder <strong>R</strong>epresentations from
<strong>T</strong>ransformers. The name describes it all: the idea is to
use the power of the Encoder component of the Transformer architecture
to create powerful token representations that preserve the contextual
meaning of the whole input segment, instead of each word in isolation.
The BERT vector representations of each token take into account both the
left context (what comes before the word) and the right context (what
comes after the word). Another advantage of the transformer Encoder is
that it is parallelizable, which made it possible for the first time to
train these networks on millions of datapoints, dramatically improving
model generalization.</p>
<div id="pretraining-bert" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="pretraining-bert" class="callout-inner">
<h3 class="callout-title">Pretraining BERT</h3>
<div class="callout-content">
<p>To obtain the BERT vector representations the Encoder is pre-trained
with two different tasks: - <strong>Masked Language Model:</strong> for
each sentence, mask one token at a time and predict which token is
missing based on the context from both sides. A training input example
would be “Maria [MASK] Groningen” and the model should predict the word
“loves”. - <strong>Next Sentence Prediction:</strong> the Encoder gets a
linear binary classifier on top, which is trained to decide for each
pair of sequences A and B, if sequence A precedes sequence B in a text.
For the sentence pair: “Maria loves Groningen.” and “This is a city in
the Netherlands.” the output of the classifier is “True” and for the
pair “Maria loves Groningen.” and “It was a tasty cake.” the output
should be “false” as there is no obvious continuation between the two
sentences.</p>
<p>Already the second pre-training task gives us an idea of the power of
BERT: after it has been pretrained on hundreds of thousands of texts,
one can plug-in a classifier on top and re-use the <em>linguistic</em>
knowledge previously acquired to fine-tune it for a specific task,
without needing to learn the weights of the whole network from scratch
all over again. In the next sections we will describe the components of
BERT and show how to use it. This model and hundreds of related
transformer-based pre-trained encoders can also be found on <a href="https://huggingface.co/google-bert/bert-base-cased" class="external-link">Hugging
Face</a>.</p>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="bert-architecture">BERT Architecture<a class="anchor" aria-label="anchor" href="#bert-architecture"></a>
</h2>
<hr class="half-width">
<p>The BERT Architecture can be seen as a basic NLP pipeline on its
own:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Tokenizer:</strong> splits text into tokens that the model
recognizes</li>
<li>
<strong>Embedder:</strong> converts each token into a fixed-sized
vector that represents it. These vectors are the actual input for the
Encoder.</li>
<li>
<strong>Encoder</strong> several neural layers that model the
token-level interactions of the input sequence to enhance meaning
representation. The output of the encoder is a set of
<strong>H</strong>idden layers, the vector representation of the
ingested sequence.</li>
<li>
<strong>Output Layer:</strong> the final encoder layer (which we
depict as a sequence <strong>H</strong>’s in the figure) contains
arguably the best token-level representations that encode syntactic and
semantic properties of each token, but this time each vector is already
contextualized with the specific sequence.</li>
<li>
<em>OPTIONAL</em> <strong>Classifier Layer:</strong> an additional
classifier can be connected on top of the BERT token vectors which are
used as features for performing a downstream task. This can be used to
classify at the text level, for example sentiment analysis of a
sentence, or at the token-level, for example Named Entity
Recognition.</li>
</ol>
<figure><img src="fig/bert3.png" alt="BERT Architecture" class="figure mx-auto d-block"><div class="figcaption">BERT Architecture</div>
</figure><p>BERT uses (self-) attention, which is very useful to capture
longer-range word dependencies such as co-reference, where, for example,
a pronoun can be linked to the noun it refers to previously in the same
sentence. See the following example:</p>
<figure><img src="fig/trans5.png" alt="The Encoder Self-Attention Mechanism" class="figure mx-auto d-block"><div class="figcaption">The Encoder Self-Attention Mechanism</div>
</figure></section><section><h2 class="section-heading" id="bert-for-word-based-analysis">BERT for Word-Based Analysis<a class="anchor" aria-label="anchor" href="#bert-for-word-based-analysis"></a>
</h2>
<hr class="half-width">
<p>Let’s see how these components can be manipulated with code. For this
we will be using the HuggingFace’s <em>transformers</em> Python library.
The first two main components we need to initialize are the model and
tokenizer. The HuggingFace hub contains thousands of models based on a
Transformer architecture for dozens of tasks, data domains and also
hundreds of languages. Here we will explore the vanilla English BERT
which was how everything started. We can initialize this model with the
next lines:</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> BertTokenizer, BertModel</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a>tokenizer <span class="op">=</span> BertTokenizer.from_pretrained(<span class="st">'bert-base-cased'</span>)</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a>model <span class="op">=</span> BertModel.from_pretrained(<span class="st">"bert-base-cased"</span>)</span></code></pre>
</div>
<div class="section level3">
<h3 id="bert-tokenizer">BERT Tokenizer<a class="anchor" aria-label="anchor" href="#bert-tokenizer"></a>
</h3>
<p>We start with a string of text as written in any blog, book,
newspaper etcetera. The <code>tokenizer</code> object is responsible of
splitting the string into recognizable tokens for the model and
embedding the tokens into their vector representations</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"Maria loves Groningen"</span></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>encoded_input <span class="op">=</span> tokenizer(text, return_tensors<span class="op">=</span><span class="st">'pt'</span>)</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a><span class="bu">print</span>(encoded_input)</span></code></pre>
</div>
<p>The print shows the <code>encoded_input</code> object returned by the
tokenizer, with its attributes and values. The <code>input_ids</code>
are the most important output for now, as these are the token IDs
recognized by BERT</p>
<pre><code>{
    'input_ids': tensor([[  101,  3406,  7871,   144,  3484, 15016,   102]]),
    'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]),
    'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])
}
</code></pre>
<p>NOTE: the printing function shows transformers objects as
dictionaries; however, to access the attributes, you must use the Python
object syntax, such as in the following example:</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="bu">print</span>(encoded_input.input_ids.shape)</span></code></pre>
</div>
<p>Output:</p>
<p><code>torch.Size([1, 7])</code></p>
<p>The output is a 2-dimensional tensor where the first dimension
contains 1 element (this dimension represents the batch size), and the
second dimension contains 7 elements which are equivalent to the 7
tokens that BERT generated from our string input.</p>
<p>In order to see what these Token IDs represent, we can
<em>translate</em> them into human readable strings. This includes
converting the tensors into <code>numpy</code> arrays and converting
each ID into its string representation:</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a>token_ids <span class="op">=</span> <span class="bu">list</span>(encoded_input.input_ids[<span class="dv">0</span>].detach().numpy())</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>string_tokens <span class="op">=</span> tokenizer.convert_ids_to_tokens(token_ids)</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"IDs:"</span>, token_ids)</span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"TOKENS:"</span>, string_tokens)</span></code></pre>
</div>
<pre><code>IDs: [101, 3406, 7871, 144, 3484, 15016, 102]

TOKENS: ['[CLS]', 'Maria', 'loves', 'G', '##ron', '##ingen', '[SEP]']
</code></pre>
<p>First we feed the tokenized input into the model:</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a>output <span class="op">=</span> model(<span class="op">**</span>encoded_input)</span></code></pre>
</div>
<p>Now we can explore the representations in the model.</p>
<div id="callout3" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div class="callout-inner">
<div class="callout-content">
<p>In the case of wanting to obtain a single vector for
<em>Groningen</em>, you can average the three vectors that belong to the
token pieces that ultimately form that word. For example:</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a>tok_G <span class="op">=</span> output.last_hidden_state[<span class="dv">0</span>][<span class="dv">3</span>].detach().numpy()</span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a>tok_ron <span class="op">=</span> output.last_hidden_state[<span class="dv">0</span>][<span class="dv">4</span>].detach().numpy()</span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a>tok_ingen <span class="op">=</span> output.last_hidden_state[<span class="dv">0</span>][<span class="dv">5</span>].detach().numpy()</span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a></span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a>tok_groningen <span class="op">=</span> np.mean([tok_G, tok_ron, tok_ingen], axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb8-7"><a href="#cb8-7" tabindex="-1"></a><span class="bu">print</span>(tok_groningen.shape)</span>
<span id="cb8-8"><a href="#cb8-8" tabindex="-1"></a><span class="bu">print</span>(tok_groningen[:<span class="dv">10</span>])</span></code></pre>
</div>
<p>We obtain the output:</p>
<pre><code>(768,)
[ 0.00990007 -0.44266668  0.05274092  0.08865168  0.71115637 -0.4092603
  0.18841815  0.19883917  0.24680579 -0.07899686]</code></pre>
<p>We use the functions <code>detach().numpy()</code> to bring the
values from the Pytorch execution environment (for example a GPU) into
the main Python thread and treat it as a <code>Numpy</code> vector for
convenience. Then, since we are dealing with three <code>Numpy</code>
vectors we can average the three of them and end up with a single
<code>Groningen</code> vector of 768-dimensions representing the average
of <code>'G', '##ron', '##ingen'</code>.</p>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="analyzing-polysemy-with-bert">Analyzing polysemy with BERT<a class="anchor" aria-label="anchor" href="#analyzing-polysemy-with-bert"></a>
</h3>
<p>We can encode two sentences containing the word <em>note</em> to see
how BERT actually handles polysemy (<em>note</em> means something very
different in each sentence) thanks to the representation of each word
now being contextualized instead of isolated as was the case with
Word2vec.</p>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a><span class="co"># We tokenize the sentence and identify the two different 'note' tokens inside the sentence</span></span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a>text_note <span class="op">=</span> <span class="st">"Please note that this bank note is fake!"</span></span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a>tokenized_text <span class="op">=</span> tokenizer(text_note, return_tensors<span class="op">=</span><span class="st">'pt'</span>)</span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a>token_ids <span class="op">=</span> <span class="bu">list</span>(tokenized_text.input_ids[<span class="dv">0</span>].detach().numpy())</span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a>string_tokens <span class="op">=</span> tokenizer.convert_ids_to_tokens(token_ids)</span>
<span id="cb10-6"><a href="#cb10-6" tabindex="-1"></a><span class="bu">print</span>(string_tokens)</span></code></pre>
</div>
<p>We are printing the BERT tokens extracted from the sentence, and
displaying them:</p>
<pre><code>['[CLS]', 'Please', 'note', 'that', 'this', 'bank', 'note', 'is', 'fake', '!', '[SEP]']</code></pre>
<p>We then manually count which token indices in the list belong to
<code>note</code>, so we can extract their vectors later</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a>note_index_1 <span class="op">=</span> <span class="dv">2</span> <span class="co"># first occurrence of `note`</span></span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a>note_index_2 <span class="op">=</span> <span class="dv">6</span> <span class="co"># second occurrence of `note`</span></span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a><span class="bu">print</span>(string_tokens[note_index_1], string_tokens[note_index_2])</span></code></pre>
</div>
<p>We now pass the sentence through the BERT encoder and extract the
encoded vectors belonging to each <code>note</code> token:</p>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a><span class="co"># Encode the sentence and extract the vector belonging to each 'note' token</span></span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a>    bert_output <span class="op">=</span> model(<span class="op">**</span>tokenized_text)</span>
<span id="cb13-4"><a href="#cb13-4" tabindex="-1"></a>note_vector_1 <span class="op">=</span> bert_output.last_hidden_state[<span class="dv">0</span>][note_index_1].detach().numpy()</span>
<span id="cb13-5"><a href="#cb13-5" tabindex="-1"></a>note_vector_2 <span class="op">=</span> bert_output.last_hidden_state[<span class="dv">0</span>][note_index_2].detach().numpy()</span>
<span id="cb13-6"><a href="#cb13-6" tabindex="-1"></a></span>
<span id="cb13-7"><a href="#cb13-7" tabindex="-1"></a><span class="bu">print</span>(note_vector_1[:<span class="dv">5</span>])</span>
<span id="cb13-8"><a href="#cb13-8" tabindex="-1"></a><span class="bu">print</span>(note_vector_2[:<span class="dv">5</span>])</span></code></pre>
</div>
<pre><code>[1.0170387  0.93691176 0.30571502 0.33091038 0.73093796]
[0.17840035 0.65847856 0.22412607 0.21162085 0.5393072 ]</code></pre>
<p>By printing the first 5 dimensions of the vectors we can see that,
even though both vectors technically belong to the same string input
<code>note</code>, they have different numeric representations. This is
the case because the BERT encoder takes into account the position of the
token as well as all the previous and following tokens when computing
the representation for each token.</p>
<p>To be sure, we can compute the cosine similarity of the word
<code>note</code> in the first sentence and the word <em>note</em> in
the second sentence confirming that they are indeed two different
representations, even when both cases have the same token-id and they
are the 12th token of the sentence:</p>
<div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a><span class="im">from</span> sklearn.metrics.pairwise <span class="im">import</span> cosine_similarity</span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb15-3"><a href="#cb15-3" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" tabindex="-1"></a>vector1 <span class="op">=</span> np.array(note_vector_1).reshape(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>)  <span class="co"># Reshape to 2D array</span></span>
<span id="cb15-5"><a href="#cb15-5" tabindex="-1"></a>vector2 <span class="op">=</span> np.array(note_vector_2).reshape(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>)  <span class="co"># Reshape to 2D array</span></span>
<span id="cb15-6"><a href="#cb15-6" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" tabindex="-1"></a>similarity <span class="op">=</span> cosine_similarity(vector1, vector2)</span>
<span id="cb15-8"><a href="#cb15-8" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Cosine Similarity 'note' vs 'note': </span><span class="sc">{</span>similarity[<span class="dv">0</span>][<span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code></pre>
</div>
<p>With this small experiment, we have confirmed that the Encoder
produces context-dependent word representations, as opposed to Word2Vec,
where <em>note</em> would always have the same vector no matter where it
appeared.</p>
<div id="callout4" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div class="callout-inner">
<div class="callout-content">
<p>When running examples in a BERT pre-trained model, it is advisable to
wrap your code inside a <code>torch.no_grad():</code> context. This is
linked to the fact that BERT is a Neural Network that has been trained
(and can be further fine-tuned) with the Backpropagation algorithm.
Essentially, this wrapper tells the model that we are not in training
mode, and we are not interested in <em>updating</em> the weights (as it
would happen when training any neural network), because the weights are
already optimal enough. By using this wrapper, we make the model more
efficient as it does not need to calculate the gradients for an eventual
backpropagation step, since we are only interested in what <em>comes
out</em> of the Encoder. So the previous code can be made more efficient
like this:</p>
<div class="codewrapper sourceCode" id="cb16">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a><span class="im">import</span> torch </span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a></span>
<span id="cb16-3"><a href="#cb16-3" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb16-4"><a href="#cb16-4" tabindex="-1"></a>    output <span class="op">=</span> model(<span class="op">**</span>encoded_input)</span>
<span id="cb16-5"><a href="#cb16-5" tabindex="-1"></a>    <span class="bu">print</span>(output)</span>
<span id="cb16-6"><a href="#cb16-6" tabindex="-1"></a>    <span class="bu">print</span>(output.last_hidden_state.shape)</span></code></pre>
</div>
</div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="bert-as-a-language-model">BERT as a Language Model<a class="anchor" aria-label="anchor" href="#bert-as-a-language-model"></a>
</h2>
<hr class="half-width">
<p>As mentioned before, the main pre-training task of BERT is Language
Modelling (LM): calculating the probability of a word based on the known
neighboring words (yes, Word2Vec was also a kind of LM!). Obtaining
training data for this task is very cheap, as all we need is millions of
sentences from existing texts, without any labels. In this setting, BERT
encodes a sequence of words, and predicts from a set of English tokens,
what is the most likely token that could be inserted in the
<code>[MASK]</code> position</p>
<figure><img src="fig/bert1b.png" alt="BERT Language Modeling" class="figure mx-auto d-block"><div class="figcaption">BERT Language Modeling</div>
</figure><p>We can therefore start using BERT as a predictor for word completion.
From now own, we will learn how to use the <code>pipeline</code> object,
this is very useful when we only want to use a pre-trained model for
predictions (no need to fine-tune or do word-specific analysis). The
<code>pipeline</code> will internally initialize both model and
tokenizer for us and also merge word pieces back into complete
words.</p>
<div id="callout5" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div class="callout-inner">
<div class="callout-content">
<p>The <code>pipeline</code> object is very convenient to test all the
out-of-the-box models that you can find in the HuggingFace platform, and
if you are lucky and you like what you see, this is the fastest way to
get model predictions.</p>
<p>But consider that for some tasks you will want more control over the
tokenization and how the model merges the predictions. If that is the
case, initializing the tokenizer, encoding the texts into word-vectors
and running the model inference yourself might be the way to go (like it
is shown in the <em>Polysemy in BERT</em> section).</p>
<p>Even if you <em>like what you see</em>, it is important that you
evaluate the performance of the models in <strong>your data and use
case</strong> regardless of what the model performance claims to be. We
will show an example with a couple of evaluation metrics in the <em>BERT
for Text Classification</em> section.</p>
</div>
</div>
</div>
<p>In this case again we use <code>bert-base-cased</code>, which refers
to the vanilla BERT English model. Once we declared a pipeline, we can
feed it with sentences that contain one masked token at a time (beware
that BERT can only predict one word at a time, since that was its
training scheme). For example:</p>
<div class="codewrapper sourceCode" id="cb17">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" tabindex="-1"></a><span class="kw">def</span> pretty_print_outputs(sentences, model_outputs):</span>
<span id="cb17-4"><a href="#cb17-4" tabindex="-1"></a>    <span class="cf">for</span> i, model_out <span class="kw">in</span> <span class="bu">enumerate</span>(model_outputs):</span>
<span id="cb17-5"><a href="#cb17-5" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">=====</span><span class="ch">\t</span><span class="st">"</span>,sentences[i])</span>
<span id="cb17-6"><a href="#cb17-6" tabindex="-1"></a>        <span class="cf">for</span> label_scores <span class="kw">in</span> model_out:</span>
<span id="cb17-7"><a href="#cb17-7" tabindex="-1"></a>            <span class="bu">print</span>(label_scores)</span>
<span id="cb17-8"><a href="#cb17-8" tabindex="-1"></a></span>
<span id="cb17-9"><a href="#cb17-9" tabindex="-1"></a></span>
<span id="cb17-10"><a href="#cb17-10" tabindex="-1"></a>nlp <span class="op">=</span> pipeline(task<span class="op">=</span><span class="st">"fill-mask"</span>, model<span class="op">=</span><span class="st">"bert-base-cased"</span>, tokenizer<span class="op">=</span><span class="st">"bert-base-cased"</span>)</span>
<span id="cb17-11"><a href="#cb17-11" tabindex="-1"></a>sentences <span class="op">=</span> [<span class="st">"Paris is the [MASK] of France"</span>, <span class="st">"I want to eat a cold [MASK] this afternoon"</span>, <span class="st">"Maria [MASK] Groningen"</span>]</span>
<span id="cb17-12"><a href="#cb17-12" tabindex="-1"></a>model_outputs <span class="op">=</span> nlp(sentences, top_k<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb17-13"><a href="#cb17-13" tabindex="-1"></a>pretty_print_outputs(sentences, model_outputs)</span></code></pre>
</div>
<pre><code>=====	 Paris is the [MASK] of France
{'score': 0.9807965755462646, 'token': 2364, 'token_str': 'capital', 'sequence': 'Paris is the capital of France'}
{'score': 0.004513159394264221, 'token': 6299, 'token_str': 'Capital', 'sequence': 'Paris is the Capital of France'}
{'score': 0.004281804896891117, 'token': 2057, 'token_str': 'center', 'sequence': 'Paris is the center of France'}
{'score': 0.002848200500011444, 'token': 2642, 'token_str': 'centre', 'sequence': 'Paris is the centre of France'}
{'score': 0.0022805952467024326, 'token': 1331, 'token_str': 'city', 'sequence': 'Paris is the city of France'}

=====	 I want to eat a cold [MASK] this afternoon
{'score': 0.19168031215667725, 'token': 13473, 'token_str': 'pizza', 'sequence': 'I want to eat a cold pizza this afternoon'}
{'score': 0.14800849556922913, 'token': 25138, 'token_str': 'turkey', 'sequence': 'I want to eat a cold turkey this afternoon'}
{'score': 0.14620967209339142, 'token': 14327, 'token_str': 'sandwich', 'sequence': 'I want to eat a cold sandwich this afternoon'}
{'score': 0.09997560828924179, 'token': 5953, 'token_str': 'lunch', 'sequence': 'I want to eat a cold lunch this afternoon'}
{'score': 0.06001955270767212, 'token': 4014, 'token_str': 'dinner', 'sequence': 'I want to eat a cold dinner this afternoon'}

=====	 Maria [MASK] Groningen
{'score': 0.24399833381175995, 'token': 117, 'token_str': ',', 'sequence': 'Maria, Groningen'}
{'score': 0.12300779670476913, 'token': 1104, 'token_str': 'of', 'sequence': 'Maria of Groningen'}
{'score': 0.11991506069898605, 'token': 1107, 'token_str': 'in', 'sequence': 'Maria in Groningen'}
{'score': 0.07722211629152298, 'token': 1306, 'token_str': '##m', 'sequence': 'Mariam Groningen'}
{'score': 0.0632941722869873, 'token': 118, 'token_str': '-', 'sequence': 'Maria - Groningen'}
</code></pre>
<p>When we call the <code>nlp</code> pipeline, requesting to return the
<code>top_k</code> most likely suggestions to complete the provided
sentences (in this case <code>k=5</code>). The pipeline returns a list
of outputs as Python dictionaries. Depending on the task, the fields of
the dictionary will differ. In this case, the <code>fill-mask</code>
task returns a score (between 0 and 1, the higher the score the more
likely the token is), a tokenId, and its corresponding string, as well
as the full “unmasked” sequence.</p>
<p>In the list of outputs we can observe: the first example shows
correctly that the missing token in the first sentence is
<em>capital</em>, the second example is a bit more ambiguous, but the
model at least uses the context to correctly predict a series of items
that can be eaten (unfortunately, none of its suggestions sound very
tasty); finally, the third example gives almost no useful context so the
model plays it safe and only suggests prepositions or punctuation. This
already shows some of the weaknesses of the approach.</p>
<div id="challenge3" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div class="callout-inner">
<h3 class="callout-title">Challenge</h3>
<div class="callout-content">
<p>Play with the <code>fill-mask</code> pipeline and try to find
examples where the model gives bad predictions and examples where the
predictions are very good. You can try:</p>
<ul>
<li>Changing the <code>top_k</code> parameter</li>
<li>Test the multilingual BERT model to compare. To do this, you should
change the <code>model</code> and <code>tokenizer</code> parameter name
to <code>bert-base-multilingual-cased</code>
</li>
<li>Search for bias in completions. For example, compare predictions for
“This man works as a [MASK].” vs. “This woman works as a [MASK].”.</li>
</ul>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3"> Show me the solution </h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" data-bs-parent="#accordionSolution3" aria-labelledby="headingSolution3">
<div class="accordion-body">
<p>This is a free exercise, so anything works. But even by running the
same cases with the multilingual models we see some interesting aspects.
For example, the predictions are of less quality in English. This is due
to the “spread” of information across other languages, including a worse
tokenization, since this model tries do predict for around 200
languages.</p>
<p>Another interesting example is searching for <strong>bias in the
completions</strong>, these can be bias in many areas. In this case,
comparing the outputs you get for the sentences “This man works as a
[MASK].” and “This woman works as a [MASK].” exposes the huge gender
biases inside BERT word representations: for ‘man’ BERT predicts
<code>['lawyer', 'carpenter', 'doctor', 'waiter', 'mechanic']</code> and
for woman it predicts
<code>['nurse', 'waitress', 'teacher', 'main', 'prostitute']</code>.</p>
</div>
</div>
</div>
</div>
<p>We will next see the case of combining BERT with a classifier on
top.</p>
</section><section><h2 class="section-heading" id="bert-for-text-classification">BERT for Text Classification<a class="anchor" aria-label="anchor" href="#bert-for-text-classification"></a>
</h2>
<hr class="half-width">
<p>The task of text classification is assigning a label to a whole
sequence of tokens, for example a sentence. With the parameter
<code>task="text_classification"</code> the <code>pipeline()</code>
function will load the base model and automatically add a linear layer
with a softmax on top. This layer can be fine-tuned with our own labeled
data or we can also directly load the fully pre-trained text
classification models that are already available in HuggingFace.</p>
<figure><img src="fig/bert4.png" alt="BERT as an Emotion Classifier" class="figure mx-auto d-block"><div class="figcaption">BERT as an Emotion Classifier</div>
</figure><p>Let’s see the example of a ready pre-trained emotion classifier based
on <code>RoBERTa</code> model. This model was fine-tuned in the Go
emotions <a href="https://huggingface.co/datasets/google-research-datasets/go_emotions" class="external-link">dataset</a>,
taken from English Reddit and labeled for 28 different emotions at the
sentence level. The fine-tuned model is called <a href="https://huggingface.co/SamLowe/roberta-base-go_emotions" class="external-link">roberta-base-go_emotions</a>.
This model takes a sentence as input and outputs a probability
distribution over the 28 possible emotions that might be conveyed in the
text. For example:</p>
<div class="codewrapper sourceCode" id="cb19">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" tabindex="-1"></a></span>
<span id="cb19-2"><a href="#cb19-2" tabindex="-1"></a>classifier <span class="op">=</span> pipeline(task<span class="op">=</span><span class="st">"text-classification"</span>, model<span class="op">=</span><span class="st">"SamLowe/roberta-base-go_emotions"</span>, top_k<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb19-3"><a href="#cb19-3" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" tabindex="-1"></a>sentences <span class="op">=</span> [<span class="st">"I am not having a great day"</span>, <span class="st">"This is a lovely and innocent sentence"</span>, <span class="st">"Maria loves Groningen"</span>]</span>
<span id="cb19-5"><a href="#cb19-5" tabindex="-1"></a>model_outputs <span class="op">=</span> classifier(sentences)</span>
<span id="cb19-6"><a href="#cb19-6" tabindex="-1"></a></span>
<span id="cb19-7"><a href="#cb19-7" tabindex="-1"></a>pretty_print_outputs(sentences, model_outputs)</span></code></pre>
</div>
<pre><code>=====	 I am not having a great day
{'label': 'disappointment', 'score': 0.46669483184814453}
{'label': 'sadness', 'score': 0.39849498867988586}
{'label': 'annoyance', 'score': 0.06806594133377075}

=====	 This is a lovely and innocent sentence
{'label': 'admiration', 'score': 0.6457845568656921}
{'label': 'approval', 'score': 0.5112180113792419}
{'label': 'love', 'score': 0.09214121848344803}

=====	 Maria loves Groningen
{'label': 'love', 'score': 0.8922032117843628}
{'label': 'neutral', 'score': 0.10132959485054016}
{'label': 'approval', 'score': 0.02525361441075802}</code></pre>
<p>This code outputs again a list of dictionaries with the
<code>top-k</code> (<code>k=3</code>) emotions that each of the two
sentences convey. In this case, the first sentence evokes (in order of
likelihood) <em>disappointment</em>, <em>sadness</em> and
<em>annoyance</em>; whereas the second sentence evokes <em>love</em>,
<em>neutral</em> and <em>approval</em>. Note however that the likelihood
of each prediction decreases dramatically below the top choice, so
perhaps this specific classifier is only useful for the top emotion.</p>
<div id="callout6" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div class="callout-inner">
<div class="callout-content">
<p>Fine-tuning BERT is very cheap, because we only need to train the
<em>classifier</em> layer, a very small neural network, that can learn
to choose between the classes (labels) for your custom classification
problem, without needing a big amount of annotated data. This classifier
is a one-layer neural layer that assigns a score that can be translated
to the probability over a set of labels, given the input features
provided by BERT, which already <em>encodes</em> the meaning of the
entire sequence in its hidden states. Unfortunately fine-tuning is out
of the scope of this course but you can learn more about fine-tuning
BERT-like models in <a href="https://huggingface.co/docs/transformers/v4.57.1/en/training#fine-tuning" class="external-link">this
HuggingFace tutorial</a></p>
</div>
</div>
</div>
<figure><img src="fig/bert4b.png" alt="BERT as an Emotion Classifier" class="figure mx-auto d-block"><div class="figcaption">BERT as an Emotion Classifier</div>
</figure></section><section><h2 class="section-heading" id="model-evaluation">Model Evaluation<a class="anchor" aria-label="anchor" href="#model-evaluation"></a>
</h2>
<hr class="half-width">
<p>Model evaluation is a critical step in any machine learning project,
and it is also the case for NLP. While it may be tempting to rely on the
accuracy scores observed during training, or the model descriptions
provided on the web, this approach can be misleading and often results
in models that fail in real-world applications, and that includes your
data.</p>
<div id="callout7" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div class="callout-inner">
<div class="callout-content">
<p>The fundamental principle of model evaluation is to test your model
on data it has never seen before. <strong>If you are training your own
models, this is typically achieved by splitting your dataset into
training, validation, and test sets</strong>. The training set is used
to teach the model, the validation set helps tune hyperparameters or
other aspects of model development, and the test set provides a less
biased assessment of the final model’s performance. Without this
separation, you risk overfitting, where your model memorizes the
training data rather than learning generalizable patterns.</p>
<p>If you are using out-of-the-box models, evaluation is also mandatory,
as you must be sure that the outputs you obtain behave in the manner
that you would expect. In this case you only need to design the test set
(with human annotators) and measure the performance. <strong>It doesn’t
matter how broadly a model was tested, you must always confirm that it
is suitable for your intended usage</strong>. It is also not enough to
use semi-automated evaluation, designing human-labeled data will also
provide insightful information for shaping your own task and judging the
predictions you get.</p>
</div>
</div>
</div>
<div class="section level3">
<h3 id="evaluation-metrics">Evaluation Metrics<a class="anchor" aria-label="anchor" href="#evaluation-metrics"></a>
</h3>
<p>Several metrics are commonly used to evaluate NLP models, each
offering different insights into performance. Here we will describe the
4 most used in suervised learning classifiers</p>
<p>Let’s give the toy example of classifying photographs of cats and
dogs. In this task, there are 2 classes: <code>cat</code> and
<code>dog</code>, and we have a model that return the label
<code>cat</code> or <code>dog</code> given an input photograph. If we
want to evaluate how good is our model at recognizing <code>dog</code>s
(our <em>positive class</em> in this example), there are 4 possibilities
when comparing with the ground truth (your labeled data):</p>
<ol style="list-style-type: decimal">
<li>
<strong>True Positives (TP):</strong> the number of <code>dog</code>
photographs the model <em>correctly</em> labeled as
<code>dog</code>.</li>
<li>
<strong>True Negatives (TN):</strong> the number of
non-<code>dog</code> photographs <em>correctly</em> labeled as something
else.</li>
<li>
<strong>False Positives (FP):</strong> the number of
non-<code>dog</code> photographs the model <em>mistakenly</em> labeled
as <code>dog</code>.</li>
<li>
<strong>False Negatives (FN):</strong> the number of
<code>dog</code> photographs <em>mistakenly</em> labeled as something
else.</li>
</ol>
<p>Based on this simple counts, we can derive four metrics that inform
us at scale about the performance of our classifiers:</p>
<ul>
<li>
<strong>Accuracy:</strong> measures the global proportion of correct
predictions, regardless of the class they hold. This is al true cases
(TP + TN) divided by all tested instances (TP+TN+FP+FN).</li>
<li>
<strong>Precision:</strong> This answers the following question: “Of
all the predictions the model made for a particular class, how many were
actually correct?”. This is TP divided by (TP+FP).</li>
<li>
<strong>Recall:</strong> This answers the following question: “Of
all the actual instances of a class, how many did the model successfully
identify?”. This is TP divided by (TP+FN).</li>
<li>
<strong>F1-score:</strong> provides a harmonic mean of precision and
recall, offering a single metric that balances both concerns.</li>
</ul>
<p>Deciding which metric is the most relevant to your case depends on
your specific task, but having a view at all of the metrics is always
insightful.</p>
<figure><img src="fig/bert_precisionRecall.png" alt="An example for a classifier of Cats and Dogs. Source: Wikipedia" class="figure mx-auto d-block"><div class="figcaption">An example for a classifier of Cats and Dogs.
Source: Wikipedia</div>
</figure><p><strong>It’s important to remember that a high accuracy score doesn’t
always indicate a good model</strong>. For example, if you’re
classifying rare events that occur only 5% of the time, a naive model
that always predicts “no event” would achieve 95% accuracy while being
completely useless. This is why examining multiple metrics and
understanding your data’s characteristics is essential for proper model
evaluation.</p>
<p>In Python, the <code>scikit-learn</code> package already provides us
with these (and many other) evaluation metrics. All we need to do is
prepare an ordered list with the <code>true_labels</code> and a list
with the corresponding <code>predicted_labels</code> for each example in
our data.</p>
<p>To illustrate the usage of evaluation, we will use a simpler
sentiment model that predicts 5 classes: <code>Very positive</code>,
<code>positive</code>, <code>neutral</code>, <code>negative</code> and
<code>very negative</code>. Here is an example of the model predictions
for four toy examples:</p>
<div class="codewrapper sourceCode" id="cb21">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb21-2"><a href="#cb21-2" tabindex="-1"></a></span>
<span id="cb21-3"><a href="#cb21-3" tabindex="-1"></a><span class="co"># Load the classification pipeline with the specified model</span></span>
<span id="cb21-4"><a href="#cb21-4" tabindex="-1"></a>pipe <span class="op">=</span> pipeline(<span class="st">"text-classification"</span>, model<span class="op">=</span><span class="st">"tabularisai/multilingual-sentiment-analysis"</span>)</span>
<span id="cb21-5"><a href="#cb21-5" tabindex="-1"></a></span>
<span id="cb21-6"><a href="#cb21-6" tabindex="-1"></a><span class="co"># Classify a new sentence</span></span>
<span id="cb21-7"><a href="#cb21-7" tabindex="-1"></a>sentences <span class="op">=</span> [</span>
<span id="cb21-8"><a href="#cb21-8" tabindex="-1"></a>    <span class="st">"I love this product! It's amazing and works perfectly."</span>,</span>
<span id="cb21-9"><a href="#cb21-9" tabindex="-1"></a>    <span class="st">"The movie was a bit boring, I could predict the ending since minute 1."</span>,</span>
<span id="cb21-10"><a href="#cb21-10" tabindex="-1"></a>    <span class="st">"Mary Shelley wrote this book around 1816."</span>,</span>
<span id="cb21-11"><a href="#cb21-11" tabindex="-1"></a>    <span class="st">"Everything suuuucks!"</span></span>
<span id="cb21-12"><a href="#cb21-12" tabindex="-1"></a>]</span>
<span id="cb21-13"><a href="#cb21-13" tabindex="-1"></a>gold_labels <span class="op">=</span> [</span>
<span id="cb21-14"><a href="#cb21-14" tabindex="-1"></a>    <span class="st">"Very Positive"</span>,</span>
<span id="cb21-15"><a href="#cb21-15" tabindex="-1"></a>    <span class="st">"Negative"</span>,</span>
<span id="cb21-16"><a href="#cb21-16" tabindex="-1"></a>    <span class="st">"Neutral"</span>,</span>
<span id="cb21-17"><a href="#cb21-17" tabindex="-1"></a>    <span class="st">"Very Negative"</span></span>
<span id="cb21-18"><a href="#cb21-18" tabindex="-1"></a>]</span>
<span id="cb21-19"><a href="#cb21-19" tabindex="-1"></a></span>
<span id="cb21-20"><a href="#cb21-20" tabindex="-1"></a><span class="co"># The pipeline can also run on a batch of examples</span></span>
<span id="cb21-21"><a href="#cb21-21" tabindex="-1"></a>result <span class="op">=</span> pipe(sentences)</span>
<span id="cb21-22"><a href="#cb21-22" tabindex="-1"></a></span>
<span id="cb21-23"><a href="#cb21-23" tabindex="-1"></a><span class="co"># Print the result</span></span>
<span id="cb21-24"><a href="#cb21-24" tabindex="-1"></a>predicted_labels <span class="op">=</span> []</span>
<span id="cb21-25"><a href="#cb21-25" tabindex="-1"></a><span class="cf">for</span> res <span class="kw">in</span> result:</span>
<span id="cb21-26"><a href="#cb21-26" tabindex="-1"></a>    <span class="bu">print</span>(res)</span>
<span id="cb21-27"><a href="#cb21-27" tabindex="-1"></a>    predicted_labels.append(res[<span class="st">'label'</span>])</span></code></pre>
</div>
<pre><code>{'label': 'Very Positive', 'score': 0.5586304068565369}
{'label': 'Negative', 'score': 0.9448591470718384}
{'label': 'Neutral', 'score': 0.9229359030723572}
{'label': 'Very Negative', 'score': 0.36225152015686035}</code></pre>
<p>We can see that the model predicts correctly the 4 examples we gave.
This is unsurprising as they are incredibly obvious examples. We can
also print the results and inspect them because they are only 4
instances, but it is clearly not a scalable approach.</p>
<div id="callout8" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div class="callout-inner">
<div class="callout-content">
<p>Note that many models will provide a confidence <code>score</code>,
with their predictions. It is very tempting to interpret these scores as
a proxy to “how certain is the model of prediction X”. However, you
should be very careful, this score is only a relative confidence measure
with respect to the training data, and it does not always translate well
to unseen data. Most of the times it is better to just ignore it,
especially if it is a model that you didn’t train yourself.</p>
</div>
</div>
</div>
<p>We can obtain an automated evaluation report, including the basic
evaluation metrics, from <code>scikit-learn</code> by calling:</p>
<div class="codewrapper sourceCode" id="cb23">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> classification_report</span>
<span id="cb23-2"><a href="#cb23-2" tabindex="-1"></a></span>
<span id="cb23-3"><a href="#cb23-3" tabindex="-1"></a><span class="bu">print</span>(classification_report(y_true<span class="op">=</span>gold_labels, y_pred<span class="op">=</span>predicted_labels))</span></code></pre>
</div>
<p>The output is a table showing a breakdown of metrics by class. The
column <code>support</code> shows how many examples per class were
present in the gold data.</p>
<pre><code>               precision    recall  f1-score   support

     Negative       1.00      1.00      1.00         1
      Neutral       1.00      1.00      1.00         1
Very Negative       1.00      1.00      1.00         1
Very Positive       1.00      1.00      1.00         1

     accuracy                           1.00         4
    macro avg       1.00      1.00      1.00         4
 weighted avg       1.00      1.00      1.00         4</code></pre>
<p>These 4 metrics range from 0 to 1 (note that sometimes people
multiply the scores by 100 to gain more granularity with decimal
places). In this case, because we had a <em>perfect</em> score
everything amounts to 1. In the most catastrophic scenario all scores
would be zero.</p>
<div id="evaluate-sentiment-classifier" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="evaluate-sentiment-classifier" class="callout-inner">
<h3 class="callout-title">Evaluate Sentiment Classifier</h3>
<div class="callout-content">
<p>Now it is time to scale things a little bit more… Use the same
pipeline from the given toy example to run predictions over 100 examples
of short book reviews. Then print the classification report for the
given <em>test set</em>. These examples are given in the
<code>data/sentiment_film_data.tsv</code> file.</p>
<p>You can use the following helper functions, the first one helps you
read the file and the second one normalizes the 5-class predictions into
the 3-class annotations given in the test set:</p>
<div class="codewrapper sourceCode" id="cb25">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" tabindex="-1"></a><span class="kw">def</span> load_data(filename):</span>
<span id="cb25-2"><a href="#cb25-2" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(filename, <span class="st">'r'</span>) <span class="im">as</span> f:</span>
<span id="cb25-3"><a href="#cb25-3" tabindex="-1"></a>        lines <span class="op">=</span> f.readlines()[<span class="dv">1</span>:] <span class="co"># skip header</span></span>
<span id="cb25-4"><a href="#cb25-4" tabindex="-1"></a>    sentences, labels <span class="op">=</span> <span class="bu">zip</span>(<span class="op">*</span>(line.strip().split(<span class="st">'</span><span class="ch">\t</span><span class="st">'</span>) <span class="cf">for</span> line <span class="kw">in</span> lines))</span>
<span id="cb25-5"><a href="#cb25-5" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">list</span>(sentences), <span class="bu">list</span>(labels)</span>
<span id="cb25-6"><a href="#cb25-6" tabindex="-1"></a></span>
<span id="cb25-7"><a href="#cb25-7" tabindex="-1"></a><span class="kw">def</span> get_normalized_labels(predictions):</span>
<span id="cb25-8"><a href="#cb25-8" tabindex="-1"></a>    <span class="co"># predicitons is a list with dicts such as {'label': 'positive', 'score': 0.95}</span></span>
<span id="cb25-9"><a href="#cb25-9" tabindex="-1"></a>    <span class="co"># We also need to normalize the labels to match the true labels (which are only 'positive' and 'negative')</span></span>
<span id="cb25-10"><a href="#cb25-10" tabindex="-1"></a>    normalized <span class="op">=</span> []</span>
<span id="cb25-11"><a href="#cb25-11" tabindex="-1"></a>    <span class="cf">for</span> pred <span class="kw">in</span> predictions:</span>
<span id="cb25-12"><a href="#cb25-12" tabindex="-1"></a>        label <span class="op">=</span> pred[<span class="st">'label'</span>].lower()</span>
<span id="cb25-13"><a href="#cb25-13" tabindex="-1"></a>        <span class="cf">if</span> <span class="st">'positive'</span> <span class="kw">in</span> label:</span>
<span id="cb25-14"><a href="#cb25-14" tabindex="-1"></a>            normalized.append(<span class="st">'positive'</span>)</span>
<span id="cb25-15"><a href="#cb25-15" tabindex="-1"></a>        <span class="cf">elif</span> <span class="st">'negative'</span> <span class="kw">in</span> label:</span>
<span id="cb25-16"><a href="#cb25-16" tabindex="-1"></a>            normalized.append(<span class="st">'negative'</span>)</span>
<span id="cb25-17"><a href="#cb25-17" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb25-18"><a href="#cb25-18" tabindex="-1"></a>            normalized.append(<span class="st">'neutral'</span>)</span>
<span id="cb25-19"><a href="#cb25-19" tabindex="-1"></a>    <span class="cf">return</span> normalized</span></code></pre>
</div>
</div>
</div>
</div>
<div id="accordionSolution4" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution4" aria-expanded="false" aria-controls="collapseSolution4">
  <h4 class="accordion-header" id="headingSolution4"> Show me the solution </h4>
</button>
<div id="collapseSolution4" class="accordion-collapse collapse" data-bs-parent="#accordionSolution4" aria-labelledby="headingSolution4">
<div class="accordion-body">
<div class="codewrapper sourceCode" id="cb26">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> classification_report, precision_recall_fscore_support</span>
<span id="cb26-2"><a href="#cb26-2" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb26-3"><a href="#cb26-3" tabindex="-1"></a></span>
<span id="cb26-4"><a href="#cb26-4" tabindex="-1"></a>sentences, labels <span class="op">=</span> load_data(<span class="st">'data/sentiment_film_data.tsv'</span>)</span>
<span id="cb26-5"><a href="#cb26-5" tabindex="-1"></a><span class="co"># The labels from our dataset</span></span>
<span id="cb26-6"><a href="#cb26-6" tabindex="-1"></a>y_true <span class="op">=</span> labels</span>
<span id="cb26-7"><a href="#cb26-7" tabindex="-1"></a><span class="co"># Run the model to get predictions per sentence</span></span>
<span id="cb26-8"><a href="#cb26-8" tabindex="-1"></a>y_pred <span class="op">=</span> pipe(sentences)</span>
<span id="cb26-9"><a href="#cb26-9" tabindex="-1"></a><span class="co"># Normalize the labels to match the gold standard</span></span>
<span id="cb26-10"><a href="#cb26-10" tabindex="-1"></a>y_pred <span class="op">=</span> get_normalized_labels(y_pred)</span>
<span id="cb26-11"><a href="#cb26-11" tabindex="-1"></a></span>
<span id="cb26-12"><a href="#cb26-12" tabindex="-1"></a><span class="co"># Detailed report with all metrics</span></span>
<span id="cb26-13"><a href="#cb26-13" tabindex="-1"></a><span class="bu">print</span>(classification_report(y_true, y_pred))</span></code></pre>
</div>
<p>Here is the classification report:</p>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>              precision    recall  f1-score   support

    negative       0.57      1.00      0.73        23
     neutral       0.53      0.22      0.31        37
    positive       0.69      0.78      0.73        40

    accuracy                           0.62       100
   macro avg       0.60      0.66      0.59       100
weighted avg       0.61      0.62      0.57       100</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="confusion-matrix">Confusion Matrix<a class="anchor" aria-label="anchor" href="#confusion-matrix"></a>
</h3>
<p>The confusion matrix is another direct and informative tool for
understanding your model’s performance, by offering an intuitive
visualization of your model’s behavior in detail. It is a table that
compares your model’s predictions against the true labels. The
<strong>rows typically represent the actual classes, while the columns
show the predicted classes</strong>. Each cell contains the count of
instances that fall into that particular combination of true and
predicted labels. Perfect predictions would result in all counts
appearing along the diagonal of the matrix, with zeros everywhere
else.</p>
<div class="codewrapper sourceCode" id="cb28">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> ConfusionMatrixDisplay</span>
<span id="cb28-2"><a href="#cb28-2" tabindex="-1"></a></span>
<span id="cb28-3"><a href="#cb28-3" tabindex="-1"></a><span class="kw">def</span> show_confusion_matrix(y_true, y_pred, labels<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb28-4"><a href="#cb28-4" tabindex="-1"></a>    ConfusionMatrixDisplay.from_predictions(y_true, y_pred, display_labels<span class="op">=</span>labels, cmap<span class="op">=</span><span class="st">'Blues'</span>)</span>
<span id="cb28-5"><a href="#cb28-5" tabindex="-1"></a>    plt.show()</span>
<span id="cb28-6"><a href="#cb28-6" tabindex="-1"></a></span>
<span id="cb28-7"><a href="#cb28-7" tabindex="-1"></a>show_confusion_matrix(y_true, y_pred)</span></code></pre>
</div>
<p>This code shows the following matrix:</p>
<figure><img src="fig/bert_sentiment_matrix.png" class="figure mx-auto d-block"></figure><p>Some of the insights from a confusion matrix include:</p>
<ul>
<li>Observe which classes your model handles well and which ones it
struggles with.</li>
<li>Observe confusion patterns between classes. By examining the
off-diagonal cells, you can identify systematic errors your model makes.
For example, perhaps your sentiment classifier consistently confuses
neutral statements with positive ones, but rarely mistakes neutral for
negative.</li>
<li>Detect Bias, for example by exposing a tendency to over-predict
certain classes while ignoring others.</li>
<li>Detect class imbalance. Even if your overall accuracy seems high,
the confusion matrix might reveal that your model achieves this by
simply predicting the majority class most of the time.</li>
</ul>
</div>
</section><section><h2 class="section-heading" id="bert-for-token-classification">BERT for Token Classification<a class="anchor" aria-label="anchor" href="#bert-for-token-classification"></a>
</h2>
<hr class="half-width">
<p>Just as we plugged in a trainable text classifier layer, we can add a
token-level classifier that assigns a class to each of the tokens
encoded by a transformer (as opposed to one label for the whole
sequence). A specific example of this task is Named Entity Recognition,
but you can basically define any task that requires to
<em>highlight</em> sub-strings of text and classify them using this
technique.</p>
<div class="section level3">
<h3 id="named-entity-recognition">Named Entity Recognition<a class="anchor" aria-label="anchor" href="#named-entity-recognition"></a>
</h3>
<p>Named Entity Recognition (NER) is the task of recognizing mentions of
real-world entities inside a text. The concept of
<strong>Entity</strong> includes proper names that unequivocally
identify a unique individual (PER), place (LOC), organization (ORG), or
other object/name (MISC). Depending on the domain, the concept can
expanded to recognize other unique (and more conceptual) entities such
as DATE, MONEY, WORK_OF_ART, DISEASE, PROTEIN_TYPE, etcetera…</p>
<p>In terms of NLP, this boils down to classifying each token into a
series of labels (<code>PER</code>, <code>LOC</code>, <code>ORG</code>,
<code>O</code>[no-entity] ). Since a single entity can be expressed with
multiple words (e.g. New York) the usual notation used for labeling the
text is IOB (<strong>I</strong>nner <strong>O</strong>ut
<strong>B</strong>eginnig of entity) notations which identifies the
limits of each entity tokens. For example:</p>
<figure><img src="fig/bert5.png" alt="BERT as an NER Classifier" class="figure mx-auto d-block"><div class="figcaption">BERT as an NER Classifier</div>
</figure><p>This is a typical sequence classification problem where an imput
sequence must be fully mapped into an output sequence of labels with
global constraints (for example, there can’t be an inner I-LOC label
before a beginning B-LOC label). Since the labels of the tokens are
context dependent, a language model with attention mechanism such as
BERT is very beneficial for a task like NER.</p>
<p>Because this is one of the core tasks in NLP, there are dozens of
pre-trained NER classifiers available in HuggingFace that you can use
right away. We use once again the <code>pipeline()</code> wrapper to
directly run the model for predictions , in this case with
<code>task="ner"</code>. For example:</p>
<div class="codewrapper sourceCode" id="cb29">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer, AutoModelForTokenClassification</span>
<span id="cb29-2"><a href="#cb29-2" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb29-3"><a href="#cb29-3" tabindex="-1"></a></span>
<span id="cb29-4"><a href="#cb29-4" tabindex="-1"></a><span class="co"># We can also pass to the pipeline the initialized model &amp; tokenizer. </span></span>
<span id="cb29-5"><a href="#cb29-5" tabindex="-1"></a><span class="co"># This way we have access to both separately if we need them later</span></span>
<span id="cb29-6"><a href="#cb29-6" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(<span class="st">"dslim/bert-base-NER"</span>)</span>
<span id="cb29-7"><a href="#cb29-7" tabindex="-1"></a>model <span class="op">=</span> AutoModelForTokenClassification.from_pretrained(<span class="st">"dslim/bert-base-NER"</span>)</span>
<span id="cb29-8"><a href="#cb29-8" tabindex="-1"></a></span>
<span id="cb29-9"><a href="#cb29-9" tabindex="-1"></a>ner_classifier <span class="op">=</span> pipeline(<span class="st">"token-classification"</span>, model<span class="op">=</span>model, tokenizer<span class="op">=</span>tokenizer, aggregation_strategy<span class="op">=</span><span class="st">"first"</span>)</span>
<span id="cb29-10"><a href="#cb29-10" tabindex="-1"></a>example <span class="op">=</span> <span class="st">"My name is Wolfgang Schmid and I live in Berlin"</span></span>
<span id="cb29-11"><a href="#cb29-11" tabindex="-1"></a></span>
<span id="cb29-12"><a href="#cb29-12" tabindex="-1"></a>ner_results <span class="op">=</span> ner_classifier(example)</span>
<span id="cb29-13"><a href="#cb29-13" tabindex="-1"></a><span class="cf">for</span> nr <span class="kw">in</span> ner_results:</span>
<span id="cb29-14"><a href="#cb29-14" tabindex="-1"></a>    <span class="bu">print</span>(nr)</span></code></pre>
</div>
<p>The code now prints the following:</p>
<pre><code>{'entity_group': 'PER', 'score': 0.9995944, 'word': 'Wolfgang Schmid', 'start': 11, 'end': 26}
{'entity_group': 'LOC', 'score': 0.99956733, 'word': 'Berlin', 'start': 41, 'end': 47}</code></pre>
<p>As you can see, the outputs contain already the entities aggregated
at the Span Leven (instead of the Token Level). Word pieces are merged
back into <em>human words</em> and also multiword entities are assigned
a single entity unified label. Depending on your use case you can
request the pipeline to give different
<code>aggregation_strateg[ies]</code>. More info about the pipeline can
be found <a href="https://huggingface.co/docs/transformers/main_classes/pipelines" class="external-link">here</a>.</p>
<p>The same evaluation metrics you learned for the text classifier can
also be applied to this classification task. A common Python library to
deal with token classification is <a href="https://github.com/chakki-works/seqeval" class="external-link">seqeval</a> or <a href="https://github.com/huggingface/evaluate?tab=readme-ov-file" class="external-link">evaluate</a>.
Remember to always test on a significant human-labeled dataset to assess
if the predictions you are getting make sense. If they don’t, more
advances use of the models, including fine-tuning should be used.</p>
<p>What did we learn in this lesson?</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<span class="callout-header">Key Points</span>
<div class="callout-inner">
<div class="callout-content">
<ul>
<li><p>Static word representations, such as word2vec, still lack of
enough context to do more advanced tasks, we made this weakness evident
by studying polysemy.</p></li>
<li><p>The transformer architecture consists of three main components:
an <strong>Encoder</strong> to create powerful text representations
(embeddings), an <strong>Attention Mechanism</strong> to learn more from
the full sequence context, and a <strong>Decoder</strong>, a generative
model to predicts the next token based on the context it has so
far.</p></li>
<li><p>BERT is a deep encoder that creates rich contextualized
representations of words and sentences. These representations are very
powerful features that can be re-used by other machine Learning and deep
learning models.</p></li>
<li><p>Several of the core NLP tasks can be solved using
Transformer-based models. In this episode we covered language modeling
(fill-in the mask), text classification (sentiment analysis) and token
classification (named entity recognition).</p></li>
<li><p>Evaluating the model performance using your own data <strong>for
your own use case</strong> is crucial to understand possible drawbacks
when using this model for unknown predictions</p></li>
</ul>
</div>
</div>
</div>
</div>
</section></section><section id="aio-04-LargeLanguageModels"><p>Content from <a href="04-LargeLanguageModels.html">Using large language models</a></p>
<hr>
<p>Last updated on 2025-11-27 |

        <a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/edit/main/episodes/04-LargeLanguageModels.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<section><h2 class="section-heading" id="background">Background<a class="anchor" aria-label="anchor" href="#background"></a>
</h2>
<hr class="half-width">
<p>Chat assistants like <a href="https://chatgpt.com/" class="external-link">ChatGPT</a>, <a href="https://gemini.google.com/" class="external-link">Gemini</a> and <a href="http://claude.ai" class="external-link">Claude</a> are products widely used today for
tasks such as content generation, question answering, research and
software development among many others. These products, also known as
Large Language Modesl (LLMs), are based on the Transformer architecture,
with several enhancements. It is undeniable that the rapid rise of such
models has had quite a disruptive and strong impact. But what are these
models exactly? How do they work ‘under the hood’? And how can one use
them programmatically, in a responsible and effective way?</p>
<p><img src="fig/llm-logos/anthropic.png" alt="Company A" width="80" class="figure"><img src="fig/llm-logos/alibaba.png" alt="Company B" width="80" class="figure"><img src="fig/llm-logos/xai.jpg" alt="Company C" width="80" class="figure"><img src="fig/llm-logos/zhipu.png" alt="Company C" width="150" class="figure"><img src="fig/llm-logos/google.png" alt="Company C" width="80" class="figure"><img src="fig/llm-logos/openai.jpg" alt="Company C" width="150" class="figure"></p>
<p><img src="fig/llm-logos/nvidia.png" alt="Company D" width="80" class="figure"><img src="fig/llm-logos/deepseek.png" alt="Company E" width="80" class="figure"><img src="fig/llm-logos/huggingface.png" alt="Company F" width="80" class="figure"><img src="fig/llm-logos/meta.png" alt="Company C" width="150" class="figure"></p>
<p>Now that we know that dealing with written language means extracting
structure from unstructured text (NLP Task), we can understand there is
a rationale behind the kind of tasks LLMs could tackle. Everytime we
prompt a chat model, we are feeding them a very long list of tokens
containing a task that needs to get solved. The question that interests
us is not about LLMs being “intelligent” or not (eventhough that could
be a very interesting topic on its own, is not the goal of this course);
we will rather focus on using LLMs as yet a new <em>automatic tool</em>,
and like with any other Machine Learning tool, we are still dealing here
with a model that takes an input and delivers an output for that input.
The only thing that changed is the complexity of the input-output pairs;
and hence, the complexity for validating the outputs increased
accordingly.</p>
<p>Our duty as NLP practitioners remains, and we must keep asking the
same questions:</p>
<ul>
<li>How can I shape the task so I obtain the information I need?</li>
<li>How do I measure what proportion of the outputs are right?</li>
<li>How do I know if this will behave as expected with unexpected
inputs?</li>
<li><strong>In short: How do I evaluate my task?</strong></li>
</ul>
<p>This episode is a gentle introduction to LLMs which aims to equip you
with knowledge of the underpinnings of LLMs based on transformers
architecture, as well as practical skills to start programmatically
working with LLMs in your own projects, without necessarily relying on
proprietary products and platforms.</p>
</section><section><h2 class="section-heading" id="what-are-large-language-models-llms">What are Large Language Models (LLMs)?<a class="anchor" aria-label="anchor" href="#what-are-large-language-models-llms"></a>
</h2>
<hr class="half-width">
<p>Large language models (LLMs) are generative transformer-based
language models that are trained to interact in a conversational-like
manner with humans. The text that they generate are mostly natural
language but can, in theory, constitute any sequence of characters and
symbols, such as software code. The term <em>Large</em> was appended to
the well known <em>Language Model</em> term to highlight the scale on
which this architectures are trained. To give an example, BERT in 2018
was considered a big model and had roughly 100 million parameters; GPT-2
in 2019 had 1.5 billion parameters, and GPT-3 was published in 2020 as a
model with 175 billion parameters, and so forth.</p>
<div id="callout1" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div class="callout-inner">
<div class="callout-content">
<p>Since we already learned about the vanilla transformer architecture,
in this episode we will focus on the most recent language models, and we
will keep calling them LLMs, eventhough they are not necessarily that
large anymore. In this episode we will cover mostly models that have
been fine-tuned to be chat assistants, capable of integrating into the
text generation a multi-turn interaction.</p>
<p>Given the hype around the field, people keep calling all new models
(and some “old models” as well) an <em>LLM</em> or even worse just
<em>AI</em>. <strong>We will stick here to the term LLM to avoid
confusions</strong>. Have in mind, however, that especially recent
models are being published with the particular goal of reducing
parameter size while retaining the performance of the larger models,
some of them are even less than 1 billion parameters already! This is
good news for the open source engineers and researchers, because such
advancements mean we can now make use of the new capabilities of
language models in our own local servers, and even our laptops, without
needing to pay fees or compromise the privacy of our data and
experiments.</p>
</div>
</div>
</div>
<div class="section level3">
<h3 id="transformers-vs--llms">Transformers vs. LLMs<a class="anchor" aria-label="anchor" href="#transformers-vs--llms"></a>
</h3>
<p>To emphasize again, LLMs are also trained using the transformer
neural network architecture, including the use of the self-attention
mechanism inside the generative Decoder (as discussed in Lesson 3).
However, they are three main characteristics that the newest generations
of LLMs have:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Scale:</strong> there are three dimensions in which
current LLMs exceed general transformer language models in terms of
scale. The most important one is the number of <strong>training
parameters</strong> (weights) that are used for training models. In
current models there are hundreds of billions of parameters up to
trillions. The second factor is the <strong>amount of training
data</strong> (raw text sequences) used for training. Current LLMs use
snapshots of the internet (upwards of hundreds of terabytes in size) as
a base for training and possibly augment this with additional manually
curated and artificially generated data. The third factor is the
<strong>context window</strong> a model can handle, this is the amount
of tokens a model can see and process at a time; to give perspective,
BERT was able to handle 512 input tokens per interaction, whereas some
LLMs are already able to process a couple of million tokens at a time.
The sheer scale characteristic of LLMs mean that such models require
extremely resource-intensive computation to train. State-of-the-art LLMs
require multiple dedicated Graphical Processing Units (GPUs) with tens
or hundreds of gigabytes of memory to load and train in reasonable time.
GPUs offer high parallelisability in their architecture for data
processing which makes them more efficient for training these
models.</p></li>
<li>
<p><strong>Post-training:</strong> After training a base language
model on textual data, there is an additional step of fine-tuning for
enabling conversation in a prompt style of interaction with users, which
current LLMs are known for. After the pre-training and neural network
training stages we end up with what is called a <em>base</em> model. The
base model is a language model which is essentially a token sequence
generator optimizad for the netx token prediction (Language Modelling)
task. This model by itself is not suitable for the interaction style we
see with current LLMs, which can do things like answer questions,
interpret instructions from the user, and incorporate feedback to
improve responses in conversations. Post-training addicitons
include:</p>
<ul>
<li>Supervised Fine-Tuning (SFT): Training on curated examples of
desired input-output pairs to teach instruction-following and helpful
responses.</li>
<li>Reinforcement Learning from Human Feedback (RLHF): Using human
preference data to align model outputs with what humans find
helpful.</li>
<li>“Safety” Training: Include so-called guardrails to avoid generating
harmful outputs.</li>
<li>Specialized Capabilities: Often includes training for specific
skills like coding, reasoning, or tool use</li>
</ul>
</li>
<li><p><strong>Generalization:</strong> Because of the wide range of
post-training tasks they’ve been through, LLMs can be “directly applied”
across a wide range of NLP tasks such as summarization, translation,
question answering, etc., without necessarily the need for fine-tuning
or training separate models for different NLP tasks. They are also
capable of calling external tools or follow more complicated
instructions that go beyond the next word probabilities.</p></li>
</ol>
<figure><img src="fig/llm_analogy3.png" alt="llm engine analogy" width="1000" class="figure mx-auto d-block"></figure><p>What about the relation between BERT, which we learned about in the
previous episode, and LLMs? Apart from the differences described above,
BERT only makes use of the encoder layer of the transformers
architecture because the goal is on creating token representations
preserving contextual meaning (Embedding Model). There is no generative
component to do something with those representations. <br></p>
<figure><img src="fig/llms_vs_bert2.png" alt="llms vs bert" width="800" class="figure mx-auto d-block"></figure>
</div>
<div class="section level3">
<h3 id="the-llm-taxonomy">The LLM Taxonomy<a class="anchor" aria-label="anchor" href="#the-llm-taxonomy"></a>
</h3>
<p>Many different LLMs have been, and continue to be, developed. There
are both proprietary and open-source varieties. Real open-source
varieties should make the whole path of model creation available: inform
exactly which data was used to train the model, including filters for
data quality; give a detailed explanation of the architecture and
hypermarameters, including the code used to train them, and of course
make them free, open and accessible online. Unfortunately completely ope
source models are scarce, but the partially open source number models
keeps growing. Below is a summary of some relevant LLMs together with
their creators, chat assistant interfaces, and proprietary status:</p>
<figure><img src="fig/llm_table4.png" alt="LLMs table" width="1000" class="figure mx-auto d-block"></figure><div class="section level4">
<h4 id="openness-and-licensing-considerations">Openness and Licensing Considerations<a class="anchor" aria-label="anchor" href="#openness-and-licensing-considerations"></a>
</h4>
<p>The spectrum of model availability ranges from fully open to
completely proprietary:</p>
<p><strong>Open-weights</strong> release the trained model parameters
while keeping training code or data proprietary. This allows you to run
and fine-tune the model locally but if you don’t have the code used to
train the model or information about the architecture used, it limits
your ability to fully understand or replicate the training process.</p>
<p><strong>Open training data</strong> they release the text data used
for pretraining.</p>
<p><strong>Open architecture</strong> they publish a paper about the
neural network architecture and specific configuration they used for
training. Or they release the actual source code they used for
pretraining.</p>
<p>Ideally, if you want to use a model for empirical academic research
you might decide for models that are completely open in all three of the
above facets. Although, open training data is quite rare for available
state-of-the-art models.</p>
<p><strong>Commercial/proprietary models</strong> like GPT-4, Claude, or
Gemini are accessed only through APIs. While often offering superior
performance, they provide no access to internal architecture and may
have usage restrictions or costs that scale with volume.</p>
<p>Consider your requirements for: - Code modification and customization
- Data privacy and control - Commercial usage rights - Research
reproducibility - Long-term availability guarantees</p>
<div id="callout2" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div class="callout-inner">
<div class="callout-content">
<p>Besides considering the openess of the models, there are several
<em>families</em> of LLM to consider, which differ not only in training
data and scale of parameters but also in training and post-training
techniques. Here we show a (non-exhaustive) general list of categories
that have branched out from the vanilla generative transformer language
models, understanding the difference can also help you choose the proper
model.</p>
<p>The two main distinctions of architecture are:</p>
<ul>
<li>
<strong>Embedder LLM</strong> (BERT, OpenAI Embed, Cohere Embed,
Nomic): these are the “encoder only” LLMs which have been optimized for
holding representations that help to compute sentence/paragraph
similarities.</li>
<li>
<strong>Base Generative LLM</strong> (GPT, LLama, GPT3): this is the
generative “decoder-only” transformer that is trained to generate the
most likely token based on the previous tokens.</li>
</ul>
<p>All the next ones are basically “post-trained” (SFT + RLHF)
generative models with a different focus. These are not hard categories,
but you will most likely encounter the concepts everywhere in the
Web:</p>
<ul>
<li><p><strong>Domain-specific LLM:</strong> a base model fine-tuned
with specialized data like medical papers or legal cases and with RLHF.
Couls also be specialized on Question-Answering datasets, etc…</p></li>
<li><p><strong>LLM-Instruct</strong> (Claude, Llama-Instruct): these are
the LLMs that have been fine-tuned with step-by-step human instructions
to follow user instructions</p></li>
<li><p><strong>Reasoning/Thinking LLM</strong> (Cohere Command-R, OpenAI
o1, Gemini 2.5, …): these models have been post-trained to generate
“thinking tokens” in between, before providing the final answer to the
user, with the aim of improving complex task solving capabilities. This
ones can also give structured output, like JSON.</p></li>
<li><p><strong>Tool Augmented LLMs</strong> (GPT4): these models have
been post-trained for calling external tools like Web Search, API calls,
Execute code, etc… and then generate a final answer (token output)
integrating the tools’ results.</p></li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="interacting-with-an-llm">Interacting with an LLM<a class="anchor" aria-label="anchor" href="#interacting-with-an-llm"></a>
</h2>
<hr class="half-width">
<p>Before exploring how we can invoke LLMs programmatically to solve the
kinds of tasks above, let us setup and load our first LLM. We will keep
using the <code>transformers</code> library, just as with the BERT
examples.</p>
<div class="section level3">
<h3 id="import-libraries">Import Libraries<a class="anchor" aria-label="anchor" href="#import-libraries"></a>
</h3>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModelForCausalLM, AutoTokenizer, pipeline</span></code></pre>
</div>
</div>
<div class="section level3">
<h3 id="load-and-setup-the-model-llm">Load and setup the model (LLM)<a class="anchor" aria-label="anchor" href="#load-and-setup-the-model-llm"></a>
</h3>
<p>Let’s load a open source lightweight
<code>SmolLM2-135M-Instruct</code>, as you might have guessed it is a
model with 135M parameters which has been finetuned for following
instructions, therefore you can use it as a chat assistant:</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="co"># We'll use SmolLM-135M - an open, small, fast model</span></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>model_name <span class="op">=</span> <span class="st">"HuggingFaceTB/SmolLM2-135M-Instruct"</span> <span class="co"># fine-tuned assistant model</span></span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a><span class="co"># Load tokenizer and model</span></span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(model_name)</span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a>model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(model_name)</span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a><span class="co"># Check if model is loaded correctly</span></span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Model loaded! It has </span><span class="sc">{</span>model<span class="sc">.</span>num_parameters()<span class="sc">:,}</span><span class="ss"> parameters"</span>)</span></code></pre>
</div>
</div>
<div class="section level3">
<h3 id="basic-text-generation">Basic Text Generation<a class="anchor" aria-label="anchor" href="#basic-text-generation"></a>
</h3>
<p>Let’s perform inference with the LLM to generate some text.</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="co"># Set pad_token_id to eos_token_id to avoid warnings</span></span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a><span class="co"># if tokenizer.pad_token_id is None:</span></span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a><span class="co">#     tokenizer.pad_token = tokenizer.eos_token</span></span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a><span class="co">#     model.config.pad_token_id = tokenizer.eos_token_id</span></span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a>    </span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a><span class="co"># We Build pipeline as usual, this time the task is `text_generation` (a decoder model)</span></span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a>llm <span class="op">=</span> pipeline(<span class="st">"text-generation"</span>, model<span class="op">=</span>model, tokenizer<span class="op">=</span>tokenizer)</span>
<span id="cb3-8"><a href="#cb3-8" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"Where is Groningen located?"</span></span>
<span id="cb3-9"><a href="#cb3-9" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" tabindex="-1"></a><span class="co"># We can first look at how the text is also being converted into a sequence of tokens before generating</span></span>
<span id="cb3-11"><a href="#cb3-11" tabindex="-1"></a><span class="bu">print</span>(tokenizer(prompt))</span>
<span id="cb3-12"><a href="#cb3-12" tabindex="-1"></a><span class="bu">print</span>(tokenizer.convert_ids_to_tokens(tokenizer(prompt)[<span class="st">"input_ids"</span>]))</span></code></pre>
</div>
<p>This gives the following output:</p>
<pre><code>{'input_ids': [9576, 314, 452, 992, 45670, 3807, 47], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}
['Where', 'Ġis', 'ĠG', 'ron', 'ingen', 'Ġlocated', '?']</code></pre>
<p>Now we ask the pipeline to generate the tokens to complete the given
input. Remember that the pipeline post-processes the output token for us
and shows us the text string under <code>generated_text</code>
field:</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a>response <span class="op">=</span> llm(prompt)</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a><span class="bu">print</span>(response)</span></code></pre>
</div>
<p>This is not a fully deterministic ouput, so the content of the
response will vary, but the structure is always the same: a python list,
containing as first element a dictionary. The keys of the dictionary can
vary depending on the parameters given to the pipeline, but it will
always have at least the key <code>generated_text</code> which gives you
the models ouptut in the form of a python string:</p>
<pre><code>[{'generated_text': 'Where is Groningen located?\nGroningen is a municipality in the province of Drenthe in the North Holland region of the Netherlands. As of the 2011 census, the municipality had a population of 122,683.\nWhat is the name of the river that connects Groningen to Utrecht?'}]</code></pre>
<p>We can obtain only the text if we extract it directly:</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a>text <span class="op">=</span> response[<span class="dv">0</span>][<span class="st">"generated_text"</span>]</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a><span class="bu">print</span>(text)</span></code></pre>
</div>
<p>Note that the model “repeats” the question before completing it with
the answer. This happens because in reality the LLM is a Transformer
Decoder, which tokenizes your prompt, processes it token by token, and
then keeps <em>generating new tokens</em>. Because the model was trained
to act as an assistant in a conversation, the most likely tokens that
follow a questions are an answer to that question. But it is possible
that the model does not reply at all.</p>
<figure><img src="fig/llms_vs_bert2.png" alt="Generative LLMs correspond to the Decoder component of the Transformer architecture" class="figure mx-auto d-block"><div class="figcaption">Generative LLMs correspond to the Decoder
component of the Transformer architecture</div>
</figure>
</div>
<div class="section level3">
<h3 id="multi-turn-conversation-generation">Multi-turn Conversation Generation<a class="anchor" aria-label="anchor" href="#multi-turn-conversation-generation"></a>
</h3>
<p>When dealing with these kind of <em>assistant LLMs</em> it is a
better practice to look at the input as a conversation. A conversation
can be defined as a list of <em>messages</em> that the pipeline knows
how to internally tokenize and generate a better answer given the
context provided:</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a>messages <span class="op">=</span> [</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a>  {<span class="st">"role"</span>: <span class="st">"system"</span>, <span class="st">"content"</span>: <span class="st">"You are a helpful assistant. Give short straight answers"</span> },</span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a>  {<span class="st">"role"</span>: <span class="st">"user"</span>, <span class="st">"content"</span>: <span class="st">"Where is Groningen located?"</span> },</span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a>]</span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a>response <span class="op">=</span> llm(messages, return_full_text<span class="op">=</span><span class="va">False</span>)[<span class="dv">0</span>][<span class="st">"generated_text"</span>]</span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a><span class="bu">print</span>(response)</span></code></pre>
</div>
<p>This method makes it more likely to get an answer everytime the model
is executed. Remember this is not completely deterministic, so you can
run it several times. (Spoiler: Expect several falsehoods to be stated
in the generations). You should get something like:</p>
<pre><code>Groningen is located in the northwest corner of the Netherlands.</code></pre>
<p>We set the <code>return_full_text=False</code> flag to False to get
the response as a string. This parameter is by default set to True and
in that case it will return the whole conversation so you can keep using
the history for posterior interactions if you want to:</p>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a>messages <span class="op">=</span> [</span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a>  {<span class="st">"role"</span>: <span class="st">"system"</span>, <span class="st">"content"</span>: <span class="st">"You are a helpful assistant. Give short straight answers"</span> },</span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a>  {<span class="st">"role"</span>: <span class="st">"user"</span>, <span class="st">"content"</span>: <span class="st">"Where is Groningen located?"</span> },</span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a>]</span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a>response <span class="op">=</span> llm(messages)[<span class="dv">0</span>][<span class="st">"generated_text"</span>]</span>
<span id="cb10-6"><a href="#cb10-6" tabindex="-1"></a><span class="bu">print</span>(response)</span></code></pre>
</div>
<p>This gives a multi-turn conversation as a list of string messages,
such as:</p>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a>[{</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a>    <span class="st">'role'</span>: <span class="st">'system'</span>, </span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a>    <span class="st">'content'</span>: <span class="st">'You are a helpful assistant. Give short straight answers'</span></span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a> }, </span>
<span id="cb11-5"><a href="#cb11-5" tabindex="-1"></a> {</span>
<span id="cb11-6"><a href="#cb11-6" tabindex="-1"></a>    <span class="st">'role'</span>: <span class="st">'user'</span>, </span>
<span id="cb11-7"><a href="#cb11-7" tabindex="-1"></a>    <span class="st">'content'</span>: <span class="st">'Where is Groningen located?'</span></span>
<span id="cb11-8"><a href="#cb11-8" tabindex="-1"></a> }, </span>
<span id="cb11-9"><a href="#cb11-9" tabindex="-1"></a> {</span>
<span id="cb11-10"><a href="#cb11-10" tabindex="-1"></a>    <span class="st">'role'</span>: <span class="st">'assistant'</span>, </span>
<span id="cb11-11"><a href="#cb11-11" tabindex="-1"></a>    <span class="st">'content'</span>: <span class="st">'Groningen is located in the northern part of the Netherlands.'</span></span>
<span id="cb11-12"><a href="#cb11-12" tabindex="-1"></a>}]</span></code></pre>
</div>
<div id="callout3" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div class="callout-inner">
<div class="callout-content">
<p>You may have noticed there are three main <code>roles</code> that
most LLMs recognize in an interaction:</p>
<ul>
<li>
<strong>System:</strong> This role is normally only used once at the
beginning of an interaction, it contains the general prompting
strategies (instructions) that your model should follow. Think of it as
the user manual (where you explain what the task is) that the LLM will
read before performing the actual task. This is where <em>prompt
engineering</em> happens.</li>
<li>
<strong>User:</strong> also referred sometimes as
<strong>Human</strong>, includes all messages <em>human generated</em>
in the dialogue are given the user role. In a multi-turn dialogue, each
utterance you submit will be given the user role, to distinguish them
from the automatically generated sequences. These are quesitons
submitted ot the model, or <em>input instances</em> for which you expect
an output.</li>
<li>
<strong>Assistant:</strong> All messages <em>automatically
generated</em> based on the model text generation capabilities, these
are the <em>model outputs</em>.</li>
</ul>
<p>Different models might recognize also other roles, but these are the
core ones. You can always pass directly a text prompt, even if you do
only one-turn interactions with the LLMs (which many times is the
recommended option to avoid biasing model repsonses based on previous
ones), but it a good practice to follow these roles since the start, to
make your code more scalable. These roles are internally translated into
special tokens and processed together as a single sequence. See a <a href="https://tiktokenizer.vercel.app/?model=gpt-4o" class="external-link">tiktokenizer
example for GPT-4o</a></p>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="hyperparameters-for-text-generation">Hyperparameters for Text Generation<a class="anchor" aria-label="anchor" href="#hyperparameters-for-text-generation"></a>
</h3>
<p>Besides the existence of roles, the <code>pipeline()</code> method
has several hyperparameters (function arguments) for that help us
control better how the <code>text-generation</code> task will be done.
These are some of the most common:</p>
<p><strong><em>max_new_tokens</em>:</strong> sets maximum number of
tokens (roughly words/word pieces) that the model will generate in
total. It’s a hard limit - generation stops when this limit is reached,
even mid-sentence. Useful for controlling cost / time. The more tokens
you need to generate for an answer the more time it takes. LLMs called
through paid APIs often charge per a set number of tokens (e.g. $0.008
per 1000 tokens).</p>
<p><strong><em>temperature</em>:</strong> positive float value that
controls the randomness/creativity of the model’s token selection during
generation. The model predicts probabilities for each possible next
token, temperature modifies these probabilities before making the final
choice. 0.0: Completely deterministic - always picks the most likely
token; 1.0+: More random, and “creative”, but potentially less
coherent.</p>
<p><strong><em>do_sample</em>:</strong> when do_sample=True, the model
generates text by sampling from the probability distribution of possible
next tokens. If do_sample=False, the model uses <a href="https://huggingface.co/docs/transformers/generation_strategies" class="external-link">greedy
decoding</a> (always picking the most likely next token), which makes
the output more deterministic but often repetitive.</p>
<p><strong><em>top_k</em>:</strong> This is a sampling strategy called
<a href="https://arxiv.org/pdf/1805.04833" class="external-link">Top-K sampling</a>. Instead
of considering all possible next tokens, the model looks at the k most
likely tokens (based on their probabilities) and samples only from that
reduced set. If top_k=50, the model restricts its choices to the top 50
most probable words at each step.</p>
<div id="discussion1" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<span class="callout-header">Discussion</span>
<div class="callout-inner">
<h3 class="callout-title">Challenge</h3>
<div class="callout-content">
<p>Take 10 minutes to play with the hyperparamenters and observe how
they affect outputs, eventhough you are using exactly the same model
every time. Try different prompts, messages and values and see if the
ouptuts match your intuitions. You can manipulate the values explicitly
in the pipeline such as:</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a></span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a>messages <span class="op">=</span> [</span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a>  {<span class="st">"role"</span>: <span class="st">"system"</span>, <span class="st">"content"</span>: <span class="st">"You are a helpful assistant. Only tell me 'yes' or 'no' and a one-sentence explanation for your answer."</span> },</span>
<span id="cb12-4"><a href="#cb12-4" tabindex="-1"></a>  {<span class="st">"role"</span>: <span class="st">"user"</span>, <span class="st">"content"</span>: <span class="st">"Is NLP the best research field?"</span> },</span>
<span id="cb12-5"><a href="#cb12-5" tabindex="-1"></a>]</span>
<span id="cb12-6"><a href="#cb12-6" tabindex="-1"></a></span>
<span id="cb12-7"><a href="#cb12-7" tabindex="-1"></a>response <span class="op">=</span> llm(messages, max_new_tokens<span class="op">=</span><span class="dv">100</span>, do_sample<span class="op">=</span><span class="va">True</span>, top_k<span class="op">=</span><span class="dv">5</span>, temperature<span class="op">=</span><span class="fl">0.7</span>)[<span class="dv">0</span>][<span class="st">"generated_text"</span>]</span></code></pre>
</div>
<p>Some aspects to discuss:</p>
<ol style="list-style-type: decimal">
<li>
<strong>What did you notice about the models’ responses?</strong>
<ul>
<li>Were they always accurate? Always coherent?</li>
<li>How did different prompts affect the quality?</li>
</ul>
</li>
<li>
<strong>Temperature Effects:</strong>
<ul>
<li>What happened when temperature was low (e.g. 0.0 or 0.1) vs. high
(e.g. 1.2)?</li>
<li>Under which circumstances would you want more random / creative
responses vs. consistent responses?</li>
</ul>
</li>
<li>
<strong>Max Length Effects:</strong>
<ul>
<li>Did you notice a difference in speed of responses when adjusting the
max_length parameter?</li>
</ul>
</li>
</ol>
</div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="prompting-to-solve-nlp-tasks">Prompting to Solve NLP Tasks<a class="anchor" aria-label="anchor" href="#prompting-to-solve-nlp-tasks"></a>
</h2>
<hr class="half-width">
<p>Now we will link some of the funamentals we learned earlier with the
usage of LLMs. As mentioned before, we can consither the LLM as a
component that given an input provides an output, thus we can evaluate
it as any other Machine Learning classifier. We will re-visit the task
of Sentiment Analysis to see how we can shape the task to make the
generative model provide the outputs we need.</p>
<p>But before doing that, we will take the opportunity to introduce <a href="https://github.com/ollama/ollama" class="external-link">Ollama</a>, an open-source tool
that allows you to run LLMs locally on your own computer, instead of
having to call remote services such as OpenAI, Gemini, etc. Ollama lets
you download and execute some of the most popular (semi-)open source
models, including embedding models (Encoder-based) and generative models
(Chat-Assistants), “thinking” models, etc… Take a look at the <a href="https://ollama.com/search" class="external-link">whole list</a>. Consider that the size
of the models you can download and effectively use will depend on how
powerful your laptop is. You can use Ollama as a standalone applicaiton
in Windows/Mac/Linux, but you can also call its local API using python
so you can integrate the LLMs into your scripts.</p>
<p>We will use Ollama thorugh the <a href="https://docs.langchain.com/" class="external-link">langchain</a> interface to give you
the opportunity to also get to know this popular python library. Using
Langchain allows you to write a single piece of code and be flexible to
use many closed and open source models and even get production-level
code with minimal modifications, so it is nice to try it. This is a very
complex library and we will only cover the basics to give you a
headstart. We are going to test the <a href="https://ollama.com/library/llama3.2:1b" class="external-link">llama3.2:1b</a> model
which you can download in the Ollama interface, or also from your
terminal by running:</p>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">BASH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode bash" tabindex="0"><code class="sourceCode bash"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a><span class="ex">ollama</span> pull llama3.2:1b</span></code></pre>
</div>
<p>Let’s see how does the <code>langchain</code> code looks to get
exactly the same kind of interaction as with the HuggingFace code.</p>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a><span class="im">from</span> langchain_ollama <span class="im">import</span> ChatOllama</span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" tabindex="-1"></a>llm <span class="op">=</span> ChatOllama(</span>
<span id="cb14-4"><a href="#cb14-4" tabindex="-1"></a>    model<span class="op">=</span><span class="st">"llama3.2:1b"</span>,</span>
<span id="cb14-5"><a href="#cb14-5" tabindex="-1"></a>    temperature<span class="op">=</span><span class="fl">0.7</span>,</span>
<span id="cb14-6"><a href="#cb14-6" tabindex="-1"></a>    num_predict<span class="op">=</span><span class="dv">100</span>, <span class="co"># Same as max_new_tokens</span></span>
<span id="cb14-7"><a href="#cb14-7" tabindex="-1"></a>    top_k<span class="op">=</span><span class="dv">5</span>, <span class="co"># Only consider the first 5 most likely tokens</span></span>
<span id="cb14-8"><a href="#cb14-8" tabindex="-1"></a>    top_p<span class="op">=</span><span class="fl">0.9</span> <span class="co"># Just consider next tokens form the top_p most likely options. Similar to do_sample</span></span>
<span id="cb14-9"><a href="#cb14-9" tabindex="-1"></a>)</span>
<span id="cb14-10"><a href="#cb14-10" tabindex="-1"></a></span>
<span id="cb14-11"><a href="#cb14-11" tabindex="-1"></a><span class="co"># Define messages in LangChain format</span></span>
<span id="cb14-12"><a href="#cb14-12" tabindex="-1"></a>messages <span class="op">=</span> [</span>
<span id="cb14-13"><a href="#cb14-13" tabindex="-1"></a>    (<span class="st">"system"</span>, <span class="st">"You are a helpful assistant. Give short straight answers"</span>),</span>
<span id="cb14-14"><a href="#cb14-14" tabindex="-1"></a>    (<span class="st">"human"</span>, <span class="st">"Where is Groningen located?"</span>),</span>
<span id="cb14-15"><a href="#cb14-15" tabindex="-1"></a>]</span>
<span id="cb14-16"><a href="#cb14-16" tabindex="-1"></a></span>
<span id="cb14-17"><a href="#cb14-17" tabindex="-1"></a><span class="co"># Get response</span></span>
<span id="cb14-18"><a href="#cb14-18" tabindex="-1"></a>response <span class="op">=</span> llm.invoke(messages)</span>
<span id="cb14-19"><a href="#cb14-19" tabindex="-1"></a></span>
<span id="cb14-20"><a href="#cb14-20" tabindex="-1"></a><span class="co"># Show the Response object (includes useful metadata stats)</span></span>
<span id="cb14-21"><a href="#cb14-21" tabindex="-1"></a><span class="bu">print</span>(response)</span>
<span id="cb14-22"><a href="#cb14-22" tabindex="-1"></a></span>
<span id="cb14-23"><a href="#cb14-23" tabindex="-1"></a><span class="co"># Show content only</span></span>
<span id="cb14-24"><a href="#cb14-24" tabindex="-1"></a><span class="bu">print</span>(response.content)</span></code></pre>
</div>
<div class="section level3">
<h3 id="example-sentiment-analysis">Example: Sentiment Analysis<a class="anchor" aria-label="anchor" href="#example-sentiment-analysis"></a>
</h3>
<p>Let us now try the sentiment analysis task to see how well different
models (with different number of parameters perform). Consider the
following set of lines from product reviews:</p>
<p><strong>Product reviews:</strong></p>
<ol style="list-style-type: decimal">
<li>I love this movie! It was absolutely fantastic and made my day.
<strong>positive</strong>
</li>
<li>This product is terrible. I hate everything about it.
<strong>negative</strong>
</li>
<li>Nothing says quality like a phone that dies after 20 minutes.
<strong>negative</strong>
</li>
<li>The movie was dark and depressing — exactly what I was hoping for.
<strong>positive</strong>
</li>
<li>The food was delicious, but the service was painfully slow.
<strong>mixed</strong>
</li>
</ol>
<p>Set the prompt for this as (substitute the above sentences for
<a href="https://r-text.org/" class="external-link">text</a> each time):</p>
<p><code>Classify the sentiment of the following text as either POSITIVE or NEGATIVE. Text: "{text}"</code></p>
<p>Examine the results afterwards to see which models correctly
classified them and which didn’t.</p>
<div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a>sentiment_llm <span class="op">=</span> ChatOllama(</span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a>    model<span class="op">=</span><span class="st">"llama3.2:1b"</span>,</span>
<span id="cb15-3"><a href="#cb15-3" tabindex="-1"></a>    temperature<span class="op">=</span><span class="dv">0</span>, <span class="co"># Want to be as deterministic as possible</span></span>
<span id="cb15-4"><a href="#cb15-4" tabindex="-1"></a>    num_predict<span class="op">=</span><span class="dv">10</span>, <span class="co"># Keep the answer very short</span></span>
<span id="cb15-5"><a href="#cb15-5" tabindex="-1"></a>    top_k<span class="op">=</span><span class="dv">1</span>, <span class="co"># Only consider the next most likely token (Greedy)</span></span>
<span id="cb15-6"><a href="#cb15-6" tabindex="-1"></a>)</span>
<span id="cb15-7"><a href="#cb15-7" tabindex="-1"></a></span>
<span id="cb15-8"><a href="#cb15-8" tabindex="-1"></a>sentiment_texts <span class="op">=</span> [</span>
<span id="cb15-9"><a href="#cb15-9" tabindex="-1"></a>    <span class="st">"I love this movie! It was absolutely fantastic and made my day."</span>,</span>
<span id="cb15-10"><a href="#cb15-10" tabindex="-1"></a>    <span class="st">"This product is terrible. I hate everything about it."</span>,</span>
<span id="cb15-11"><a href="#cb15-11" tabindex="-1"></a>    <span class="st">"Nothing says quality like a phone that dies after 20 minutes."</span>,</span>
<span id="cb15-12"><a href="#cb15-12" tabindex="-1"></a>    <span class="st">"The movie was dark and depressing — exactly what I was hoping for."</span>,</span>
<span id="cb15-13"><a href="#cb15-13" tabindex="-1"></a>    <span class="st">"The food was delicious, but the service was painfully slow."</span></span>
<span id="cb15-14"><a href="#cb15-14" tabindex="-1"></a>]</span>
<span id="cb15-15"><a href="#cb15-15" tabindex="-1"></a></span>
<span id="cb15-16"><a href="#cb15-16" tabindex="-1"></a>general_prompt <span class="op">=</span> <span class="st">"Classify the sentiment of the following text as either POSITIVE or NEGATIVE. Just return the label, nothing else!"</span></span>
<span id="cb15-17"><a href="#cb15-17" tabindex="-1"></a><span class="cf">for</span> text <span class="kw">in</span> sentiment_texts:</span>
<span id="cb15-18"><a href="#cb15-18" tabindex="-1"></a>    messages <span class="op">=</span> [(<span class="st">"system"</span>, general_prompt), (<span class="st">"human"</span>, text)]</span>
<span id="cb15-19"><a href="#cb15-19" tabindex="-1"></a>    response <span class="op">=</span> sentiment_llm.invoke(messages)</span>
<span id="cb15-20"><a href="#cb15-20" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Example: </span><span class="sc">{</span>text<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb15-21"><a href="#cb15-21" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Response: </span><span class="sc">{</span>response<span class="sc">.</span>content<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb15-22"><a href="#cb15-22" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"------"</span>)</span></code></pre>
</div>
<div id="challenge1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div class="callout-inner">
<h3 class="callout-title">Challenge</h3>
<div class="callout-content">
<p>Apply the classifier to the <code>data/sentiment_film_data.tsv</code>
dataset. Evaluate it using the classification report from scikit-learn.
you can reuse the code from lesson 3. 1. How does the performance
compare to the BERT-based classifier? 2. Can you tweak the prompt to
improve the results?</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<p>Here the code to do the same excercise as lesson 3 but with the
generative approach:</p>
<div class="codewrapper sourceCode" id="cb16">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> classification_report</span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a></span>
<span id="cb16-3"><a href="#cb16-3" tabindex="-1"></a><span class="kw">def</span> load_data(filename):</span>
<span id="cb16-4"><a href="#cb16-4" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(filename, <span class="st">'r'</span>) <span class="im">as</span> f:</span>
<span id="cb16-5"><a href="#cb16-5" tabindex="-1"></a>        lines <span class="op">=</span> f.readlines()[<span class="dv">1</span>:] <span class="co"># skip header</span></span>
<span id="cb16-6"><a href="#cb16-6" tabindex="-1"></a>    sentences, labels <span class="op">=</span> <span class="bu">zip</span>(<span class="op">*</span>(line.strip().split(<span class="st">'</span><span class="ch">\t</span><span class="st">'</span>) <span class="cf">for</span> line <span class="kw">in</span> lines))</span>
<span id="cb16-7"><a href="#cb16-7" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">list</span>(sentences), <span class="bu">list</span>(labels)</span>
<span id="cb16-8"><a href="#cb16-8" tabindex="-1"></a></span>
<span id="cb16-9"><a href="#cb16-9" tabindex="-1"></a>sentiment_llm <span class="op">=</span> ChatOllama(</span>
<span id="cb16-10"><a href="#cb16-10" tabindex="-1"></a>    model<span class="op">=</span><span class="st">"llama3.2:1b"</span>,</span>
<span id="cb16-11"><a href="#cb16-11" tabindex="-1"></a>    temperature<span class="op">=</span><span class="dv">0</span>, <span class="co"># Want to be as deterministic as possible</span></span>
<span id="cb16-12"><a href="#cb16-12" tabindex="-1"></a>    num_predict<span class="op">=</span><span class="dv">10</span>, <span class="co"># Keep the answer very short</span></span>
<span id="cb16-13"><a href="#cb16-13" tabindex="-1"></a>    top_k<span class="op">=</span><span class="dv">1</span>, <span class="co"># Only consider the next most likely token (Greedy)</span></span>
<span id="cb16-14"><a href="#cb16-14" tabindex="-1"></a>)</span>
<span id="cb16-15"><a href="#cb16-15" tabindex="-1"></a></span>
<span id="cb16-16"><a href="#cb16-16" tabindex="-1"></a>sentences, y_true <span class="op">=</span> load_data(<span class="st">'data/sentiment_film_data.tsv'</span>)</span>
<span id="cb16-17"><a href="#cb16-17" tabindex="-1"></a></span>
<span id="cb16-18"><a href="#cb16-18" tabindex="-1"></a>y_pred <span class="op">=</span> []</span>
<span id="cb16-19"><a href="#cb16-19" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"Classify the sentiment of the following text as POSITIVE, NEGATIVE or NEUTRAL. Just return the label, nothing else!"</span></span>
<span id="cb16-20"><a href="#cb16-20" tabindex="-1"></a><span class="cf">for</span> sent <span class="kw">in</span> sentences:</span>
<span id="cb16-21"><a href="#cb16-21" tabindex="-1"></a>    messages <span class="op">=</span> [(<span class="st">"system"</span>, prompt), (<span class="st">"human"</span>, sent)]</span>
<span id="cb16-22"><a href="#cb16-22" tabindex="-1"></a>    response <span class="op">=</span> sentiment_llm.invoke(messages)</span>
<span id="cb16-23"><a href="#cb16-23" tabindex="-1"></a>    <span class="bu">print</span>(sent, response.content)</span>
<span id="cb16-24"><a href="#cb16-24" tabindex="-1"></a>    y_pred.append(response.content.lower())</span>
<span id="cb16-25"><a href="#cb16-25" tabindex="-1"></a></span>
<span id="cb16-26"><a href="#cb16-26" tabindex="-1"></a><span class="bu">print</span>(classification_report(y_true, y_pred))</span></code></pre>
</div>
<p>Report:</p>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>              precision    recall  f1-score   support

    negative       0.50      1.00      0.67        23
     neutral       0.43      0.08      0.14        37
    positive       0.70      0.82      0.76        40

    accuracy                           0.59       100
   macro avg       0.54      0.64      0.52       100
weighted avg       0.55      0.59      0.51       100</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="key-takeaways">Key Takeaways<a class="anchor" aria-label="anchor" href="#key-takeaways"></a>
</h3>
<ul>
<li>
<strong>LLMs are generative models</strong> - they predict the next
most likely tokens</li>
<li>
<strong>Prompts matter</strong> - the way you ask affects what you
get</li>
<li>
<strong>Hyperparameters control behavior</strong> - temperature,
max_length, etc. tune the output</li>
<li>
<strong>Models have limitations</strong> - they can be wrong,
inconsistent, or biased</li>
<li>
<strong>Size vs Speed trade-off</strong> - smaller models are faster
but less capable</li>
</ul>
<hr>
</div>
</section><section><h2 class="section-heading" id="drawbacks-and-biases-with-llms">Drawbacks and Biases with LLMs<a class="anchor" aria-label="anchor" href="#drawbacks-and-biases-with-llms"></a>
</h2>
<hr class="half-width">
<p>This section will examine some important behavior or characteristics
of LLMs that should be taken into account when using them. Most of these
behaviors are a consequence of:</p>
<ul>
<li>The Transformer network architecture itself and the assumptions that
are made when processing text, as early as the tokenization step.</li>
<li>The huge amount of data that was fed to these models, since quantity
was preferred over quality, they have inherited a lot of undesired
information including misinformation, hate-speech, pornography,
artificially generated (noisy) data etc…</li>
<li>The post-training step, which was meant to impose guardrails and
avoid undesired behaviour, also imposes intentional or indirect bias in
the manner in which data was labeled, pushing cultural pre-conceptions
and human biases and labeling errors to be the “preferred answers”.</li>
</ul>
<p>It is also important to keep in mind that behavior can vary across
different LLMs, and since they are constantly being updated and
improved, some of the limitations may be addressed and some of the
behaviors may be replaced or modified in the near future. This again
highlights the importance to always run your own evaluation for your
task, regardless of model’s performance claims.</p>
<p>Let’s take a look at some unexpected behaviors. We load the ollama
model to run the experiments:</p>
<div class="codewrapper sourceCode" id="cb18">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" tabindex="-1"></a>chatbot <span class="op">=</span> ChatOllama(</span>
<span id="cb18-2"><a href="#cb18-2" tabindex="-1"></a>    model<span class="op">=</span><span class="st">"llama3.2:1b"</span>,</span>
<span id="cb18-3"><a href="#cb18-3" tabindex="-1"></a>    temperature<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb18-4"><a href="#cb18-4" tabindex="-1"></a>    num_predict<span class="op">=</span><span class="dv">512</span></span>
<span id="cb18-5"><a href="#cb18-5" tabindex="-1"></a>)</span></code></pre>
</div>
<div class="section level3">
<h3 id="hallucinations">Hallucinations<a class="anchor" aria-label="anchor" href="#hallucinations"></a>
</h3>
<p>Hallucination in LLMs refers to the generation of content that is
factually incorrect, nonsensical, or not grounded in any specific
factual source. This occurs when the model produces confident-sounding
responses that contain false information, fabricated facts, or logical
inconsistencies, despite appearing coherent and plausible. <a href="https://arxiv.org/abs/2406.04175" class="external-link"><em>Confabulation</em></a> is
another term for hallucination in LLMs.</p>
<div class="codewrapper sourceCode" id="cb19">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" tabindex="-1"></a>chatbot <span class="op">=</span> ChatOllama(</span>
<span id="cb19-2"><a href="#cb19-2" tabindex="-1"></a>    model<span class="op">=</span><span class="st">"llama3.2:1b"</span>,</span>
<span id="cb19-3"><a href="#cb19-3" tabindex="-1"></a>    temperature<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb19-4"><a href="#cb19-4" tabindex="-1"></a>    num_predict<span class="op">=</span><span class="dv">500</span>,</span>
<span id="cb19-5"><a href="#cb19-5" tabindex="-1"></a>    top_p<span class="op">=</span><span class="fl">0.9</span></span>
<span id="cb19-6"><a href="#cb19-6" tabindex="-1"></a>)</span>
<span id="cb19-7"><a href="#cb19-7" tabindex="-1"></a></span>
<span id="cb19-8"><a href="#cb19-8" tabindex="-1"></a>halluc_prompt <span class="op">=</span> <span class="st">"Who is Railen Ackerby?"</span></span>
<span id="cb19-9"><a href="#cb19-9" tabindex="-1"></a>response <span class="op">=</span> chatbot.invoke(halluc_prompt)</span>
<span id="cb19-10"><a href="#cb19-10" tabindex="-1"></a><span class="bu">print</span>(response.content)</span></code></pre>
</div>
<p>Railen Ackerby is a made up name. Smaller models would even make up a
complete biography for this fake person we just made up. Given it was
such a known drawback, newer models have implemented guardrails to avoid
producing such outputs for very unlikely names in the training data. The
<em>interesting hallucination</em> actually happens even on top of such
guardrail, because at least llama3.2:1b will first tell you such name
does not exist and then still make up some alternative story about it,
which is even harder to verify if it is true or not, and that is
precisely the biggest risk with these kinds of outputs. Note that this
happens even when the temperature was set to zero and the top_p set to
0.9, which supposedly make more truthful answers.</p>
</div>
<div class="section level3">
<h3 id="non-determinism">Non-determinism<a class="anchor" aria-label="anchor" href="#non-determinism"></a>
</h3>
<p>By now we know that setting the temperature to 0.0 will get us the
same answer if we run the experiment <em>under the same
circumstances</em>. But even changing the machine where the code is run
could already give a different answer for the same prompt. Moreover,
setting the temperature to 0.0 restrains the model for giving more
“interesting” correlations which are useful for most tasks.</p>
</div>
<div class="section level3">
<h3 id="biases-and-fairness">Biases and fairness<a class="anchor" aria-label="anchor" href="#biases-and-fairness"></a>
</h3>
<p>LLMs can be biased towards certain stereotypes represented in the
pretraining data. Let’s look again at an example of gender bias. E.g.,
stereotype of nurses usually being female.</p>
<div class="codewrapper sourceCode" id="cb20">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" tabindex="-1"></a>bias_prompt <span class="op">=</span> <span class="st">"Write a two paragraph story where a nurse, a pilot, and a CEO are having lunch together."</span></span>
<span id="cb20-2"><a href="#cb20-2" tabindex="-1"></a>response <span class="op">=</span> chatbot.invoke(bias_prompt)</span>
<span id="cb20-3"><a href="#cb20-3" tabindex="-1"></a><span class="bu">print</span>(response.content)</span></code></pre>
</div>
<p>If this example works, a short story should be generated by the LLM
about a nurse and most likely it will be a female nuerse, either marked
by usage of words such as “she” and “her” to reveal gender or even
giving her a name. We never gave any gender explicitly to the model. In
our example we got the following story:</p>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>As the sun shone down on the bustling airport, Nurse Emma sat across from Pilot Jack at a small café, sipping on a cup of coffee. They had been friends for years, bonding over their shared love of adventure and good food. Jack, ever the entrepreneur, had recently taken to flying his own plane as a way to get some much-needed perspective on the business world. Emma was happy to provide medical care to his passengers, and in return, he offered her a unique glimpse into the high-stakes world of aviation....</code></pre>
</div>
<p>Another common case is asking for apparently equivalent facts about
different entities. The model will answer with the same confidence but
with different degrees of accuracy. For example:</p>
<div class="codewrapper sourceCode" id="cb22">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" tabindex="-1"></a><span class="co"># Hint: John Adams (1797 - 1801)</span></span>
<span id="cb22-2"><a href="#cb22-2" tabindex="-1"></a>biased_prompt <span class="op">=</span> <span class="st">"Who was the second president of the United States?"</span></span>
<span id="cb22-3"><a href="#cb22-3" tabindex="-1"></a></span>
<span id="cb22-4"><a href="#cb22-4" tabindex="-1"></a>response <span class="op">=</span> chatbot.invoke(outdated_prompt)</span>
<span id="cb22-5"><a href="#cb22-5" tabindex="-1"></a><span class="bu">print</span>(response.content)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The second president of the United States was John Adams. He served from 1797 to 1801, succeeding George Washington.</code></pre>
</div>
<p>we get a factually correct and confident answer. Let’s try exactly
the same question for a different country:</p>
<div class="codewrapper sourceCode" id="cb24">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" tabindex="-1"></a><span class="co"># Hint: Vicente Guerrero (April 1829 - December 1829)</span></span>
<span id="cb24-2"><a href="#cb24-2" tabindex="-1"></a>biased_prompt <span class="op">=</span> <span class="st">"Who was the second president of Mexico?"</span></span>
<span id="cb24-3"><a href="#cb24-3" tabindex="-1"></a></span>
<span id="cb24-4"><a href="#cb24-4" tabindex="-1"></a>response <span class="op">=</span> chatbot.invoke(outdated_prompt)</span>
<span id="cb24-5"><a href="#cb24-5" tabindex="-1"></a><span class="bu">print</span>(response.content)</span></code></pre>
</div>
<p>we got his answer:</p>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The second president of Mexico was Antonio López de Santa Anna. He served from 1833 to 1835 and again from 1838 to 1841.</code></pre>
</div>
<p>Initially we see the same confident syntax to answer the question,
but it is factually incorrect. The results of course will vary per model
and configuration of prompts. But beware that prompts that involve the
anglosphere in particular will be more accurately responded, whereas the
more you deviate from the “western standards” the less accuracy you will
get. In the case of Mexico it is particularly dissapointing as even the
English Wikipedia page has that information in a prominent place.</p>
</div>
<div class="section level3">
<h3 id="outdated-knowledge">Outdated Knowledge<a class="anchor" aria-label="anchor" href="#outdated-knowledge"></a>
</h3>
<p>Another common drawback is that models have a cut-off training date.
Again, guardrails are supposed to be in place and they do seem to work
for obvious cases:</p>
<div class="codewrapper sourceCode" id="cb26">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" tabindex="-1"></a>outdated_prompt <span class="op">=</span> <span class="st">"Who is the president of the United States?"</span></span>
<span id="cb26-2"><a href="#cb26-2" tabindex="-1"></a>response <span class="op">=</span> chatbot.invoke(outdated_prompt)</span>
<span id="cb26-3"><a href="#cb26-3" tabindex="-1"></a><span class="bu">print</span>(response.content)</span></code></pre>
</div>
<p>with the following output:</p>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>As of my last update in 2023, the President of the United States is Joe Biden. He has been serving as the 46th President of the United States since January 20, 2021.</code></pre>
</div>
<p>The model warns us that its latest data is from 2023. However, for
less obvious cases the model still answers confidently without any
warning. And once more, the problem is not with the obviously wrong
answers, the problem is when we are not aware if the temporality of the
answer is valid or not. For example:</p>
<div class="codewrapper sourceCode" id="cb28">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" tabindex="-1"></a>outdated_prompt <span class="op">=</span> <span class="st">"When was the last time Argentina won the World Cup?"</span></span>
<span id="cb28-2"><a href="#cb28-2" tabindex="-1"></a>response <span class="op">=</span> chatbot.invoke(outdated_prompt)</span>
<span id="cb28-3"><a href="#cb28-3" tabindex="-1"></a><span class="bu">print</span>(response.content)</span></code></pre>
</div>
<p>We got this output, which is incorrect since Argentina won again in
2024. This time the model doesn’t warn us:</p>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Argentina has not won the FIFA World Cup since 1986, when they defeated West Germany 3-2 in a penalty shootout after the game ended 0-0.</code></pre>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<span class="callout-header">Key Points</span>
<div class="callout-inner">
<div class="callout-content">
<ul>
<li>We learned how are so called LLMs different from the first
generation of Transformers</li>
<li>There are different kinds of LLMs and understanding their
differences and limitations are a key aspect for choosing the best model
for your case</li>
<li>We learned how to use HuggingFace pipeline with SmolLMv2, an open
source model.</li>
<li>We learned how to use Ollama to run conversational models in our
laptop</li>
<li>Classification tasks can be done using generative models if we
define the prompt in a careful way</li>
<li>Hidden biases will always be present when using LLMs, we should be
aware of those before we draw conclusions from the outputs.</li>
</ul>
</div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="advanced-material">Advanced Material<a class="anchor" aria-label="anchor" href="#advanced-material"></a>
</h2>
<hr class="half-width">
<div class="section level3">
<h3 id="llm-selection-criteria">1. LLM selection criteria<a class="anchor" aria-label="anchor" href="#llm-selection-criteria"></a>
</h3>
<p>Choosing the right LLM for your specific use case requires
consideration of multiple factors. This section will guide you through
some decision points that will help you select an appropriate model for
your needs.</p>
<div class="section level4">
<h4 id="openness-and-licensing-considerations-1">1.1 Openness and Licensing Considerations<a class="anchor" aria-label="anchor" href="#openness-and-licensing-considerations-1"></a>
</h4>
<p>The spectrum of model availability ranges from fully open to
completely proprietary:</p>
<p><strong>Open-weights</strong> release the trained model parameters
while keeping training code or data proprietary. This allows you to run
and fine-tune the model locally but if you don’t have the code used to
train the model or information about the architecture used, it limits
your ability to fully understand or replicate the training process.</p>
<p><strong>Open training data</strong> they release the text data used
for pretraining.</p>
<p><strong>Open architecture</strong> they publish a paper about the
neural network architecture and specific configuration they used for
training. Or they release the actual source code they used for
pretraining.</p>
<p>Ideally, if you want to use a model for empirical academic research
you might decide for models that are completely open in all three of the
above facets. Although, open training data is quite rare for available
state-of-the-art models.</p>
<p><strong>Commercial/proprietary models</strong> like GPT-4, Claude, or
Gemini are accessed only through APIs. While often offering superior
performance, they provide no access to internal architecture and may
have usage restrictions or costs that scale with volume.</p>
<p>Consider your requirements for: - Code modification and customization
- Data privacy and control - Commercial usage rights - Research
reproducibility - Long-term availability guarantees</p>
</div>
<div class="section level4">
<h4 id="hardware-and-compute-requirements">1.2 Hardware and Compute Requirements<a class="anchor" aria-label="anchor" href="#hardware-and-compute-requirements"></a>
</h4>
<p>Your available computational resources significantly constrain your
model options:</p>
<p><strong>Modern GPU access</strong> (RTX 4090, A100, H100, etc.)
enables you to run larger models locally. Consider: - VRAM requirements:
7B parameter models typically need 14+ GB, 13B models need 26+ GB, 70B
models require 140+ GB or multi-GPU setups - Inference speed
requirements for your application - Whether you need real-time responses
or can accept slower processing</p>
<p><strong>CPU-only environments</strong> limit you to smaller models
(such as SmolLM2 and SmolLM3) or <a href="https://ojs.aaai.org/index.php/AAAI/article/view/29908" class="external-link">quantized</a>
versions.</p>
<p><strong>Cloud/API access</strong> removes hardware constraints but
introduces ongoing costs and potential latency issues.</p>
</div>
<div class="section level4">
<h4 id="performance-evaluation">1.3 Performance Evaluation<a class="anchor" aria-label="anchor" href="#performance-evaluation"></a>
</h4>
<p>Different models excel at different tasks. Some evaluation criteria
include:</p>
<p><strong>General capability benchmarks</strong> like those found on
the <a href="https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard" class="external-link">Open
LLM Leaderboard</a> provide standardized comparisons across models for
reasoning, knowledge, and language understanding tasks.</p>
<p><strong>Multilingual performance</strong> varies significantly
between models. The <a href="https://mmluprox.github.io/" class="external-link">MMLU-Pro
benchmark</a> offers insights into cross-lingual capabilities if you
need support for non-English languages.</p>
<p><strong>Task-specific performance</strong> should be evaluated based
on your particular needs: - Code generation - Mathematical reasoning -
Reading comprehension and summarization - Creative writing and dialogue
quality - Scientific and technical domain knowledge</p>
<p>Always validate benchmark performance with your own test cases, as
real-world performance may differ from standardized evaluations.</p>
</div>
<div class="section level4">
<h4 id="purpose-or-use-case">1.4 Purpose or Use Case<a class="anchor" aria-label="anchor" href="#purpose-or-use-case"></a>
</h4>
<p><strong>Scientific and research applications</strong> often
prioritize reproducibility, transparency, and the ability to modify
model behavior. Open-source models with detailed documentation are
typically preferred (e.g. SmolLM, LLama, Olmo)</p>
<p><strong>Applications (mobile or web apps)</strong> may require: -
Reliable API uptime and support - Clear licensing for commercial use -
Scalability to handle many concurrent users - Content filtering and
safety features</p>
<p><strong>Personal or educational use</strong> might emphasize: -
Cost-effectiveness - Ease of setup and use</p>
</div>
<div class="section level4">
<h4 id="integration-and-deployment-considerations">1.5 Integration and Deployment Considerations<a class="anchor" aria-label="anchor" href="#integration-and-deployment-considerations"></a>
</h4>
<p><strong>Software integration</strong> requirements affect model
choice: - API-based models offer simpler integration but require
internet connectivity - Local models provide more control but require
more complex deployment - Consider latency requirements, offline
capabilities, and data privacy needs</p>
<p><strong>Hosting and serving capabilities</strong> determine whether
you can run models locally: - Do you have the infrastructure to serve
models at scale? - Are you self-hosting the model?</p>
</div>
<div class="section level4">
<h4 id="domain-specific-models">1.6 Domain-Specific Models<a class="anchor" aria-label="anchor" href="#domain-specific-models"></a>
</h4>
<p>Many models have been fine-tuned for specific domains or tasks. For
example:</p>
<ul>
<li>Medical and healthcare applications (e.g., <a href="https://huggingface.co/microsoft/biogpt" class="external-link">BioGPT</a>)</li>
<li>Legal document processing (e.g., <a href="https://huggingface.co/Equall/Saul-7B-Instruct-v1" class="external-link">SaulLM</a>)</li>
</ul>
<p>Remember that the LLM landscape evolves rapidly. New models are
released frequently, and performance benchmarks should be regularly
reassessed. Consider building your system with model-agnostic interfaces
to facilitate future transitions between different LLMs as your needs
evolve or better options become available.</p>
<hr>
</div>
</div>
<div class="section level3">
<h3 id="how-are-llms-trained">2. How are LLMs trained?<a class="anchor" aria-label="anchor" href="#how-are-llms-trained"></a>
</h3>
<p>Training LLMs involves a series of steps. There are two main phases:
pretraining and post training. Pretraining generally involves the
following substeps:</p>
<div class="section level4">
<h4 id="obtaining-and-pre-processing-textual-data-for-training">2.1 Obtaining and pre-processing textual data for training<a class="anchor" aria-label="anchor" href="#obtaining-and-pre-processing-textual-data-for-training"></a>
</h4>
<ul>
<li>
<em>Downloading and pre-processing text:</em> State-of-the-art LLMs
include entire snapshots of the internet as the core textual data for
training. This data can be sourced from efforts such as <a href="https://commoncrawl.org/" class="external-link">CommonCrawl</a>. Proprietary LLMs may
augment or supplement this training data with additional licensed or
proprietary textual data (e.g., books) from other sources or companies.
The raw web pages are not usable by themselves, we need to extract the
raw text from those HTML pages. This requires a preprocessing or data
cleaning step.</li>
</ul>
<figure><img src="fig/html_to_text.png" alt="html to text processing" width="800" class="figure mx-auto d-block"></figure><ul>
<li>
<em>Tokenization:</em> As we saw in Lesson 01, the raw text itself
cannot be used in the training step, we need a way to tokenize and
encode the text for processing by the neural network. As an example of
what these encodings look like for OpenAI models like GPT, you can visit
<a href="https://tiktokenizer.f2api.com/" class="external-link">TikTokenizer</a>.</li>
</ul>
<figure><img src="fig/text_to_tokenids.png" alt="tokenization" width="800" class="figure mx-auto d-block"></figure>
</div>
<div class="section level4">
<h4 id="neural-network-training">2.2 Neural network training<a class="anchor" aria-label="anchor" href="#neural-network-training"></a>
</h4>
<p>With LLMs the training goal is to predict the next token in a
one-dimensional sequence of tokens. This is different from BERT where
the goal is to predict masked tokens in the input sequence. BERT is
therefore not natively developed for generating text, whereas LLMs are.
In the internals of the transformer architecture, this is illustrated by
the fact that BERT only makes use of the Encoder component to create its
contextualised word embeddings. It does not use the Decoder component to
generate new tokens for the input sequence.</p>
<figure><img src="fig/llm_training_goal.png" alt="training goal llms" width="800" class="figure mx-auto d-block"></figure><p>After training we obtain a <em>base</em> LLM which is predicts or
generates token sequences that resemble its training data. However, a
post training step is required in order to fine-tune the model to accept
instructions, answer questions in a conversational style and to have
behavior that is more suitable for interaction with humans.</p>
</div>
<div class="section level4">
<h4 id="post-training">2.3 Post training<a class="anchor" aria-label="anchor" href="#post-training"></a>
</h4>
<p>What does post training for LLMs look like? Why is this step
necessary? What would happen if you skip this step and just use the base
model trained in Step 2.2 for inference? The answer is that the base
model is just a token sequence predictor. It just predicts the most
likely next token for an input sequence of tokens. It does not
understand how to deal with conversations or to interpret instructions
(the intentions and tone behind written communication).</p>
<p>Therefore, you may encounter unexpected conversations like this if
interacting with a base LLM:</p>
<p><strong>Not Following Instructions</strong></p>
<pre><code>Human: Summarize this paragraph in one sentence: The Nile is the longest river in Africa and flows through 11 countries before emptying into the Mediterranean Sea.
Raw LLM: The Nile is the longest river in Africa and flows through 11 countries before emptying into the Mediterranean Sea.</code></pre>
<p>In this example interaction, the model was trained to predict text,
not to follow instructions. So it might not give expected or correct
responses although, statistically, these response tokens are indeed the
next most likely tokens.</p>
<p><strong>Regurgitation</strong></p>
<pre><code>Human: Donald John Trump (born June 14, 1946) is an American politician,
Raw LLM: media personality, and businessman who is the 47th president of the United States. A member of the Republican Party, he served as the 45th president from 2017 to 2021...</code></pre>
<figure><img src="fig/trump.png" alt="tokenization" width="800" class="figure mx-auto d-block"></figure><p>In this interaction, the model just “regurgitates” verbatim a
response from Wikipedia (part of it’s training data), instead of
creatively formulating a response in its own words. These are just a few
examples of the kind of undesirable behavior one might encounter when
doing inference with the base model without post training.</p>
<p>Therefore, to avoid issues like this, and to be useful as
conversational assistants, LLMs generally have to be fine-tuned on
additional unseen conversation datasets (containing on the order of
hundreds of thousands of conversations). These datasets are manually
created and validated by humans. For proprietary models, these humans
are hired professionally to contribute to this dataset from platforms
such as Upwork and ScaleAI. There are also open-source efforts to create
such datasets such as <a href="https://huggingface.co/datasets/OpenAssistant/oasst2" class="external-link">OpenAssistant</a>.</p>
<p>Once such a dataset is obtained, the conversations need to be
represented as a one-dimensional token sequence. Conversations therefore
have to be encoded into such representations before being used as input
for training. The encoding method for conversations currently differ
among LLMs. The figure below shows how GPT-4 tokenizes and encodes input
conversations as one dimensional token sequences (<a href="https://tiktokenizer.f2api.com/" class="external-link">TikTokenizer</a> is a page for
visualizing and experimenting with tokenization approaches used by
OpenAI LLMs).</p>
<figure><img src="fig/tokenize_conversations.png" alt="tokenization for conversation data" width="800" class="figure mx-auto d-block"></figure><p>After encoding or tokenization, the conversational data is supplied
as input for fine-tuning the base model (replacing the original training
data). I.e., the base model’s weights or parameters are updated through
this process.</p>
<p>Below are some examples of conversational data that one might use to
fine-tune an LLM. In general, you want multi-turn conversations where
each participant in the conversation gets the opportunity to respond
multiple times. Because this is the likely mode of interaction that will
take place between humans and LLM-based chat assistants:</p>
<pre><code>**Conversation 1 (Travel Planning):**
Human: Can you suggest a weekend getaway near Amsterdam?
LLM: Sure! Giethoorn – Known as the “Venice of the North,” this village has canals, thatched-roof houses, and a peaceful atmosphere. Utrecht, a charming city with medieval streets, beautiful canals, and cozy cafés.
Human: Which one would be better for a quiet, relaxing trip?
LLM: Giethoorn would be the quieter option. Unlike Utrecht, which is a bustling city, Giethoorn is a small village with a calm, serene atmosphere, ideal for a peaceful getaway.

**Conversation 2 (Math Help):**
Human: What's the derivative of 3𝑥^2 + 5𝑥 − 7?
LLM: The derivative is 6𝑥 + 5.
Human: Great, and what about the second derivative?
LLM: The second derivative is 6.

**Conversation 3 (Creative Writing):**
Human: Give me a one-line horror story.
LLM: The reflection in the mirror smiled back before I did.
Human: Can you make it even creepier?
LLM: The reflection winked, though I was alone in the room.</code></pre>
<hr>
</div>
</div>
<div class="section level3">
<h3 id="llm-prompting-strategies">3. LLM Prompting strategies<a class="anchor" aria-label="anchor" href="#llm-prompting-strategies"></a>
</h3>
<p>See <a href="">notebooks/prompting.ipynb</a>…</p>
<hr>
</div>
<div class="section level3">
<h3 id="solving-nlp-tasks-with-llms">4. Solving NLP tasks with LLMs<a class="anchor" aria-label="anchor" href="#solving-nlp-tasks-with-llms"></a>
</h3>
<p>Write and execute the following prompts in Python code within Jupyter
notebook.</p>
<ol style="list-style-type: decimal">
<li>
<p><strong>Question answering:</strong></p>
<ul>
<li>answering general knowledge questions <br>
</li>
</ul>
<pre><code>Human: What is the longest river in the world?
LLM: The Nile River in Africa is traditionally considered the longest river in the world, stretching about 6,650 km (4,130 miles) through 11 countries before emptying into the Mediterranean Sea.</code></pre>
<p><br></p>
<ul>
<li>expert advice in a particular domain or scientific field <br>
</li>
</ul>
<pre><code>Human: What are good strategies for film-making on a budget in Night conditions?
LLM: Night filmmaking can be both creatively rewarding and technically challenging, especially on a budget. Here are some strategies that indie filmmakers often use to maximize image quality and mood without expensive gear:...</code></pre>
<p><br></p>
</li>
<li>
<p><strong>Text generation:</strong></p>
<ul>
<li>Writing essays, business plans and other documents <br>
</li>
</ul>
<pre><code>Human: Write a template cover letter for a mid-level software engineering position at a large AI startup.
LLM: Dear [Hiring Manager’s Name], I am excited to apply for the Software Engineer position at [Company Name]...</code></pre>
<p><br></p>
<ul>
<li>Creative writing (slogans, poems, fiction, jokes) <br>
</li>
</ul>
<pre><code>Human: Write a two sentence comedy sketch.
LLM: [Scene: A man sits in a job interview.] Interviewer: "Your resume says you’re fluent in Python?" Candidate: pulls out a snake from his briefcase "She does most of the talking."</code></pre>
<p><br></p>
</li>
<li>
<p><strong>Text summarisation</strong> <br></p>
<pre><code>Human: Shorten: "Amsterdam, the capital of the Netherlands, is a city celebrated for its rich history, cultural diversity, and iconic canals. Known as the “Venice of the North,” it is home to a vast network of waterways lined with elegant 17th-century houses, giving the city its distinct charm. Amsterdam seamlessly blends old-world character with modern vibrancy, attracting millions of visitors each year."
LLM: Amsterdam, the capital of the Netherlands, is known for its history, canals, and 17th-century houses, combining old charm with modern vibrancy that attracts millions each year.</code></pre>
<p><br></p>
</li>
<li>
<p><strong>Sentiment or text classification</strong> <br></p>
<pre><code>Sentiment analysis
Human: Is this a positive, neutral or negative statement about the movie: "Incredible and not in a good way."
LLM: That’s a negative statement about the movie. The phrase “Incredible” usually suggests something impressive, but the qualifier “not in a good way” makes it clear the speaker means the film was bad or shockingly poor.</code></pre>
<p><br></p>
<pre><code>Text classification
Human: Categorise this statement into either sports, business, or science: "Lionel Messi scored twice to lead Argentina to victory in the World Cup qualifier."
LLM: This statement falls under Sports. It talks about a soccer player and a match result, which clearly relates to athletic competition.</code></pre>
<p><br></p>
</li>
<li><p><strong>Language translation</strong> <br><code>Human: Translate "I have recently naturalised as a Dutch citizen" into Dutch.     LLM: Ik ben onlangs genaturaliseerd als Nederlands staatsburger.</code>
<br></p></li>
<li>
<p><strong>Generating software code</strong> <br></p>
<pre><code>Human: Write a short Python function to demonstrate bubble sort
LLM: ...</code></pre>
<div class="codewrapper sourceCode" id="cb41">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" tabindex="-1"></a><span class="kw">def</span> bubble_sort(arr):</span>
<span id="cb41-2"><a href="#cb41-2" tabindex="-1"></a>    n <span class="op">=</span> <span class="bu">len</span>(arr)</span>
<span id="cb41-3"><a href="#cb41-3" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb41-4"><a href="#cb41-4" tabindex="-1"></a>        <span class="co"># Last i elements are already in place</span></span>
<span id="cb41-5"><a href="#cb41-5" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, n <span class="op">-</span> i <span class="op">-</span> <span class="dv">1</span>):</span>
<span id="cb41-6"><a href="#cb41-6" tabindex="-1"></a>            <span class="cf">if</span> arr[j] <span class="op">&gt;</span> arr[j <span class="op">+</span> <span class="dv">1</span>]:</span>
<span id="cb41-7"><a href="#cb41-7" tabindex="-1"></a>                <span class="co"># Swap if the element is greater than the next</span></span>
<span id="cb41-8"><a href="#cb41-8" tabindex="-1"></a>                arr[j], arr[j <span class="op">+</span> <span class="dv">1</span>] <span class="op">=</span> arr[j <span class="op">+</span> <span class="dv">1</span>], arr[j]</span>
<span id="cb41-9"><a href="#cb41-9" tabindex="-1"></a>    <span class="cf">return</span> arr</span>
<span id="cb41-10"><a href="#cb41-10" tabindex="-1"></a></span>
<span id="cb41-11"><a href="#cb41-11" tabindex="-1"></a><span class="co"># Example usage</span></span>
<span id="cb41-12"><a href="#cb41-12" tabindex="-1"></a>numbers <span class="op">=</span> [<span class="dv">64</span>, <span class="dv">34</span>, <span class="dv">25</span>, <span class="dv">12</span>, <span class="dv">22</span>, <span class="dv">11</span>, <span class="dv">90</span>]</span>
<span id="cb41-13"><a href="#cb41-13" tabindex="-1"></a>sorted_numbers <span class="op">=</span> bubble_sort(numbers)</span>
<span id="cb41-14"><a href="#cb41-14" tabindex="-1"></a><span class="bu">print</span>(sorted_numbers)</span></code></pre>
</div>
</li>
</ol>
<p>See <a href="">notebooks/classifandeval.ipynb</a>…</p>
<hr>
</div>
<div class="section level3">
<h3 id="creating-your-own-chat-assistant">5. Creating your own chat assistant<a class="anchor" aria-label="anchor" href="#creating-your-own-chat-assistant"></a>
</h3>
<p>See <a href="">notebooks/chatbot.ipynb</a>…</p>
</div>
<div class="section level3">
<h3 id="biases-with-llms">6. Biases with LLMs<a class="anchor" aria-label="anchor" href="#biases-with-llms"></a>
</h3>
<p>See <a href="">notebooks/responsible.ipynb</a>…</p>
<hr>
</div>
</section></section>
</div>
    </main>
</div>
<!-- END  : inst/pkgdown/templates/content-extra.html -->

      </div>
<!--/div.row-->
      		<footer class="row footer mx-md-3"><hr>
<div class="col-md-6">
        <p>This lesson is subject to the <a href="CODE_OF_CONDUCT.html">Code of Conduct</a></p>
        <p>

        <a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/edit/main/README.md" class="external-link">Edit on GitHub</a>

	
        | <a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/blob/main/CONTRIBUTING.md" class="external-link">Contributing</a>
        | <a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/" class="external-link">Source</a></p>
				<p><a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/blob/main/CITATION.cff" class="external-link">Cite</a> | <a href="mailto:team@carpentries.org">Contact</a> | <a href="https://carpentries.org/about/" class="external-link">About</a></p>
			</div>
			<div class="col-md-6">

        <p>Materials licensed under <a href="LICENSE.html">CC-BY 4.0</a> by the authors</p>

        <p>Template licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/" class="external-link">CC-BY 4.0</a> by <a href="https://carpentries.org/" class="external-link">The Carpentries</a></p>
        <p>Built with <a href="https://github.com/carpentries/sandpaper/tree/0.17.1" class="external-link">sandpaper (0.17.1)</a>, <a href="https://github.com/carpentries/pegboard/tree/0.7.9" class="external-link">pegboard (0.7.9)</a>, and <a href="https://github.com/carpentries/varnish/tree/1.0.7" class="external-link">varnish (1.0.7)</a></p>
			</div>
		</footer>
</div> <!-- / div.container -->
	<div id="to-top">
		<a href="#top">
      <i class="search-icon" data-feather="arrow-up" role="img" aria-label="Back To Top"></i><br><!-- <span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top --><span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top
		</a>
	</div>
  <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "LearningResource",
  "@id": "https://esciencecenter-digital-skills.github.io/Natural-language-processing/aio.html",
  "inLanguage": "en",
  "dct:conformsTo": "https://bioschemas.org/profiles/LearningResource/1.0-RELEASE",
  "description": "A Carpentries Lesson teaching foundational data and coding skills to researchers worldwide",
  "keywords": "software, data, lesson, The Carpentries, NLP, English, social sciences, pre-alpha",
  "name": "All in One View",
  "creativeWorkStatus": "active",
  "url": "https://esciencecenter-digital-skills.github.io/Natural-language-processing/aio.html",
  "identifier": "https://esciencecenter-digital-skills.github.io/Natural-language-processing/aio.html",
  "dateCreated": "2024-04-24",
  "dateModified": "2025-11-27",
  "datePublished": "2025-11-27"
}

  </script><script>
		feather.replace();
	</script>
</body>
</html><!-- END:   inst/pkgdown/templates/layout.html-->

