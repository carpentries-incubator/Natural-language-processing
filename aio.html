<!DOCTYPE html>
<!-- START: inst/pkgdown/templates/layout.html --><!-- Generated by pkgdown: do not edit by hand --><html lang="en" data-bs-theme="auto">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<title>Fundamentals of Natural Language Processing (NLP) in Python: All in One View</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<script src="assets/themetoggle.js"></script><link rel="stylesheet" type="text/css" href="assets/styles.css">
<script src="assets/scripts.js" type="text/javascript"></script><!-- mathjax --><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      config: ["MMLorHTML.js"],
      jax: ["input/TeX","input/MathML","output/HTML-CSS","output/NativeMML", "output/PreviewHTML"],
      extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
      TeX: {
        extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
      },
      tex2jax: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [ ['$$','$$'], ['\\[', '\\]'] ],
        processEscapes: true
      }
    });
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><!-- Responsive Favicon for The Carpentries --><link rel="apple-touch-icon" sizes="180x180" href="favicons/incubator/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicons/incubator/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicons/incubator/favicon-16x16.png">
<link rel="manifest" href="favicons/incubator/site.webmanifest">
<link rel="mask-icon" href="favicons/incubator/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" media="(prefers-color-scheme: light)" content="white">
<meta name="theme-color" media="(prefers-color-scheme: dark)" content="black">
</head>
<body>
    <header id="top" class="navbar navbar-expand-md top-nav incubator"><svg xmlns="http://www.w3.org/2000/svg" class="d-none"><symbol id="check2" viewbox="0 0 16 16"><path d="M13.854 3.646a.5.5 0 0 1 0 .708l-7 7a.5.5 0 0 1-.708 0l-3.5-3.5a.5.5 0 1 1 .708-.708L6.5 10.293l6.646-6.647a.5.5 0 0 1 .708 0z"></path></symbol><symbol id="circle-half" viewbox="0 0 16 16"><path d="M8 15A7 7 0 1 0 8 1v14zm0 1A8 8 0 1 1 8 0a8 8 0 0 1 0 16z"></path></symbol><symbol id="moon-stars-fill" viewbox="0 0 16 16"><path d="M6 .278a.768.768 0 0 1 .08.858 7.208 7.208 0 0 0-.878 3.46c0 4.021 3.278 7.277 7.318 7.277.527 0 1.04-.055 1.533-.16a.787.787 0 0 1 .81.316.733.733 0 0 1-.031.893A8.349 8.349 0 0 1 8.344 16C3.734 16 0 12.286 0 7.71 0 4.266 2.114 1.312 5.124.06A.752.752 0 0 1 6 .278z"></path><path d="M10.794 3.148a.217.217 0 0 1 .412 0l.387 1.162c.173.518.579.924 1.097 1.097l1.162.387a.217.217 0 0 1 0 .412l-1.162.387a1.734 1.734 0 0 0-1.097 1.097l-.387 1.162a.217.217 0 0 1-.412 0l-.387-1.162A1.734 1.734 0 0 0 9.31 6.593l-1.162-.387a.217.217 0 0 1 0-.412l1.162-.387a1.734 1.734 0 0 0 1.097-1.097l.387-1.162zM13.863.099a.145.145 0 0 1 .274 0l.258.774c.115.346.386.617.732.732l.774.258a.145.145 0 0 1 0 .274l-.774.258a1.156 1.156 0 0 0-.732.732l-.258.774a.145.145 0 0 1-.274 0l-.258-.774a1.156 1.156 0 0 0-.732-.732l-.774-.258a.145.145 0 0 1 0-.274l.774-.258c.346-.115.617-.386.732-.732L13.863.1z"></path></symbol><symbol id="sun-fill" viewbox="0 0 16 16"><path d="M8 12a4 4 0 1 0 0-8 4 4 0 0 0 0 8zM8 0a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 0zm0 13a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 13zm8-5a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2a.5.5 0 0 1 .5.5zM3 8a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2A.5.5 0 0 1 3 8zm10.657-5.657a.5.5 0 0 1 0 .707l-1.414 1.415a.5.5 0 1 1-.707-.708l1.414-1.414a.5.5 0 0 1 .707 0zm-9.193 9.193a.5.5 0 0 1 0 .707L3.05 13.657a.5.5 0 0 1-.707-.707l1.414-1.414a.5.5 0 0 1 .707 0zm9.193 2.121a.5.5 0 0 1-.707 0l-1.414-1.414a.5.5 0 0 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .707zM4.464 4.465a.5.5 0 0 1-.707 0L2.343 3.05a.5.5 0 1 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .708z"></path></symbol></svg><a class="visually-hidden-focusable skip-link" href="#main-content">Skip to main content</a>
  <div class="container-fluid top-nav-container">
    <div class="col-md-8">
      <div class="large-logo">
        <img id="incubator-logo" alt="Carpentries Incubator" src="assets/images/incubator-logo.svg"><span class="badge text-bg-danger">
          <abbr title="This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught.">
            <a href="https://docs.carpentries.org/resources/curriculum/lesson-life-cycle.html" class="external-link alert-link">
              <i aria-hidden="true" class="icon" data-feather="alert-octagon" style="border-radius: 5px"></i>
              Pre-Alpha
            </a>
            <span class="visually-hidden">This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught.</span>
          </abbr>
        </span>

      </div>
    </div>
    <div class="selector-container">
      <div id="theme-selector">
        <li class="nav-item dropdown" id="theme-button-list">
          <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
            <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="bd-theme-text">
<li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                Light
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                Dark
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                Auto
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
          </ul>
</li>
      </div>

      <div class="dropdown" id="instructor-dropdown">
        <button class="btn btn-secondary dropdown-toggle bordered-button" type="button" id="dropdownMenu1" data-bs-toggle="dropdown" aria-expanded="false">
          <i aria-hidden="true" class="icon" data-feather="eye"></i> Learner View <i data-feather="chevron-down"></i>
        </button>
        <ul class="dropdown-menu" aria-labelledby="dropdownMenu1">
<li><button class="dropdown-item" type="button" onclick="window.location.href='instructor/aio.html';">Instructor View</button></li>
        </ul>
</div>
    </div>
  </div>
  <hr></header><nav class="navbar navbar-expand-xl bottom-nav incubator" aria-label="Main Navigation"><div class="container-fluid nav-container">
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle Navigation">
      <span class="navbar-toggler-icon"></span>
      <span class="menu-title">Menu</span>
    </button>
    <div class="nav-logo">
      <img class="small-logo" alt="Carpentries Incubator" src="assets/images/incubator-logo-sm.svg">
</div>
    <div class="lesson-title-md">
      Fundamentals of Natural Language Processing (NLP) in Python
    </div>
    <div class="search-icon-sm">
      <!-- TODO: do not show until we have search
        <i role="img" aria-label="Search the All In One page" data-feather="search"></i>
      -->
    </div>
    <div class="desktop-nav">
      <ul class="navbar-nav me-auto mb-2 mb-lg-0">
<li class="nav-item">
          <span class="lesson-title">
            Fundamentals of Natural Language Processing (NLP) in Python
          </span>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="key-points.html">Key Points</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="reference.html#glossary">Glossary</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="profiles.html">Learner Profiles</a>
        </li>
        <li class="nav-item dropdown">
          <button class="nav-link dropdown-toggle" id="navbarDropdown" data-bs-toggle="dropdown" aria-expanded="false">
            More <i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu" aria-labelledby="navbarDropdown"></ul>
</li>
      </ul>
</div>
    <!--
    <form class="d-flex col-md-2 search-form">
      <fieldset disabled>
      <input class="form-control me-2 searchbox" type="search" placeholder="" aria-label="">
        <button class="btn btn-outline-success tablet-search-button"  type="submit">
          <i class="search-icon" data-feather="search" role="img" aria-label="Search the All In One page"></i>
        </button>
      </fieldset>
    </form>
    -->
    <a id="search-button" class="btn btn-primary" href="aio.html" role="button" aria-label="Search the All In One page">Search the All In One page</a>
  </div>
<!--/div.container-fluid -->
</nav><div class="col-md-12 mobile-title">
  Fundamentals of Natural Language Processing (NLP) in Python
</div>

<aside class="col-md-12 lesson-progress"><div style="width: %" class="percentage">
    %
  </div>
  <div class="progress incubator">
    <div class="progress-bar incubator" role="progressbar" style="width: %" aria-valuenow="" aria-label="Lesson Progress" aria-valuemin="0" aria-valuemax="100">
    </div>
  </div>
</aside><div class="container">
      <div class="row">
        <!-- START: inst/pkgdown/templates/navbar.html -->
<div id="sidebar-col" class="col-lg-4">
  <div id="sidebar" class="sidebar">
      <nav aria-labelledby="flush-headingEleven"><button role="button" aria-label="close menu" alt="close menu" aria-expanded="true" aria-controls="sidebar" class="collapse-toggle" data-collapse="Collapse " data-episodes="Episodes ">
          <i class="search-icon" data-feather="x" role="img"></i>
        </button>
        <div class="sidebar-inner">
          <div class="row mobile-row" id="theme-row-mobile">
            <div class="col" id="theme-selector">
              <li class="nav-item dropdown" id="theme-button-list">
                <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
                  <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><span class="d-lg-none ms-1" id="bd-theme-text">Toggle Theme</span>
                </button>
                <ul class="dropdown-menu dropdown-menu-right" aria-labelledby="bd-theme-text">
<li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                      Light
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                      Dark
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                      <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                      Auto
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                </ul>
</li>
            </div>
          </div>
          <div class="row mobile-row">
            <div class="col">
              <div class="sidenav-view-selector">
                <div class="accordion accordion-flush" id="accordionFlush9">
                  <div class="accordion-item">
                    <h2 class="accordion-header" id="flush-headingNine">
                      <button class="accordion-button collapsed" id="instructor" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseNine" aria-expanded="false" aria-controls="flush-collapseNine">
                        <i id="eye" aria-hidden="true" class="icon" data-feather="eye"></i> Learner View
                      </button>
                    </h2>
                    <div id="flush-collapseNine" class="accordion-collapse collapse" aria-labelledby="flush-headingNine" data-bs-parent="#accordionFlush2">
                      <div class="accordion-body">
                        <a href="instructor/aio.html">Instructor View</a>
                      </div>
                    </div>
                  </div>
<!--/div.accordion-item-->
                </div>
<!--/div.accordion-flush-->
              </div>
<!--div.sidenav-view-selector -->
            </div>
<!--/div.col -->

            <hr>
</div>
<!--/div.mobile-row -->

          <div class="accordion accordion-flush" id="accordionFlush11">
            <div class="accordion-item">

              <button id="chapters" class="accordion-button show" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseEleven" aria-expanded="false" aria-controls="flush-collapseEleven">
                <h2 class="accordion-header chapters" id="flush-headingEleven">
                  EPISODES
                </h2>
              </button>
              <div id="flush-collapseEleven" class="accordion-collapse show collapse" aria-labelledby="flush-headingEleven" data-bs-parent="#accordionFlush11">

                <div class="accordion-body">
                  <div class="accordion accordion-flush" id="accordionFlush1">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading1">
        <a href="index.html">Summary and Setup</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush2">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading2">
        <a href="01-introduction.html">1. Introduction</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush3">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading3">
        <a href="02-preprocessing.html">2. From words to vectors</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush4">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading4">
        <a href="03-transformers.html">3. Transformers: BERT and Beyond</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush5">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading5">
        <a href="04-LargeLanguageModels.html">4. Episode 3: Using large language models</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

                </div>
              </div>
            </div>

            <hr class="half-width">
<div class="accordion accordion-flush lesson-resources" id="accordionFlush12">
              <div class="accordion-item">
                <h2 class="accordion-header" id="flush-headingTwelve">
                  <button class="accordion-button collapsed" id="lesson-resources" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseTwelve" aria-expanded="false" aria-controls="flush-collapseTwelve">
                    RESOURCES
                  </button>
                </h2>
                <div id="flush-collapseTwelve" class="accordion-collapse collapse" aria-labelledby="flush-headingTwelve" data-bs-parent="#accordionFlush12">
                  <div class="accordion-body">
                    <ul>
<li>
                        <a href="key-points.html">Key Points</a>
                      </li>
                      <li>
                        <a href="reference.html#glossary">Glossary</a>
                      </li>
                      <li>
                        <a href="profiles.html">Learner Profiles</a>
                      </li>

                    </ul>
</div>
                </div>
              </div>
            </div>
            <hr class="half-width lesson-resources">
<a href="aio.html">See all in one page</a>


            <hr class="d-none d-sm-block d-md-none">
<div class="d-grid gap-1">

            </div>
          </div>
<!-- /div.accordion -->
        </div>
<!-- /div.sidebar-inner -->
      </nav>
</div>
<!-- /div.sidebar -->
  </div>
<!-- /div.sidebar-col -->
<!-- END:   inst/pkgdown/templates/navbar.html-->

        <!-- START: inst/pkgdown/templates/content-extra.html -->
  <div class="col-xl-8 col-lg-12 primary-content">
    <main id="main-content" class="main-content"><div class="container lesson-content">
        
        
<section id="aio-01-introduction"><p>Content from <a href="01-introduction.html">Introduction</a></p>
<hr>
<p>Last updated on 2025-10-07 |

        <a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/edit/main/episodes/01-introduction.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What is Natural Language Processing?</li>
<li>What are some common applications of NLP?</li>
<li>What makes text different from other data?</li>
<li>Why not just learn Large Language Models?</li>
<li>What linguistic properties should we consider when dealing with
texts?</li>
<li>How does NLP relates to Deep Learning methodologies?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Define Natural Language Processing</li>
<li>Show the most relevant NLP tasks and applications in practice</li>
<li>Learn how to handle Linguistic Data and how is Linguistics relevant
to NLP</li>
<li>Learn a general workflow for solving NLP tasks</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="what-is-nlp">What is NLP?<a class="anchor" aria-label="anchor" href="#what-is-nlp"></a>
</h2>
<hr class="half-width">
<p>Natural language processing (NLP) is an area of research and
application that focuses on making human languages processable for
computers, so that they can perform useful tasks. It is therefore not a
single method, but a collection of techniques that help us deal with
linguistic inputs. The range of techniques spans simple word counts, to
Machine Learning (ML) methods, all the way up to complex Deep Learning
(DL) architectures.</p>
<p>We use the term “natural language”, as opposed to “artificial
language” such as programming languages, which are by design constructed
to be easily formalized into machine-readable instructions. In contrast
to programming languages, natural languages are complex, ambiguous, and
heavily context-dependent, making them challenging for computers to
process. To complicate matters, there is not only a single <em>human
language</em>. More than 7000 languages are spoken around the world,
each with its own grammar, vocabulary, and cultural context.</p>
<p>In this course we will mainly focus on written language, specifically
written English, we leave out audio and speech, as they require a
different kind of input processing. But consider that we use English
only as a convenience so we can address the technical aspects of
processing textual data. While ideally most of the concepts from NLP
apply to most languages, one should always be aware that certain
languages require different approaches to solve seemingly similar
problems. We would like to encourage the usage of NLP in other less
widely known languages, especially if it is a minority language. You can
read more about this topic in this <a href="https://www.ruder.io/nlp-beyond-english/" class="external-link">blogpost</a>.</p>
<p>We can already find differences between languages in the most basic
step for processing text. Take the problem of segmenting text into
meaningful units, most of the times these units are words, in NLP we
call this task <strong>tokenization</strong>. A naive approach is to
obtain individual words by splitting text by spaces, as it seems obvious
that we always separate words with spaces. Just as human beings break up
sentences into words, phrases and other units in order to learn about
grammar and other structures of a language, NLP techniques achieve a
similar goal through tokenization. Let’s see how can we segment or
<strong>tokenize</strong> a sentence in English:</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a>english_sentence <span class="op">=</span> <span class="st">"Tokenization isn't always trivial."</span></span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a>english_words <span class="op">=</span> english_sentence.split(<span class="st">" "</span>)</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="bu">print</span>(english_words)</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(english_words))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>['Tokenization', "isn't", 'always', 'trivial.']
4</code></pre>
</div>
<p>The words are mostly well separated, however we do not get fully
formed words (we have punctuation with the period after “trivial” and
also special cases such as the abbreviation of “is not” into “isn’t”).
But at least we get a rough count of the number of words present in the
sentence. Let’s now look at the same example in Chinese:</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="co"># Chinese Translation of "Tokenization is not always trivial"</span></span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>chinese_sentence <span class="op">=</span> <span class="st">"标记化并不总是那么简单"</span> </span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a>chinese_words <span class="op">=</span> chinese_sentence.split(<span class="st">" "</span>)</span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a><span class="bu">print</span>(chinese_words)</span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(chinese_words))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>['标记化并不总是那么简单']
1</code></pre>
</div>
<p>The same example however did not work in Chinese, because Chinese
does not use spaces to separate words. This is an example of how the
idiosyncrasies of human language affects how we can process them with
computers. We therefore need to use a tokenizer specifically designed
for Chinese to obtain the list of well-formed words in the text. Here we
use a “pre-trained” tokenizer called <strong>jieba</strong>, which uses
a dictionary-based approach to correctly identify the distinct
words:</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="im">import</span> jieba  <span class="co"># A popular Chinese text segmentation library</span></span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>chinese_sentence <span class="op">=</span> <span class="st">"标记化并不总是那么简单"</span></span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a>chinese_words <span class="op">=</span> jieba.lcut(chinese_sentence)</span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a><span class="bu">print</span>(chinese_words)</span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(chinese_words))  <span class="co"># Output: 7</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>['标记', '化', '并', '不', '总是', '那么', '简单']
7</code></pre>
</div>
<p>We can trust that the output is valid because we are using a verified
library - jieba, even though we don’t speak Chinese. Another interesting
aspect is that the Chinese sentence has more words than the English one,
even though they convey the same meaning. This shows the complexity of
dealing with more than one language at a time, as is the case in task
such as <strong>Machine Translation</strong> (using computers to
translate speech or text from one human language to another).</p>
<div id="pre-trained-models-and-fine-tuning" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="pre-trained-models-and-fine-tuning" class="callout-inner">
<h3 class="callout-title">Pre-trained Models and Fine-tuning</h3>
<div class="callout-content">
<p>These two terms frequently arise in discussions of NLP. The notion of
pre-trained comes from Machine Learning and describes a model that has
already been optimized on relevant data for a given task. Such a model
can typically be loaded and applied directly to new datasets, often
working “out of the box.” without need of further refinement. Ideally,
publicly released pre-trained models have undergone rigorous testing for
both generalization and output quality on different textual data that it
was intended to be used on. Nevertheless, it remains essential to
carefully review the evaluation methods used before relying on them in
practice. It is also recommended that you perform your own evaluation of
the model on text that you intend to use it on.</p>
<p>Sometimes a pre-trained model is of good quality, but it does not fit
the nuances of our specific dataset. For example, the model was trained
on newspaper articles but you are interested in poetry. In this case, it
is common to perform <em>fine-tuning</em>, this means that instead of
training your own model from scratch, you start with the knowledge
obtained in the pre-trained model and adjust it (fine-tune it) to work
optimally with your specific data. If this is done well it leads to
increased performance in the specific task you are trying to solve. The
advantage of fine-tuning is that you often do not need a large amount of
data to improve the results, hence the popularity of the technique.</p>
</div>
</div>
</div>
<p>Natural Language Processing deals with the challenges of correctly
processing and generating text in any language. This can be as simple as
counting word frequencies to detect different writing styles, using
statistical methods to classify texts into different categories, or
using <strong>deep neural networks</strong> to generate human-like text
by exploiting word co-occurrences in large amounts of texts.</p>
</section><section><h2 class="section-heading" id="why-should-we-learn-nlp-fundamentals">Why should we learn NLP Fundamentals?<a class="anchor" aria-label="anchor" href="#why-should-we-learn-nlp-fundamentals"></a>
</h2>
<hr class="half-width">
<p>In the past decade, NLP has evolved significantly, especially in the
field of deep learning, to the point that it has become embedded in our
daily lives, one just needs to look at the term Large Language Models
(LLMs), the latest generation of NLP models, which is now ubiquitous in
news media and tech products we use on a daily basis.</p>
<p>The term LLM now is often (and wrongly) used as a synonym of
Artificial Intelligence. We could therefore think that today we just
need to learn how to manipulate LLMs in order to fulfill our research
goals involving textual data. The truth is that Language Modeling has
always been part of the core tasks of NLP, therefore, by learning NLP
you will understand better where are the main ideas behind LLMs coming
from.</p>
<figure><img src="fig/intro0_cs_nlp.png" alt="NLP is an interdisciplinary field, and LLMs are just a subset of it" class="figure mx-auto d-block"><div class="figcaption">NLP is an interdisciplinary field, and LLMs are
just a subset of it</div>
</figure><p>LLM is a blanket term for an assembly of large neural networks that
are trained on vast amounts of text data with the objective of
optimizing for language modeling. Once they are trained, they are used
to generate human-like text or fine-tunned to perform much more advanced
tasks. Indeed, the surprising and fascinating properties that emerge
from training models at this scale allows us to solve different complex
tasks such as answer elaborate questions, translate languages, solve
complex problems, generate narratives that emulate reasoning, and many
more, all of this with a single tool.</p>
<p>It is important, however, to pay attention to what is happening
behind the scenes in order to be able <strong>trace sources of errors
and biases</strong> that get hidden in the complexity of these models.
The purpose of this course is precisely to take a step back, and
understand that:</p>
<ul>
<li>There are a wide variety of tools available, beyond LLMs, that do
not require so much computing power</li>
<li>Sometimes a much simpler method than an LLM is available that can
solve our problem at hand</li>
<li>If we learn how previous approaches to solve linguistic problems
were designed, we can better understand the limitations of LLMs and how
to use them effectively</li>
<li>LLMs excel at confidently delivering information, without any
regards for correctness. This calls for a careful design of
<strong>evaluation metrics</strong> that give us a better understanding
of the quality of the generated content.</li>
</ul>
<p>Let’s go back to our problem of segmenting text and see what ChatGPT
has to say about tokenizing Chinese text:</p>
<figure><img src="fig/intro1.png" alt="ChatGPT Just Works! Does it…?" class="figure mx-auto d-block"><div class="figcaption">ChatGPT Just Works! Does it…?</div>
</figure><p>We got what sounds like a straightforward confident answer. However,
it is not clear how the model arrived at this solution. Second, we do
not know whether the solution is correct or not. In this case ChatGPT
made some assumptions for us, such as choosing a specific kind of
tokenizer to give the answer, and since we do not speak the language, we
do not know if this is indeed the best approach to tokenize Chinese
text. If we understand the concept of Token (which we will today!), then
we can be more informed about the quality of the answer, whether it is
useful to us, and therefore make a better use of the model.</p>
<p>And by the way, ChatGPT was <strong>almost</strong> correct, in the
specific case of the gpt-4 tokenizer, the model will return 12 tokens
(not 11!) for the given Chinese sentence.</p>
<figure><img src="fig/intro1b.png" alt="GPT-4 Tokenization Example" class="figure mx-auto d-block"><div class="figcaption">GPT-4 Tokenization Example</div>
</figure><p>We can also argue if the statement “Chinese is generally tokenized
character by character” is an overstatement or not. In any case, the
real question here is: Are we ok with <em>almost correct answers</em>?
Please note that this is not a call to avoid using LLM’s but a call for
a careful consideration of usage and more importantly, an attempt to
explain the mechanisms behind via NLP concepts.</p>
</section><section><h2 class="section-heading" id="language-as-data">Language as Data<a class="anchor" aria-label="anchor" href="#language-as-data"></a>
</h2>
<hr class="half-width">
<p>From a more technical perspective, NLP focuses on applying advanced
statistical techniques to linguistic data. This is a key factor, since
we need a structured dataset with a well defined set of features in
order to manipulate it numerically. Your first task as an NLP
practitioner is to <strong>understand what aspects of textual data are
relevant for your application</strong> and apply techniques to
systematically extract meaningful features from unstructured data (if
using statistics or Machine Learning) or choose an appropriate neural
architecture (if using Deep Learning) that can help solve our problem at
hand.</p>
<div class="section level3">
<h3 id="what-is-a-word">What is a word?<a class="anchor" aria-label="anchor" href="#what-is-a-word"></a>
</h3>
<p>When dealing with language our basic data unit is usually a word. We
deal with sequences of words and with how they relate to each other to
generate meaning in text pieces. Thus, our first step will be to load a
text file and provide it with structure by splitting it into valid words
(tokenization)!</p>
<div id="token-vs-word" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="token-vs-word" class="callout-inner">
<h3 class="callout-title">Token vs Word</h3>
<div class="callout-content">
<p>For simplicity, in the rest of the course we will use the terms
“word” and “token” interchangeably, but as we just saw they do not
always have the same granularity. Originally the concept of token
comprised dictionary words, numeric symbols and punctuation. Nowadays,
tokenization has also evolved and became an optimization task on its own
(How can we segment text in a way that neural networks learn optimally
from text?). Tokenizers allow one to reconstruct or revert back to the
original pre-tokenized form of tokens or words, hence we can afford to
use <em>token</em> and <em>word</em> as synonyms. If you are curious,
you can visualize how different state-of-the-art tokenizers split text
<a href="https://tiktokenizer.vercel.app/" class="external-link">in this WebApp</a></p>
</div>
</div>
</div>
<p>Let’s open a file, read it into a string and split it by spaces. We
will print the original text and the list of “words” to see how they
look:</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">"84_frankenstein_clean.txt"</span>) <span class="im">as</span> f:</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>  text <span class="op">=</span> f.read()</span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a><span class="bu">print</span>(text[:<span class="dv">100</span>])</span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Length:"</span>, <span class="bu">len</span>(text))</span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a>proto_tokens <span class="op">=</span> text.split()</span>
<span id="cb7-8"><a href="#cb7-8" tabindex="-1"></a><span class="bu">print</span>(proto_tokens[:<span class="dv">40</span>])</span>
<span id="cb7-9"><a href="#cb7-9" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(proto_tokens))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Letter 1 St. Petersburgh, Dec. 11th, 17-- TO Mrs. Saville, England You will rejoice to hear that no disaster has accompanied the commencement of an en
Length: 417931

Proto-Tokens:
['Letter', '1', 'St.', 'Petersburgh,', 'Dec.', '11th,', '17--', 'TO', 'Mrs.', 'Saville,', 'England', 'You', 'will', 'rejoice', 'to', 'hear', 'that', 'no', 'disaster', 'has', 'accompanied', 'the', 'commencement', 'of', 'an', 'enterprise', 'which', 'you', 'have', 'regarded', 'with', 'such', 'evil', 'forebodings.', 'I', 'arrived', 'here', 'yesterday,', 'and', 'my']
74942</code></pre>
</div>
<p>Splitting by white space is possible but needs several extra steps to
separate out punctuation appropriately. A more sophisticated approach is
to use the <a href="https://github.com/explosion/spaCy" class="external-link">spaCy</a>
library to segment the text into human-readable tokens. First we will
download the pre-trained model, in this case we only need the small
English version:</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="op">!</span> python <span class="op">-</span>m spacy download en_core_web_sm</span></code></pre>
</div>
<p>This is a model that spaCy already trained for us on a subset of web
English data. Hence, the model already “knows” how to tokenize into
English words. When the model processes a string, it does not only do
the splitting for us but already provides more advanced linguistic
properties of the tokens (such as part-of-speech tags, or named
entities). Let’s now import the model and use it to parse our
document:</p>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a><span class="im">import</span> spacy</span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a>nlp <span class="op">=</span> spacy.load(<span class="st">"en_core_web_sm"</span>) <span class="co"># we load the small English model for efficiency</span></span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a>doc <span class="op">=</span> nlp(text) <span class="co"># Doc is a python object with several methods to retrieve linguistic properties</span></span>
<span id="cb10-6"><a href="#cb10-6" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" tabindex="-1"></a><span class="co"># SpaCy-Tokens</span></span>
<span id="cb10-8"><a href="#cb10-8" tabindex="-1"></a>tokens <span class="op">=</span> [token.text <span class="cf">for</span> token <span class="kw">in</span> doc] <span class="co"># Note that spacy tokens are also python objects </span></span>
<span id="cb10-9"><a href="#cb10-9" tabindex="-1"></a><span class="bu">print</span>(tokens[:<span class="dv">40</span>])</span>
<span id="cb10-10"><a href="#cb10-10" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(tokens))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>['Letter', '1', 'St.', 'Petersburgh', ',', 'Dec.', '11th', ',', '17', '-', '-', 'TO', 'Mrs.', 'Saville', ',', 'England', 'You', 'will', 'rejoice', 'to', 'hear', 'that', 'no', 'disaster', 'has', 'accompanied', 'the', 'commencement', 'of', 'an', 'enterprise', 'which', 'you', 'have', 'regarded', 'with', 'such', 'evil', 'forebodings', '.']
85713</code></pre>
</div>
<p>The differences look subtle at the beginning, but if we carefully
inspect the way spaCy splits the text, we can see the advantage of using
a specialized tokenizer. There are also several useful features that
spaCy provides us with. For example, we can choose to extract only
symbols, or only alphanumerical tokens, and more advanced linguistic
properties, for example we can remove punctuation and only keep
alphanumerical tokens:</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a>only_words <span class="op">=</span> [token <span class="cf">for</span> token <span class="kw">in</span> doc <span class="cf">if</span> token.is_alpha]  <span class="co"># Only alphanumerical tokens</span></span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a><span class="bu">print</span>(only_words[:<span class="dv">50</span>])</span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(only_words))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[Letter, Petersburgh, TO, Saville, England, You, will, rejoice, to, hear, that, no, disaster, has, accompanied, the, commencement, of, an, enterprise, which, you, have, regarded, with, such, evil, forebodings, I, arrived, here, yesterday, and, my, first, task, is, to, assure, my, dear, sister, of, my, welfare, and, increasing, confidence, in, the]
75062</code></pre>
</div>
<p>or keep only the verbs from our text:</p>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a>only_verbs <span class="op">=</span> [token <span class="cf">for</span> token <span class="kw">in</span> doc <span class="cf">if</span> token.pos_ <span class="op">==</span> <span class="st">"VERB"</span>]  <span class="co"># Only verbs</span></span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a><span class="bu">print</span>(only_verbs[:<span class="dv">10</span>])</span>
<span id="cb14-3"><a href="#cb14-3" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(only_verbs))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[rejoice, hear, accompanied, regarded, arrived, assure, increasing, walk, feel, braces]
10148</code></pre>
</div>
<p>SpaCy also predicts the sentences under the hood for us. It might
seem trivial to you as a human reader to recognize where a sentence
begins and ends but for a machine, just like finding words, finding
sentences is a task on its own, for which sentence-segmentation models
exist. In the case of Spacy, we can access the sentences like this:</p>
<div class="codewrapper sourceCode" id="cb16">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a>sentences <span class="op">=</span> [sent.text <span class="cf">for</span> sent <span class="kw">in</span> doc.sents] <span class="co"># Sentences are also python objects</span></span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a><span class="bu">print</span>(sentences[:<span class="dv">5</span>])</span>
<span id="cb16-3"><a href="#cb16-3" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(sentences))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>['Letter 1 St. Petersburgh, Dec. 11th, 17-- TO Mrs. Saville, England You will rejoice to hear that no disaster has accompanied the commencement of an enterprise which you have regarded with such evil forebodings.', 'I arrived here yesterday, and my first task is to assure my dear sister of my welfare and increasing confidence in the success of my undertaking.', 'I am already far north of London, and as I walk in the streets of Petersburgh, I feel a cold northern breeze play upon my cheeks, which braces my nerves and fills me with delight.', 'Do you understand this feeling?', 'This breeze, which has traveled from the regions towards which I am advancing, gives me a foretaste of those icy climes.']
3317</code></pre>
</div>
<p>We can also see what named entities the model predicted:</p>
<div class="codewrapper sourceCode" id="cb18">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(doc.ents))</span>
<span id="cb18-2"><a href="#cb18-2" tabindex="-1"></a><span class="cf">for</span> ent <span class="kw">in</span> doc.ents[:<span class="dv">5</span>]:</span>
<span id="cb18-3"><a href="#cb18-3" tabindex="-1"></a>    <span class="bu">print</span>(ent.label_, ent.text)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>1713
DATE Dec. 11th
CARDINAL 17
PERSON Saville
GPE England
DATE yesterday</code></pre>
</div>
<p>These are just basic tests to demonstrate how you can immediately
process the structure of text using existing NLP libraries. The spaCy
models we used are simpler relative to state of the art approaches. So
the more complex the input text and task, the more errors are likely to
appear when using such models. The biggest advantage of using these
existing libraries is that they help you transform unstructured plain
text files into structured data that you can manipulate later for your
own goals such as training language models.</p>
<div id="nlp-in-the-real-world" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="nlp-in-the-real-world" class="callout-inner">
<h3 class="callout-title">NLP in the real world</h3>
<div class="callout-content">
<p>Name three to five tools/products that you use on a daily basis and
that you think leverage NLP techniques. To do this exercise you may make
use of the Web.</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<p>These are some of the most popular NLP-based products that we use on
a daily basis:</p>
<ul>
<li>Agentic Chatbots (ChatGPT, Perplexity)</li>
<li>Voice-based assistants (e.g., Alexa, Siri, Cortana)</li>
<li>Machine translation (e.g., Google translate, DeepL, Amazon
translate)</li>
<li>Search engines (e.g., Google, Bing, DuckDuckGo)</li>
<li>Keyboard autocompletion on smartphones</li>
<li>Spam filtering</li>
<li>Spell and grammar checking apps</li>
<li>Customer care chatbots</li>
<li>Text summarization tools (e.g., news aggregators)</li>
<li>Sentiment analysis tools (e.g., social media monitoring)</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="nlp-tasks">NLP tasks<a class="anchor" aria-label="anchor" href="#nlp-tasks"></a>
</h3>
<p>The previous exercise shows that a great deal of NLP techniques are
embedded in our daily life. Indeed NLP is an important component in a
wide range of software applications that we use in our day to day
activities.</p>
<p>There are several ways to describe the tasks that NLP solves. From
the Machine Learning perspective, we have:</p>
<ul>
<li>Unsupervised tasks: exploiting existing patterns from large amounts
of text.</li>
</ul>
<figure><img src="fig/intro_unsupervised.png" width="582" alt="Unsupervised Learning" class="figure mx-auto d-block"><div class="figcaption">Unsupervised Learning</div>
</figure><ul>
<li>Supervised tasks: learning to classify texts given a labeled set of
examples</li>
</ul>
<figure><img src="fig/intro_supervised.png" width="605" alt="Supervised Learning" class="figure mx-auto d-block"><div class="figcaption">Supervised Learning</div>
</figure><p>The Deep Learning perspective usually involves the selection of the
right model among different neural network architectures to tackle an
NLP task, such as:</p>
<ul>
<li><p>Multi-layer Perceptron</p></li>
<li><p>Recurrent Neural Network</p></li>
<li><p>Convolutional Neural Network</p></li>
<li><p>Long-Short Term Memory Networks (LSTMs)</p></li>
<li><p>Transformer (including LLMs!)</p></li>
</ul>
<p>Regardless of the chosen method, below we show one possible taxonomy
of NLP tasks. The tasks are grouped together with some of their most
prominent applications. This is definitely a non-exhaustive list, as in
reality there are hundreds of them, but it is a good start:</p>
<figure><img src="fig/intro_taxonomy.png" width="630" alt="A taxonomy of NLP Tasks" class="figure mx-auto d-block"><div class="figcaption">A taxonomy of NLP Tasks</div>
</figure><ul>
<li>
<p><strong>Text Classification</strong>: Assign one or more labels
to a given piece of text. This text is usually referred to as a
<em>document</em> and in our context this can be a sentence, a
paragraph, a book chapter, etc…</p>
<ul>
<li>
<strong>Language Identification</strong>: determining the language
in which a particular input text is written.</li>
<li>
<strong>Spam Filtering</strong>: classifying emails into spam or not
spam based on their content.</li>
<li>
<strong>Authorship Attribution</strong>: determining the author of a
text based on its style and content (based on the assumption that each
author has a unique writing style).</li>
<li>
<strong>Sentiment Analysis</strong>: classifying text into positive,
negative or neutral sentiment. For example, in the sentence “I love this
product!”, the model would classify it as positive sentiment.</li>
</ul>
</li>
<li>
<p><strong>Token Classification</strong>: The task of individually
assigning one label to each word in a document. This is a one-to-one
mapping; however, because words do not occur in isolation and their
meaning depend on the sequence of words to the left or the right of
them, this is also called Word-In-Context Classification or Sequence
Labeling and usually involves syntactic and semantic analysis.</p>
<ul>
<li>
<strong>Part-Of-Speech Tagging</strong>: is the task of assigning a
part-of-speech label (e.g., noun, verb, adjective) to each word in a
sentence.</li>
<li>
<strong>Chunking</strong>: splitting a running text into “chunks” of
words that together represent a meaningful unit: phrases, sentences,
paragraphs, etc.</li>
<li>
<strong>Word Sense Disambiguation</strong>: based on the context
what does a word mean (think of “book” in “I read a book.” vs “I want to
book a flight.”)</li>
<li>
<strong>Named Entity Recognition</strong>: recognize world entities
in text, e.g. Persons, Locations, Book Titles, or many others. For
example “Mary Shelley” is a person, “Frankenstein or the Modern
Prometheus” is a book, etc.</li>
<li>
<strong>Semantic Role Labeling</strong>: the task of finding out
“Who did what to whom?” in a sentence: information from events such as
agents, participants, circumstances, subject-verb-object triples
etc.</li>
<li>
<strong>Relation Extraction</strong>: the task of identifying named
relationships between entities in a text, e.g. “Apple is based in
California” has the relation (Apple, based_in, California).</li>
<li>
<strong>Co-reference Resolution</strong>: the task of determining
which words refer to the same entity in a text, e.g. “Mary is a doctor.
She works at the hospital.” Here “She” refers to “Mary”.</li>
<li>
<strong>Entity Linking</strong>: the task of disambiguation of named
entities in a text, linking them to their corresponding entries in a
knowledge base, e.g. Mary Shelley’s biography in Wikipedia.</li>
</ul>
</li>
<li>
<p><strong>Language Modeling</strong>: Given a sequence of words,
the model predicts the next word. For example, in the sentence “The
capital of France is _____”, the model should predict “Paris” based on
the context. This task was initially useful for building solutions that
require speech and optical character recognition (even handwriting),
language translation and spelling correction. Nowadays this has scaled
up to the LLMs that we know. A byproduct of pre-trained Language
Modeling is the <strong>vectorized representation</strong> of texts
which allows to perform specific tasks such as:</p>
<ul>
<li>
<strong>Text Similarity</strong>: The task of determining how
similar two pieces of text are.</li>
<li>
<strong>Plagiarism detection</strong>: determining whether a piece
of text, B, is close enough to another known piece of text, A, which
increases the likelihood that it was plagiarized.</li>
<li>
<strong>Document clustering</strong>: grouping similar texts
together based on their content.</li>
<li>
<strong>Topic modelling</strong>: a specific instance of clustering,
here we automatically identify abstract “topics” that occur in a set of
documents, where each topic is represented as a cluster of words that
frequently appear together.</li>
<li>
<strong>Information Retrieval</strong>: this is the task of finding
relevant information or documents from a large collection of
unstructured data based on user’s query, e.g., “What’s the best
restaurant near me?”.</li>
</ul>
</li>
<li>
<p><strong>Text Generation</strong>: the task of generating text
based on a given input. This is usually done by generating the output
word by word, conditioned on both the input and the output so far. The
difference with Language Modeling is that for generation there are
higher-level generation objectives such as:</p>
<ul>
<li>
<strong>Machine Translation</strong>: translating text from one
language to another, e.g., “Hello” in English to “Que tal” in
Spanish.</li>
<li>
<strong>Summarization</strong>: generating a concise summary of a
longer text. It can be abstractive (generating new sentences that
capture the main ideas of the original text) but also extractive
(selecting important sentences from the original text).</li>
<li>
<strong>Paraphrasing</strong>: generating a new sentence that
conveys the same meaning as the original sentence, e.g., “The cat is on
the mat.” to “The mat has a cat on it.”.</li>
<li>
<strong>Question Answering</strong>: given a question and a context,
the model generates an answer. For example, given the question “What is
the capital of France?” and the Wikipedia article about France as the
context, the model should answer “Paris”. This task can be approached as
a text classification problem (where the answer is one of the predefined
options) or as a generative task (where the model generates the answer
from scratch).</li>
<li>
<strong>Conversational Agent (ChatBot)</strong>: Building a system
that interacts with a user via natural language, e.g., “What’s the
weather today, Siri?”. These agents are widely used to improve user
experience in customer service, personal assistance and many other
domains.</li>
</ul>
</li>
</ul>
<p>For the purposes of this episode, we will focus on <strong>supervised
learning</strong> tasks and we will emphasize how the
<strong>Transformer architecture</strong> is used to tackle some of
them.</p>
<div id="inputs-and-outputs" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="inputs-and-outputs" class="callout-inner">
<h3 class="callout-title">Inputs and Outputs</h3>
<div class="callout-content">
<p>Look at the NLP Task taxonomy described above and write down a couple
of examples of (Input, Output) instance pairs that you would need in
order to train a supervised model for your chosen task.</p>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2"> Show me the solution </h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" aria-labelledby="headingSolution2" data-bs-parent="#accordionSolution2">
<div class="accordion-body">
<p>Example: the task of Conversational agent. Here are 3 instances to
provide supervision for a model:</p>
<p><strong>Input:</strong> “Hello, how are you?”
<strong>Output:</strong> “I am fine thanks!”</p>
<p><strong>Input:</strong> “Do you know at what time is the World Cup
final today?” <strong>Output:</strong> “Yes, the World Cup final will be
at 6pm CET”</p>
<p><strong>Input:</strong> “What color is my shirt?”
<strong>Output:</strong> “Sorry, I am unable to see what you are
wearing.”</p>
</div>
</div>
</div>
</div>
<div id="nlp-libraries" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="nlp-libraries" class="callout-inner">
<h3 class="callout-title">NLP Libraries</h3>
<div class="callout-content">
<p>Related to the need of shaping our problems into a known task, there
are several existing NLP libraries which provide a wide range of models
that we can use out-of-the-box (without further need of modification).
We already saw simple examples using SpaCy for English and jieba for
Chinese. Again, as a non-exhaustive list, we mention some widely used
NLP libraries in Python:</p>
<ul>
<li><a href="https://github.com/nltk/nltk" class="external-link">NLTK</a></li>
<li><a href="https://github.com/explosion/spaCy" class="external-link">spaCy</a></li>
<li><a href="https://github.com/RaRe-Technologies/gensim" class="external-link">Gensim</a></li>
<li><a href="https://github.com/stanfordnlp/stanza" class="external-link">Stanza</a></li>
<li><a href="https://github.com/flairNLP/flair" class="external-link">Flair</a></li>
<li><a href="https://github.com/facebookresearch/fastText" class="external-link">FastText</a></li>
<li><a href="https://github.com/huggingface/transformers" class="external-link">HuggingFace
Transformers</a></li>
</ul>
<div class="section level3">
<h3 id="linguistic-resources">Linguistic Resources<a class="anchor" aria-label="anchor" href="#linguistic-resources"></a>
</h3>
<p>There are also several curated resources (textual data) that can help
solve your NLP-related tasks, specifically when you need highly
specialized definitions. An exhaustive list would be impossible as there
are thousands of them, and also them being language and domain
dependent. Below we mention some of the most prominent, just to give you
an idea of the kind of resources you can find, so you don’t need to
reinvent the wheel every time you start a project:</p>
<ul>
<li>
<a href="https://huggingface.co/datasets" class="external-link">HuggingFace Datasets</a>:
A large collection of datasets for NLP tasks, including text
classification, question answering, and language modeling.</li>
<li>
<a href="https://wordnet.princeton.edu/" class="external-link">WordNet</a>: A large
lexical database of English, where words are grouped into sets of
synonyms (synsets) and linked by semantic relations.</li>
<li>
<a href="https://www.europarl.europa.eu/ep-search/search.do?language=en" class="external-link">Europarl</a>:
A parallel corpus of the proceedings of the European Parliament,
available in 21 languages, which can be used for machine translation and
cross-lingual NLP tasks.</li>
<li>
<a href="https://universaldependencies.org/" class="external-link">Universal
Dependencies</a>: A collection of syntactically annotated treebanks
across 100+ languages, providing a consistent annotation scheme for
syntactic and morphological properties of words, which can be used for
cross-lingual NLP tasks.</li>
<li>
<a href="https://propbank.github.io/" class="external-link">PropBank</a>: A corpus of
texts annotated with information about basic semantic propositions,
which can be used for English semantic tasks.</li>
<li>
<a href="https://framenet.icsi.berkeley.edu/fndrupal/" class="external-link">FrameNet</a>:
A lexical resource that provides information about the semantic frames
that underlie the meanings of words (mainly verbs and nouns), including
their roles and relations.</li>
<li>
<a href="https://babelnet.org/" class="external-link">BabelNet</a>: A multilingual lexical
resource that combines WordNet and Wikipedia, providing a large number
of concepts and their relations in multiple languages.</li>
<li>
<a href="https://www.wikidata.org/" class="external-link">Wikidata</a>: A free and open
knowledge base initially derived from Wikipedia, that contains
structured data about entities, their properties and relations, which
can be used to enrich NLP applications.</li>
<li>
<a href="https://github.com/allenai/dolma" class="external-link">Dolma</a>: An open
dataset of 3 trillion tokens from a diverse mix of clean web content,
academic publications, code, books, and encyclopedic materials, used to
train English large language models.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="relevant-linguistic-aspects">Relevant Linguistic Aspects<a class="anchor" aria-label="anchor" href="#relevant-linguistic-aspects"></a>
</h2>
<hr class="half-width">
<p>Natural language exhibits a set of properties that make it more
challenging to process than other types of data such as tables,
spreadsheets or time series. <strong>Language is hard to process because
it is compositional, ambiguous, discrete and sparse</strong>.</p>
<div class="section level3">
<h3 id="compositionality">Compositionality<a class="anchor" aria-label="anchor" href="#compositionality"></a>
</h3>
<p>The basic elements of written languages are characters, a sequence of
characters form words, and words in turn denote objects, concepts,
events, actions and ideas (Goldberg, 2016). Subsequently words form
phrases and sentences which are used in communication and depend on the
context in which they are used. We as humans derive the meaning of
utterances from interpreting contextual information that is present at
different levels at the same time:</p>
<figure><img src="fig/intro2_levels_lang.svg" width="573" alt="Levels of Language" class="figure mx-auto d-block"><div class="figcaption">Levels of Language</div>
</figure><p>The first two levels refer to spoken language only, and the other
four levels are present in both speech and text. Because in principle
machines do not have access to the same levels of information that we do
(they can only have independent audio, textual or visual inputs), we
need to come up with clever methods to overcome this significant
limitation. Knowing the levels of language is important so we consider
what kind of problems we are facing when attempting to solve our NLP
task at hand.</p>
</div>
<div class="section level3">
<h3 id="ambiguity">Ambiguity<a class="anchor" aria-label="anchor" href="#ambiguity"></a>
</h3>
<p>The disambiguation of meaning is usually a by-product of the context
in which utterances are expressed and also the historic accumulation of
interactions which are transmitted across generations (think for
instance to idioms – these are usually meaningless phrases that acquire
meaning only if situated within their historical and societal context).
These characteristics make NLP a particularly challenging field to work
in.</p>
<p>We cannot expect a machine to process human language and simply
understand it as it is. We need a systematic, scientific approach to
deal with it. It’s within this premise that the field of NLP is born,
primarily interested in converting the building blocks of human/natural
language into something that a machine can understand.</p>
<p>The image below shows how the levels of language relate to a few NLP
applications:</p>
<figure><img src="fig/intro3_levels_nlp.png" alt="Diagram showing building blocks of language" class="figure mx-auto d-block"><div class="figcaption">Diagram showing building blocks of
language</div>
</figure><div id="levels-of-ambiguity" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="levels-of-ambiguity" class="callout-inner">
<h3 class="callout-title">Levels of ambiguity</h3>
<div class="callout-content">
<p>Discuss what the following sentences mean. What level of ambiguity do
they represent?:</p>
<ul>
<li>“The door is unlockable from the inside.” vs “Unfortunately, the
cabinet is unlockable, so we can’t secure it”</li>
<li>“I saw the <em>cat with the stripes</em>” vs “I saw the cat <em>with
the telescope</em>”</li>
<li>“Colorless green ideas sleep furiously”</li>
<li>“I never said she stole my money.” (re-write this sentence multiple
times and each time write a different word in italics).</li>
</ul>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3"> Show me the solution </h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" aria-labelledby="headingSolution3" data-bs-parent="#accordionSolution3">
<div class="accordion-body">
<p>This is why the previous statements were difficult:</p>
<ul>
<li>“Un-lockable vs Unlock-able” is a <strong>Morphological</strong>
ambiguity: Same word form, two possible meanings</li>
<li>“I saw the cat with the telescope” is a <strong>Syntactic</strong>
ambiguity: Same sentence structure, different properties</li>
<li>“Colorless green ideas sleep furiously” <strong>Semantic</strong>
ambiguity: Grammatical but meaningless (ideas do not have color as a
property. Even if this was true, they would be either colorless or
green)</li>
<li>“I NEVER said she stole MY money.” is a <strong>Pragmatic</strong>
ambiguity: Meaning relies on word emphasis</li>
</ul>
</div>
</div>
</div>
</div>
<p>Whenever you are solving a specific task, you should ask yourself
what kind of ambiguity can affect your results, and to what degrees? At
what level are your assumptions operating when defining your research
questions? Having the answers to this can save you a lot of time when
debugging your models. Sometimes the most innocent assumptions (for
example using the wrong tokenizer) can create enormous performance drops
even when the higher level assumptions were correct.</p>
</div>
<div class="section level3">
<h3 id="sparsity">Sparsity<a class="anchor" aria-label="anchor" href="#sparsity"></a>
</h3>
<p>Another key property of linguistic data is its sparsity. This means
that if we are hunting for a specific phenomenon, we may often realize
it barely occurs inside a vast amount of text. Imagine we have the
following brief text and we are interested in <em>pizzas</em> and
<em>hamburgers</em>:</p>
<div class="codewrapper sourceCode" id="cb20">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" tabindex="-1"></a><span class="co"># A mini-corpus where our target words appear</span></span>
<span id="cb20-2"><a href="#cb20-2" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"""</span></span>
<span id="cb20-3"><a href="#cb20-3" tabindex="-1"></a><span class="st">I am hungry. Should I eat delicious pizza?</span></span>
<span id="cb20-4"><a href="#cb20-4" tabindex="-1"></a><span class="st">Or maybe I should eat a juicy hamburger instead.</span></span>
<span id="cb20-5"><a href="#cb20-5" tabindex="-1"></a><span class="st">Many people like to eat pizza because is tasty, they think pizza is delicious as hell!</span></span>
<span id="cb20-6"><a href="#cb20-6" tabindex="-1"></a><span class="st">My friend prefers to eat a hamburger and I agree with him.</span></span>
<span id="cb20-7"><a href="#cb20-7" tabindex="-1"></a><span class="st">We will drive our car to the restaurant to get the succulent hamburger.</span></span>
<span id="cb20-8"><a href="#cb20-8" tabindex="-1"></a><span class="st">Right now, our cat sleeps on the mat so we won't take him.</span></span>
<span id="cb20-9"><a href="#cb20-9" tabindex="-1"></a><span class="st">I did not wash my car, but at least the car has gasoline.</span></span>
<span id="cb20-10"><a href="#cb20-10" tabindex="-1"></a><span class="st">Perhaps when we come back we will take out the cat for a walk.</span></span>
<span id="cb20-11"><a href="#cb20-11" tabindex="-1"></a><span class="st">The cat will be happy then.</span></span>
<span id="cb20-12"><a href="#cb20-12" tabindex="-1"></a><span class="st">"""</span></span></code></pre>
</div>
<p>We can first use spaCy to tokenize the text and do some direct word
count:</p>
<div class="codewrapper sourceCode" id="cb21">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" tabindex="-1"></a><span class="im">import</span> spacy</span>
<span id="cb21-2"><a href="#cb21-2" tabindex="-1"></a>nlp <span class="op">=</span> spacy.load(<span class="st">"en_core_web_sm"</span>)</span>
<span id="cb21-3"><a href="#cb21-3" tabindex="-1"></a></span>
<span id="cb21-4"><a href="#cb21-4" tabindex="-1"></a>doc <span class="op">=</span> nlp(text)</span>
<span id="cb21-5"><a href="#cb21-5" tabindex="-1"></a>words <span class="op">=</span> [token.lower_ <span class="cf">for</span> token <span class="kw">in</span> doc <span class="cf">if</span> token.is_alpha]  <span class="co"># Filter out punctuation and new lines</span></span>
<span id="cb21-6"><a href="#cb21-6" tabindex="-1"></a><span class="bu">print</span>(words)</span>
<span id="cb21-7"><a href="#cb21-7" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(words))</span></code></pre>
</div>
<p>We have in total 104 words, but we actually want to know how many
times each word appears. For that we use the Python Counter and then we
can visualize it inside a chart with matplotlib:</p>
<div class="codewrapper sourceCode" id="cb22">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> Counter</span>
<span id="cb22-2"><a href="#cb22-2" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb22-3"><a href="#cb22-3" tabindex="-1"></a></span>
<span id="cb22-4"><a href="#cb22-4" tabindex="-1"></a>word_count <span class="op">=</span> Counter(words).most_common()</span>
<span id="cb22-5"><a href="#cb22-5" tabindex="-1"></a>tokens <span class="op">=</span> [item[<span class="dv">0</span>] <span class="cf">for</span> item <span class="kw">in</span> word_count]</span>
<span id="cb22-6"><a href="#cb22-6" tabindex="-1"></a>frequencies <span class="op">=</span> [item[<span class="dv">1</span>] <span class="cf">for</span> item <span class="kw">in</span> word_count]</span>
<span id="cb22-7"><a href="#cb22-7" tabindex="-1"></a></span>
<span id="cb22-8"><a href="#cb22-8" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">18</span>, <span class="dv">6</span>))</span>
<span id="cb22-9"><a href="#cb22-9" tabindex="-1"></a>plt.bar(tokens, frequencies)</span>
<span id="cb22-10"><a href="#cb22-10" tabindex="-1"></a>plt.xticks(rotation<span class="op">=</span><span class="dv">90</span>)</span>
<span id="cb22-11"><a href="#cb22-11" tabindex="-1"></a>plt.show()</span></code></pre>
</div>
<p>This bar chart shows us several things about sparsity, even with such
a small text:</p>
<ul>
<li><p>The most common words are filler words such as “the”, “of”, “not”
etc. These are known as <strong>stopwords</strong> because such words by
themselves generally do not hold a lot of information about the meaning
of the piece of text.</p></li>
<li><p>The two concepts (hamburger and pizza) we are interested in,
appear only 3 times each, out of 104 words (comprising only ~3% of our
corpus). This number only goes lower as the corpus size
increases</p></li>
<li><p>There is a long tail in the distribution, where actually a lot of
meaningful words are located.</p></li>
</ul>
<div id="callout4" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div class="callout-inner">
<div class="callout-content">
<p>Sparsity is closely related to what is frequently called
<strong>domain-specific data</strong>. The discourse context in which
language is used varies importantly across disciplines (domains). Take
for example law texts and medical texts which are typically filled with
domain-specific jargon. We should expect the top part of the
distribution to contain mostly the same worda as they tend to be stop
words. But once we remove the stop words, the top of the distirbution
will contain very different content words. Also, the meaning of concepts
described in each domain might significantly differ. For example the
word “trial” refers to a procedure for examining evidence in court, but
in the medical domain this could refer to a clinical “trial” which is a
procedure to test the efficacy and safety of treatments on patients. For
this reason there are specialized models and corpora that model language
use in specific domains. The concept of fine-tuning a general purpose
model with domain-specific data is also popular, even when using
LLMs.</p>
</div>
</div>
</div>
<div id="stop-words" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="stop-words" class="callout-inner">
<h3 class="callout-title">Stop Words</h3>
<div class="callout-content">
<p><strong>Stop words</strong> are extremely frequent syntactic filler
words that do not provide relevant semantic information for our use
case. For some use cases it is better to ignore them in order to fight
the sparsity phenomenon. However, consider that in many other use cases
the syntactic information that stop words provide is crucial to solve
the task.</p>
<p>SpaCy has a pre-defined list of stopwords per language. To explicitly
load the English stop words we can do:</p>
<div class="codewrapper sourceCode" id="cb23">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" tabindex="-1"></a><span class="im">from</span> spacy.lang.en.stop_words <span class="im">import</span> STOP_WORDS</span>
<span id="cb23-2"><a href="#cb23-2" tabindex="-1"></a><span class="bu">print</span>(STOP_WORDS)  <span class="co"># a set of common stopwords</span></span>
<span id="cb23-3"><a href="#cb23-3" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(STOP_WORDS)) <span class="co"># There are 326 words considered in this list</span></span></code></pre>
</div>
<p>You can also manually extend the list of stop words if you are
interested in ignoring other unlisted terms that you encounter in your
data.</p>
<p>Alternatively, you can filter out stop words when iterating your
tokens (remember the spaCy token properties!) like this:</p>
<div class="codewrapper sourceCode" id="cb24">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" tabindex="-1"></a>doc <span class="op">=</span> nlp(text)</span>
<span id="cb24-2"><a href="#cb24-2" tabindex="-1"></a>content_words <span class="op">=</span> [token.text <span class="cf">for</span> token <span class="kw">in</span> doc <span class="cf">if</span> token.is_alpha <span class="kw">and</span> <span class="kw">not</span> token.is_stop]  <span class="co"># Filter out stop words and punctuation</span></span>
<span id="cb24-3"><a href="#cb24-3" tabindex="-1"></a><span class="bu">print</span>(content_words)</span></code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="discreteness">Discreteness<a class="anchor" aria-label="anchor" href="#discreteness"></a>
</h3>
<p>There is no inherent relationship between the form of a word and its
meaning. For this reason, by syntactic or lexical analysis alone, there
is no automatic way of knowing if two words are similar in meaning or
how they relate semantically to each other. For example, “car” and “cat”
appear to be very closely related at the morphological level, only one
letter needs to change to convert one word into the other. But the two
words represent concepts or entities in the world which are very
different. Conversely, “pizza” and “hamburger” look very different (they
only share one letter in common) but are more closely related
semantically, because they both refer to typical fast foods.</p>
<p>How can we automatically know that “pizza” and “hamburger” share more
semantic properties than “car” and “cat”? One way is by looking at the
<strong>context</strong> (neighboring words) of these words. This idea
is the principle behind <strong>distributional semantics</strong>, and
aims to look at the statistical properties of language, such as word
co-occurrences (what words are typically located nearby a given word in
a given corpus of text), to understand how words relate to each
other.</p>
<p>Let’s keep using the list of words from our mini corpus:</p>
<div class="codewrapper sourceCode" id="cb25">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" tabindex="-1"></a>words <span class="op">=</span> [token.lower_ <span class="cf">for</span> token <span class="kw">in</span> doc <span class="cf">if</span> token.is_alpha]</span></code></pre>
</div>
<p>Now we will create a dictionary where we accumulate the words that
appear around our words of interest. In this case we want to find out,
according to our corpus, the most frequent words that occur around
<em>pizza</em>, <em>hamburger</em>, <em>car</em> and <em>cat</em>:</p>
<div class="codewrapper sourceCode" id="cb26">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" tabindex="-1"></a>target_words <span class="op">=</span> [<span class="st">"pizza"</span>, <span class="st">"hamburger"</span>, <span class="st">"car"</span>, <span class="st">"cat"</span>] <span class="co"># words we want to analyze</span></span>
<span id="cb26-2"><a href="#cb26-2" tabindex="-1"></a>co_occurrence <span class="op">=</span> {word: [] <span class="cf">for</span> word <span class="kw">in</span> target_words}</span>
<span id="cb26-3"><a href="#cb26-3" tabindex="-1"></a>co_occurrence</span></code></pre>
</div>
<p>We iterate over each word in our corpus, collecting its surrounding
words within a defined window. A window consists of a set number of
words to the left and right of the target word, as determined by the
window_size parameter. For example, with <code>window_size = 3</code>, a
word <code>W</code> has a window of six neighboring words—three
preceding and three following—excluding <code>W</code> itself:</p>
<div class="codewrapper sourceCode" id="cb27">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" tabindex="-1"></a>window_size <span class="op">=</span> <span class="dv">3</span> <span class="co"># How many words to look at on each side</span></span>
<span id="cb27-2"><a href="#cb27-2" tabindex="-1"></a><span class="cf">for</span> i, word <span class="kw">in</span> <span class="bu">enumerate</span>(words):</span>
<span id="cb27-3"><a href="#cb27-3" tabindex="-1"></a>    <span class="co"># If the current word is one of our target words...</span></span>
<span id="cb27-4"><a href="#cb27-4" tabindex="-1"></a>    <span class="cf">if</span> word <span class="kw">in</span> target_words:</span>
<span id="cb27-5"><a href="#cb27-5" tabindex="-1"></a>        start <span class="op">=</span> <span class="bu">max</span>(<span class="dv">0</span>, i <span class="op">-</span> window_size) <span class="co"># get the start index of the window</span></span>
<span id="cb27-6"><a href="#cb27-6" tabindex="-1"></a>        end <span class="op">=</span> <span class="bu">min</span>(<span class="bu">len</span>(words), i <span class="op">+</span> <span class="dv">1</span> <span class="op">+</span> window_size) <span class="co"># get the end index of the window</span></span>
<span id="cb27-7"><a href="#cb27-7" tabindex="-1"></a>        context <span class="op">=</span> words[start:i] <span class="op">+</span> words[i<span class="op">+</span><span class="dv">1</span>:end]  <span class="co"># Exclude the target word itself</span></span>
<span id="cb27-8"><a href="#cb27-8" tabindex="-1"></a>        co_occurrence[word].extend(context)</span>
<span id="cb27-9"><a href="#cb27-9" tabindex="-1"></a></span>
<span id="cb27-10"><a href="#cb27-10" tabindex="-1"></a><span class="bu">print</span>(co_occurrence)</span></code></pre>
</div>
<p>We call the words that fall inside this window the
<code>context</code> of a target word. We can already see other
interesting related words in the context of each target word, but a lot
of non interesting stuff is in there. To obtain even nicer results, we
can delete the stop words from the context window before adding it to
the dictionary. You can define your own stop words, here we use the
STOP_WORDS list provided by spaCy:</p>
<div class="codewrapper sourceCode" id="cb28">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" tabindex="-1"></a><span class="im">from</span> spacy.lang.en.stop_words <span class="im">import</span> STOP_WORDS</span>
<span id="cb28-2"><a href="#cb28-2" tabindex="-1"></a></span>
<span id="cb28-3"><a href="#cb28-3" tabindex="-1"></a>co_occurrence <span class="op">=</span> {word: [] <span class="cf">for</span> word <span class="kw">in</span> target_words} <span class="co"># Empty the dictionary</span></span>
<span id="cb28-4"><a href="#cb28-4" tabindex="-1"></a></span>
<span id="cb28-5"><a href="#cb28-5" tabindex="-1"></a>window_size <span class="op">=</span> <span class="dv">3</span> <span class="co"># How many words to look at on each side</span></span>
<span id="cb28-6"><a href="#cb28-6" tabindex="-1"></a><span class="cf">for</span> i, word <span class="kw">in</span> <span class="bu">enumerate</span>(words):</span>
<span id="cb28-7"><a href="#cb28-7" tabindex="-1"></a>    <span class="co"># If the current word is one of our target words...</span></span>
<span id="cb28-8"><a href="#cb28-8" tabindex="-1"></a>    <span class="cf">if</span> word <span class="kw">in</span> target_words:</span>
<span id="cb28-9"><a href="#cb28-9" tabindex="-1"></a>        start <span class="op">=</span> <span class="bu">max</span>(<span class="dv">0</span>, i <span class="op">-</span> window_size) <span class="co"># get the start index of the window</span></span>
<span id="cb28-10"><a href="#cb28-10" tabindex="-1"></a>        end <span class="op">=</span> <span class="bu">min</span>(<span class="bu">len</span>(words), i <span class="op">+</span> <span class="dv">1</span> <span class="op">+</span> window_size) <span class="co"># get the end index of the window</span></span>
<span id="cb28-11"><a href="#cb28-11" tabindex="-1"></a>        context <span class="op">=</span> words[start:i] <span class="op">+</span> words[i<span class="op">+</span><span class="dv">1</span>:end]  <span class="co"># Exclude the target word itself</span></span>
<span id="cb28-12"><a href="#cb28-12" tabindex="-1"></a>        context <span class="op">=</span> [w <span class="cf">for</span> w <span class="kw">in</span> context <span class="cf">if</span> w <span class="kw">not</span> <span class="kw">in</span> STOP_WORDS] <span class="co"># Filter out stop words</span></span>
<span id="cb28-13"><a href="#cb28-13" tabindex="-1"></a>        co_occurrence[word].extend(context)</span>
<span id="cb28-14"><a href="#cb28-14" tabindex="-1"></a></span>
<span id="cb28-15"><a href="#cb28-15" tabindex="-1"></a><span class="bu">print</span>(co_occurrence)</span></code></pre>
</div>
<p>Our dictionary keys represent each word of interest, and the values
are a list of the words that occur within <em>window_size</em> distance
of the word. Now we use a Counter to get the most common items:</p>
<div class="codewrapper sourceCode" id="cb29">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" tabindex="-1"></a><span class="co"># Print the most common context words for each target word</span></span>
<span id="cb29-2"><a href="#cb29-2" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Contextual Fingerprints:</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb29-3"><a href="#cb29-3" tabindex="-1"></a><span class="cf">for</span> word, context_list <span class="kw">in</span> co_occurrence.items():</span>
<span id="cb29-4"><a href="#cb29-4" tabindex="-1"></a>    <span class="co"># We use Counter to get a frequency count of context words</span></span>
<span id="cb29-5"><a href="#cb29-5" tabindex="-1"></a>    fingerprint <span class="op">=</span> Counter(context_list).most_common(<span class="dv">5</span>)</span>
<span id="cb29-6"><a href="#cb29-6" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"'</span><span class="sc">{</span>word<span class="sc">}</span><span class="ss">': </span><span class="sc">{</span>fingerprint<span class="sc">}</span><span class="ss">"</span>)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Contextual Fingerprints:

'pizza': [('eat', 2), ('delicious', 2), ('tasty', 2), ('maybe', 1), ('like', 1)]
'hamburger': [('eat', 2), ('juicy', 1), ('instead', 1), ('people', 1), ('agree', 1)]
'car': [('drive', 1), ('restaurant', 1), ('wash', 1), ('gasoline', 1)]
'cat': [('walk', 2), ('right', 1), ('sleeps', 1), ('happy', 1)]</code></pre>
</div>
<p>As our mini experiment demonstrates, discreteness can be combatted
with statistical co-occurrence: words with similar meaning will occur
around similar concepts, giving us an idea of similarity that has
nothing to do with syntactic or lexical form of words. This is the core
idea behind most modern semantic representation models in NLP.</p>
<div id="your-first-nlp-script" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="your-first-nlp-script" class="callout-inner">
<h3 class="callout-title">Your first NLP Script</h3>
<div class="callout-content">
<p>Choose one book file: dracula or frankenstein. Use what you have
learned so far to count how many times the words “love” and “hate”
appear in the book. What does this tell you about sparsity?</p>
<p>Then replicate the word co-occurrence experiment using the book you
chose.</p>
<p>Pair with someone that chose a different book and compare the most
common words appearing around the two target terms. What can you
conclude from this?</p>
<p>To do this experiment you should:</p>
<ol style="list-style-type: decimal">
<li>Read the file and save it into a text variable</li>
<li>Use spaCy to load the text into a Doc object.</li>
<li>Iterate through the tokens in the document and keep all tokens that
are alphanumeric (use the token.is_alpha property), and are not
stopwords (use the property token.is_stop).</li>
<li>Lowercase all the tokens to merge the instances of “Love” and “love”
into a single one.</li>
<li>Iterate through the tokens and count how many of them are exactly
“love”</li>
<li>Iterate through the tokens and count how many of them are exactly
“hate”</li>
<li>You may use the following function to compute co-occurrence.
Experiment with the values for <code>window_size</code> or
<code>most_common_words</code> and observe how the results change.</li>
</ol>
<div class="codewrapper sourceCode" id="cb31">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" tabindex="-1"></a><span class="kw">def</span> populate_co_occurrence(words, target_words, window_size<span class="op">=</span><span class="dv">3</span>, most_common_words<span class="op">=</span><span class="dv">5</span>):</span>
<span id="cb31-2"><a href="#cb31-2" tabindex="-1"></a>    co_occurrence <span class="op">=</span> {word: [] <span class="cf">for</span> word <span class="kw">in</span> target_words}</span>
<span id="cb31-3"><a href="#cb31-3" tabindex="-1"></a>    <span class="cf">for</span> i, word <span class="kw">in</span> <span class="bu">enumerate</span>(words):</span>
<span id="cb31-4"><a href="#cb31-4" tabindex="-1"></a>        <span class="cf">if</span> word <span class="kw">in</span> target_words:</span>
<span id="cb31-5"><a href="#cb31-5" tabindex="-1"></a>            start <span class="op">=</span> <span class="bu">max</span>(<span class="dv">0</span>, i <span class="op">-</span> window_size)</span>
<span id="cb31-6"><a href="#cb31-6" tabindex="-1"></a>            end <span class="op">=</span> <span class="bu">min</span>(<span class="bu">len</span>(words), i <span class="op">+</span> <span class="dv">1</span> <span class="op">+</span> window_size)</span>
<span id="cb31-7"><a href="#cb31-7" tabindex="-1"></a>            context <span class="op">=</span> words[start:i] <span class="op">+</span> words[i<span class="op">+</span><span class="dv">1</span>:end]</span>
<span id="cb31-8"><a href="#cb31-8" tabindex="-1"></a>            context <span class="op">=</span> [w <span class="cf">for</span> w <span class="kw">in</span> context <span class="cf">if</span> w <span class="kw">not</span> <span class="kw">in</span> STOP_WORDS]</span>
<span id="cb31-9"><a href="#cb31-9" tabindex="-1"></a>            co_occurrence[word].extend(context)</span>
<span id="cb31-10"><a href="#cb31-10" tabindex="-1"></a>    <span class="co"># Print the most common context words for each target word</span></span>
<span id="cb31-11"><a href="#cb31-11" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Contextual Fingerprints:</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb31-12"><a href="#cb31-12" tabindex="-1"></a>    <span class="cf">for</span> word, context_list <span class="kw">in</span> co_occurrence.items():</span>
<span id="cb31-13"><a href="#cb31-13" tabindex="-1"></a>        fingerprint <span class="op">=</span> Counter(context_list).most_common(most_common_words)</span>
<span id="cb31-14"><a href="#cb31-14" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"'</span><span class="sc">{</span>word<span class="sc">}</span><span class="ss">': </span><span class="sc">{</span>fingerprint<span class="sc">}</span><span class="ss">"</span>)</span></code></pre>
</div>
</div>
</div>
</div>
<div id="accordionSolution4" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution4" aria-expanded="false" aria-controls="collapseSolution4">
  <h4 class="accordion-header" id="headingSolution4"> Show me the solution </h4>
</button>
<div id="collapseSolution4" class="accordion-collapse collapse" aria-labelledby="headingSolution4" data-bs-parent="#accordionSolution4">
<div class="accordion-body">
<p>Following our preprocessing procedure with the <em>frankenstein
book</em>, there are <strong>30,500 content words</strong>. The word
<strong>love appears 59 times</strong> and the word <strong>hate appears
only 9 times</strong>. These are 0.22% of the total words in the text.
Even though intuitively one might expect these words to appear quite
frequently in the text, in reality they occur only a handful of times.
Code:</p>
<div class="codewrapper sourceCode" id="cb32">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">"84_frankenstein_clean.txt"</span>) <span class="im">as</span> f:</span>
<span id="cb32-2"><a href="#cb32-2" tabindex="-1"></a>  text <span class="op">=</span> f.read()</span>
<span id="cb32-3"><a href="#cb32-3" tabindex="-1"></a></span>
<span id="cb32-4"><a href="#cb32-4" tabindex="-1"></a>doc <span class="op">=</span> nlp(text)  <span class="co"># Process the text with SpaCy</span></span>
<span id="cb32-5"><a href="#cb32-5" tabindex="-1"></a>words <span class="op">=</span> [token.lower_ <span class="cf">for</span> token <span class="kw">in</span> doc <span class="cf">if</span> token.is_alpha <span class="kw">and</span> <span class="kw">not</span> token.is_stop]</span>
<span id="cb32-6"><a href="#cb32-6" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Total Words:"</span>, <span class="bu">len</span>(words))</span>
<span id="cb32-7"><a href="#cb32-7" tabindex="-1"></a></span>
<span id="cb32-8"><a href="#cb32-8" tabindex="-1"></a>love_words <span class="op">=</span> [word <span class="cf">for</span> word <span class="kw">in</span> words <span class="cf">if</span> <span class="st">"love"</span> <span class="op">==</span> word]</span>
<span id="cb32-9"><a href="#cb32-9" tabindex="-1"></a>hate_words <span class="op">=</span> [word <span class="cf">for</span> word <span class="kw">in</span> words <span class="cf">if</span> <span class="st">"hate"</span> <span class="op">==</span> word]</span>
<span id="cb32-10"><a href="#cb32-10" tabindex="-1"></a></span>
<span id="cb32-11"><a href="#cb32-11" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Love and Hate percentage:"</span>, (<span class="bu">len</span>(love_words) <span class="op">+</span> <span class="bu">len</span>(hate_words)) <span class="op">/</span> <span class="bu">len</span>(words) <span class="op">*</span> <span class="dv">100</span>, <span class="st">"</span><span class="sc">% o</span><span class="st">f content words"</span>)</span></code></pre>
</div>
</div>
</div>
</div>
</div>
<p>What did we learn in this lesson?</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<span class="callout-header">Key Points</span>
<div class="callout-inner">
<div class="callout-content">
<ul>
<li><p>NLP is a subfield of Artificial Intelligence (AI) that, using the
help of Linguistics, deals with approaches to process, understand and
generate natural language</p></li>
<li><p>Linguistic Data has special properties that we should consider
when modeling our solutions</p></li>
<li><p>Key tasks include language modeling, text classification, token
classification and text generation</p></li>
<li><p>Deep learning has significantly advanced NLP, but the challenge
remains in processing the discrete and ambiguous nature of
language</p></li>
<li><p>The ultimate goal of NLP is to enable machines to understand and
process language as humans do</p></li>
</ul>
</div>
</div>
</div>
</div>
</section></section><section id="aio-02-preprocessing"><p>Content from <a href="02-preprocessing.html">From words to vectors</a></p>
<hr>
<p>Last updated on 2025-09-24 |

        <a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/edit/main/episodes/02-preprocessing.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How do I load text and do basic linguistic analysis?</li>
<li>Why do we need to prepare a text for training?</li>
<li>How do I use words as features in a machine learning model?</li>
<li>What is a word2vec model?</li>
<li>What properties do word embeddings have?</li>
<li>What insights can I get from word embeddings?</li>
<li>How do we train a word2vec model?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<p>After following this lesson, learners will be able to:</p>
<ul>
<li>Implement a basic NLP Pipeline</li>
<li>Build a Document-Term Matrix</li>
<li>Understand the concept of word embeddings</li>
<li>Use and Explore Word2Vec models</li>
<li>Use word vectors as features for a classifier</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<hr class="half-width">
<p>In the previous episode we emphasized how text is different from
structured datasets. Given the linguistic properties embedded in
unstructured text, we also learned how to use existing libraries such as
SpaCy for segmenting text and accessing basic linguistic properties.</p>
<p>We learned the different levels of language and that it is ambiugous,
compositional and discrete. Because of this, it is hard to know how
words relate with each other, therefore obtaining meaning from text
alone is possible only through proxies that can be quantified. We made
our first attempt to approach word meaning by using co-occurrences of
words in a fixed text window around specific target words of
interest.</p>
<p>In this episode, we will expand on this idea and continue working
with words as individual features of text. We will introduce the concept
of Term-Document matrix, one of the most basic techniques to use words
as features that represent the texts where they appear, which can be fed
directly it into Machine Learning classifiers.</p>
<p>We will then visit the distributional hypothesis, which the linguist
J.R. Firth, in the 1950s, summarized with the phrase: “You shall know a
word by the company it keeps”. Based on this hypothesis, Mikolov et.
al. decided to train neural networks on large amounts of text in order
to predict a word based on it’s surrounding context or viceversa, in the
famous Word2Vec model. We will learn how to use these models, and
understand how do they map discrete words into numerical vectors that
capture the semantic similarity of words in a continuous space. By
representing words with vectors, we can mathematically manipulate them
through vector arithmetic and exploit the similarity patterns that
emerge from a collection of texts. Finally, we will show how to train
your own Word2Vec models.</p>
</section><section><h2 class="section-heading" id="preprocessing-text">Preprocessing Text<a class="anchor" aria-label="anchor" href="#preprocessing-text"></a>
</h2>
<hr class="half-width">
<p>NLP models work by learning the statistical regularities within the
constituent parts of the language (i.e, letters, digits, words and
sentences) in a text. However, text contains also other type of
information that humans find useful to convey meaning. To signal pauses,
give emphasis and convey tone, for instance, we use punctuation.
Articles, conjunctions and prepositions also alter the meaning of a
sentence. The machine does not know the difference among all of these
linguistic units, as it treats them all as equal.</p>
<p>We have already done some basic data pre-processing in the
introduction. Here we will formalize this initial step and present some
of the most common pre-processing steps when dealing with structured
data. This is analogue to the data cleaning and sanitation step in any
Machine Learning task. In the case of linguistic data, we are interested
in getting rid of unwanted components (such as rare punctuation or
formatting characters) that can confuse a tokenizer and, depending on
the task at hand, we might also be interested in normalizing our tokens
to avoid possible noise in our final results. As we already know, an NLP
module such as SpaCy comes in handy to deal with the preprocessing of
text, here is the list of the recommended (always optional!) steps:</p>
<ul>
<li>
<strong>Tokenization:</strong> splitting strings into
meaningful/useful units. This step also includes a method for “mapping
back” the segments to their character position in the original
string.</li>
<li>
<strong>Lowercasing:</strong> removing uppercases to e.g. avoid
treating “Dog” and “dog” as two different words)</li>
<li>
<strong>Punctuation and Special Character Removal:</strong> if we
are interested in <em>content only,</em> we can filter out anything that
is not alphanumerical. We can also explicitly exlude symbols that are
just noise in our dataset. Note that getting rid of punctuation can
significantly change meaning! A special mention is that new lines are a
character in text, sometimes we can use them in our benefit (for example
to separate paragraphs) but many times they are just noise.</li>
<li>
<strong>Stop Word Removal:</strong> as we’ve seen, the most frequent
words in texts are those which contribute little semantic value on their
own: articles (‘the’, ‘a’ , ‘an’), conjunctions (‘and’, ‘or’, ‘but’),
prepositions (‘on’, ‘by’), auxiliary verbs (‘is’, ‘am’), pronouns (‘he’,
‘which’), or any highly frequent word that might not be of interest in
several <em>content only</em> related tasks. A special case is the word
‘not’ which carries the significant semantic value of negation.</li>
<li>
<strong>Lemmatization:</strong> although it has become less
frequent, normalizing words into their <em>dictionary form</em> can help
to focus on relevant aspects of text. Think how “eating”, “ate”, “eaten”
are all a variation of the verb “eat”.</li>
</ul>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a> <span class="co">"Here my python code using spacy... Here we load the 'Dirty Book of Frankenstein' and the task is to arrive to the clean tokens to train Word2Vec"</span></span></code></pre>
</div>
<div id="callout1" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div class="callout-inner">
<div class="callout-content">
<ul>
<li><p>Preprocessing approaches affect significantly the quality of the
training when working with word embeddings. For example, <a href="https://link.springer.com/article/10.1007/s10579-022-09620-5" class="external-link">Rahimi
&amp; Homayounpour (2022)</a> demonstrated that for text classification
and sentiment analysis, the removal of punctuation and stopwords leads
to higher performance.</p></li>
<li><p>You do not always need to do all the preprocessing steps, and
which ones you should do depends on what you want to do. For example, if
you want to segment text into sentences then characters such as ‘.’, ‘,’
or ‘?’ are the most important; if you want to extract Named Entities
from text, you explicitly do not want to lowercase the text, as capitals
are a component in the identification process, and if you are interested
in gender bias you definitely want to keep the pronouns, etc…</p></li>
<li><p>Preprocessing can be very diffent for different languages. This
is both in terms of which steps to apply, but also which methods to use
for a specific step.</p></li>
</ul>
</div>
</div>
</div>
<p>We will prepare the data for the two experiments in this episode 1.
Build a Term-Document Matrix 2. Train a Word2Vec model</p>
<p>For both taska we need to prepare our texts by applying the same
preprocessing steps. We are focusing on content words for now, so even
though our preprocessin will unfortunately loose a lot of the original
information, in exchange we will be able manipulate words as individual
numeric representations. Therefore the preprocessing includes: cleaning
the text, tokenizing, lowercasing words, removing punctuation,
lemmatizing words and removing stop words. Let’s apply this step by
step.</p>
<div class="section level3">
<h3 id="cleaning-the-text">1. Cleaning the text<a class="anchor" aria-label="anchor" href="#cleaning-the-text"></a>
</h3>
<p>We start by importing the <code>spaCy</code> library that will help
us go through the preprocessing steps. SpaCy is a popular open-source
library for NLP in Python and it works with pre-trained languages models
that we can load and use to process and analyse the text efficiently. We
can then load the SpaCy model into the pipeline function.</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="im">import</span> spacy</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>doc <span class="op">=</span> nlp(corpus)</span></code></pre>
</div>
<p>Next, we’ll eliminate the triple dashes that separate different news
articles, as well as the vertical bars used to divide some columns.</p>
</div>
<div class="section level3">
<h3 id="tokenizing">2. Tokenizing<a class="anchor" aria-label="anchor" href="#tokenizing"></a>
</h3>
<p>Tokenization is essential in NLP, as it helps to create structure
from raw text. It involves the segmentation of the text into smaller
units referred as <code>tokens</code>. Tokens can be sentences
(e.g. <code>'the happy cat'</code>), words
(<code>'the', 'happy', 'cat'</code>), subwords
(<code>'un', 'happiness'</code>) or characters
(<code>'c','a', 't'</code>). The choice of tokens depends by the
requirement of the model used for training, and the text. This step is
carried out by a pre-trained model (called tokeniser) that has been
fine-tuned for the target language. In our case, this is
<code>en_core_web_sm</code> loaded before.</p>
<div id="callout2" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div class="callout-inner">
<div class="callout-content">
<p>A good word tokeniser for example, does not simply break up a text
based on spaces and punctuation, but it should be able to
distinguish:</p>
<ul>
<li>abbreviations that include points (e.g.: <em>e.g.</em>)</li>
<li>times (<em>11:15</em>) and dates written in various formats
(<em>01/01/2024</em> or <em>01-01-2024</em>)</li>
<li>word contractions such as <em>don’t</em>, these should be split into
<em>do</em> and <em>n’t</em>
</li>
<li>URLs</li>
</ul>
<p>Many older tokenisers are rule-based, meaning that they iterate over
a number of predefined rules to split the text into tokens, which is
useful for splitting text into word tokens for example. Modern large
language models use subword tokenisation, which are more flexible.</p>
</div>
</div>
</div>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a>spacy_corpus <span class="op">=</span> nlp(corpus_clean)</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a><span class="co"># Get the tokens from the pipeline</span></span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>tokens <span class="op">=</span> [token.text <span class="cf">for</span> token <span class="kw">in</span> spacy_corpus]</span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a>tokens[:<span class="dv">10</span>]</span></code></pre>
</div>
<p><code>['mens', 'op', 'maan', '\n ', '„', 'de', 'eagle', 'is', 'geland', '”']</code></p>
<p>As one can see the tokeniser has split each word in a token, however
it has considered also blank spaces <code>\n</code> and also
punctuation.</p>
</div>
<div class="section level3">
<h3 id="lowercasing">3. Lowercasing<a class="anchor" aria-label="anchor" href="#lowercasing"></a>
</h3>
<p>Our next step is to lowercase the text. Our goal here is to generate
a list of unique words from the text, so in order to not have words
twice in the list - once normal and once capitalised when it is at the
start of a sentence for example - we can lowercase the full text.</p>
<p>corpus_lower = corpus_clean.lower()</p>
<p>print(corpus_lower)</p>
<p>mens op maan „ de eagle is geland ” reisduur : 102 uur , uitstappen
binnen 20 iuli , 21.17 uur 45 […]</p>
</div>
<div class="section level3">
<h3 id="remove-punctuation">4. Remove punctuation<a class="anchor" aria-label="anchor" href="#remove-punctuation"></a>
</h3>
<p>The next step we will apply is to remove punctuation. We are
interested in training our model to learn the meaning of the words. This
task is highly influenced by the state of our text and punctuation would
decrease the quality of the learning as it would add spurious
information. We’ll see how the learning process works later in the
episode.</p>
<p>The punctuation symbols are defined in:</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="im">import</span> string</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>string.punctuation</span></code></pre>
</div>
<p>We can loop over these symbols to remove them from the text:</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="co"># remove punctuation from set</span></span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>tokens_no_punct <span class="op">=</span> [token <span class="cf">for</span> token <span class="kw">in</span> tokens <span class="cf">if</span> token <span class="kw">not</span> <span class="kw">in</span> string.punctuation]</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a><span class="co"># remove also blank spaces</span></span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a>tokens_no_punct <span class="op">=</span> [token <span class="cf">for</span> token <span class="kw">in</span> tokens_no_punct <span class="cf">if</span> token.strip() <span class="op">!=</span> <span class="st">''</span>]</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="bu">print</span>(tokens_no_punct[:<span class="dv">10</span>])</span></code></pre>
</div>
<p><code>['mens', 'op', 'maan', 'de', 'eagle', 'is', 'geland', 'reisduur', '102', 'uur']</code></p>
</div>
<div class="section level3">
<h3 id="stop-word-removal">5. Stop word removal<a class="anchor" aria-label="anchor" href="#stop-word-removal"></a>
</h3>
<p>For some NLP tasks only the important words in the text are needed. A
text however often contains many <code>stop words</code>: common words
such as <code>de</code>, <code>het</code>, <code>een</code> that add
little meaningful content compared to nouns and words. In those cases,
it is best to remove stop words from your corpus to reduce the number of
words to process.</p>
</div>
</section><section><h2 class="section-heading" id="term-document-matrix">Term-Document Matrix<a class="anchor" aria-label="anchor" href="#term-document-matrix"></a>
</h2>
<hr class="half-width">
<p>A Term-Document Matrix (TDM) is a matrix where:</p>
<ul>
<li>Each <strong>row</strong> is a unique word (term) in the corpus</li>
<li>Each <strong>column</strong> is a document in the corpus</li>
<li>Each <strong>cell</strong> <span class="math inline">\((i,j)\)</span> has a value of 1 if the <span class="math inline">\(term_i\)</span> appears in <span class="math inline">\(column_j\)</span> or 0 otherwise</li>
</ul>
<p>This is also sometimes known as a bag-of-words as it ignores grammar
and word sequences in exchange of emphazising content, where each
document is characterized by the words that appear in it. Similar
documents will contain similar bags of words and documents that talk
about different topics will be associated with numerical columns that
are different from each other. Let’s look at a quick example:</p>
<ul>
<li>Doc 1: “Natural language processing is exciting”</li>
<li>Doc 2: “Processing natural language helps computers understand”</li>
<li>Doc 3: “Language processing with computers is NLP”</li>
<li>Doc 4: “Today it rained a lot”</li>
</ul>
<table class="table">
<thead><tr class="header">
<th>Term</th>
<th>Doc1</th>
<th>Doc2</th>
<th>Doc3</th>
<th>Doc4</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>natural</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>language</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="odd">
<td>processing</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="even">
<td>is</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="odd">
<td>exciting</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>helps</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="odd">
<td>computers</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="even">
<td>understand</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="odd">
<td>with</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="even">
<td>NLP</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="odd">
<td>today</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr class="even">
<td>it</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr class="odd">
<td>rained</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr class="even">
<td>a</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr class="odd">
<td>lot</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>We can represent each document by taking its column and treating it
as a vector of 0’s and 1’s. The vector is of fixed size (in this case
the vocabulary size is 15), therefore suitable for traditional ML
classifiers. With TDM there is a problem of scalability, as the matrix
size grows with the amount of documents times the vocabulary found in
the documents we are processing. This means that if we have 100
documents in which 5,000 unique words appear, we would have to store a
matrix of 500,000 numbers! We also have the problem of sparsity present:
document “vectors” will have mostly 0’s. TDM is also a good solution to
characterize documents based on their vocabulary, however the converse
is even more desirable: to characterize words based on the context where
they appear, so we can study words independently of their documents of
origin, and more importantly, how do they relate to each other. To solve
these and other limitations we enter the world of word embeddings!</p>
</section><section><h2 class="section-heading" id="what-are-word-embeddings">What are word embeddings?<a class="anchor" aria-label="anchor" href="#what-are-word-embeddings"></a>
</h2>
<hr class="half-width">
<p>A Word Embedding is a word representation type that maps words in a
numerical manner (i.e., into vectors) in a multidimensional space,
capturing their meaning based on characteristics or context. Since
similar words occur in similar contexts, or have same characteristics, a
properly trained model will learn to assign similar vectors to similar
words.</p>
<p>Let’s illustrate this concept using animals. This example will show
us an intuitive way of representing things into vectors.</p>
<p>Suppose we want to represent a <code>cat</code> using measurable
characteristics:</p>
<ul>
<li>Furriness: Let’s assign a score of 70 to a cat</li>
<li>Number of legs: A cat has 4 legs</li>
</ul>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a>cat <span class="op">=</span> np.array([[<span class="dv">70</span>, <span class="dv">4</span>]])</span></code></pre>
</div>
<p>So the vector representation of a cat becomes:
<code>[70 (furriness), 4 (legs)]</code></p>
<figure><img src="fig/emb3.png" class="figure mx-auto d-block"></figure><p>This vector doesn’t fully describe a cat but provides a basis for
comparison with other animals.</p>
<p>Let’s add vectors for a dog and a caterpillar:</p>
<ul>
<li>Dog: [56, 4]</li>
<li>Caterpillar: [70, 100]</li>
</ul>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a></span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a>dog <span class="op">=</span> np.array([[<span class="dv">56</span>, <span class="dv">4</span>]])</span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a>caterpillar <span class="op">=</span> np.array([[<span class="dv">70</span>, <span class="dv">100</span>]])</span></code></pre>
</div>
<figure><img src="fig/emb5.png" class="figure mx-auto d-block"></figure><p>To determine which animal is more similar to a cat, we use
<code>cosine similarity</code>, which measures the cosine of the angle
between two vectors.</p>
<div id="callout3" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div class="callout-inner">
<div class="callout-content">
<p><a href="https://en.wikipedia.org/wiki/Cosine_similarity" class="external-link">cosine
similarity</a> ranges between [<code>-1</code> and <code>1</code>]. It
is the cosine of the angle between two vectors, divided by the product
of their length. It is a useful metric to measure how similar two
vectors are likely to be.</p>
<figure><img src="fig/emb12.png" alt="" class="figure mx-auto d-block"></figure>
</div>
</div>
</div>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="im">from</span> sklearn.metrics.pairwise <span class="im">import</span> cosine_similarity</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a>similarity_cat_dog <span class="op">=</span> cosine_similarity(cat, dog)[<span class="dv">0</span>][<span class="dv">0</span>]</span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a>similarity_cat_caterpillar <span class="op">=</span> cosine_similarity(cat, caterpillar)[<span class="dv">0</span>][<span class="dv">0</span>]</span>
<span id="cb9-5"><a href="#cb9-5" tabindex="-1"></a></span>
<span id="cb9-6"><a href="#cb9-6" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Cosine similarity between cat and dog: </span><span class="sc">{</span>similarity_cat_dog<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb9-7"><a href="#cb9-7" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Cosine similarity between cat and caterpillar: </span><span class="sc">{</span>similarity_cat_caterpillar<span class="sc">}</span><span class="ss">"</span>)</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a></span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a>Cosine similarity between cat <span class="kw">and</span> dog: <span class="fl">0.9998987965747193</span></span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a>Cosine similarity between cat <span class="kw">and</span> caterpillar: <span class="fl">0.6192653797321375</span></span></code></pre>
</div>
<p>The higher similarity score between the cat and the dog indicates
they are more similar based on these characteristics. Adding more
characteristics can enrich our vectors, detecting more semantic
nuances.</p>
<figure><img src="fig/emb6.png" class="figure mx-auto d-block"></figure><p>By representing words as vectors with multiple dimensions, we capture
more nuances of their meanings or characteristics.</p>
</section><section><h2 class="section-heading" id="explore-the-word2vec-vector-space">Explore the Word2Vec Vector Space<a class="anchor" aria-label="anchor" href="#explore-the-word2vec-vector-space"></a>
</h2>
<hr class="half-width">
<p>There are two main architectures for training Word2Vec:</p>
<ul>
<li>Continuous Bag-of-Words (CBOW): Predicts a target word based on its
surrounding context words.</li>
<li>Continuous Skip-Gram: Predicts surrounding context words given a
target word.</li>
</ul>
<figure><img src="fig/emb13.png" class="figure mx-auto d-block"></figure><div id="callout4" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div class="callout-inner">
<div class="callout-content">
<p>CBOW is faster to train, while Skip-Gram is more effective for
infrequent words. Increasing context size improves embeddings but
increases training time.</p>
</div>
</div>
</div>
<p>We will be using CBOW. We are interested in having vectors with 300
dimensions and a context size of 5 surrounding words. We include all
words present in the corpora, regardless of their frequency of
occurrence and use 4 CPU cores for training. All these specifics are
translated in only one line of code.</p>
<p>We can inspect already what’s the output of this training, by
checking the top 5 most similar words to “maan” (moon):</p>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a>word_vectors.most_similar(<span class="st">'maan'</span>, topn<span class="op">=</span><span class="dv">5</span>)</span></code></pre>
</div>
<p><code>[('plek', 0.48467501997947693), ('ouders', 0.46935707330703735), ('supe|', 0.3929591178894043), ('rotterdam', 0.37788015604019165), ('verkeerden', 0.33672046661376953)]</code></p>
<div class="section level3">
<h3 id="load-the-embeddings-and-inspect-them">Load the embeddings and inspect them<a class="anchor" aria-label="anchor" href="#load-the-embeddings-and-inspect-them"></a>
</h3>
<p>We proceed to load our models. We will load all pre-trained model
files from the original Word2Vec paper, which was trained on a big
corpus from Google News. The library <code>gensim</code> contains a
method called <code>KeyedVectors</code> which allows us to load
them.</p>
<p>Put here the cod eform the notebook… with simple w2v operations, load
existing vectors, test analogy, load neoghbors etc…</p>
</div>
<div class="section level3">
<h3 id="use-word2vec-vectors-as-features-for-a-classifier">Use Word2Vec vectors as features for a classifier<a class="anchor" aria-label="anchor" href="#use-word2vec-vectors-as-features-for-a-classifier"></a>
</h3>
<p>TODO: Here step-bystep a very simple sentiment logistic regression
classifier using word2vec as input. Maybe this is too advanced ???</p>
<div id="dataset-size-in-training" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="dataset-size-in-training" class="callout-inner">
<h3 class="callout-title">Dataset size in training</h3>
<div class="callout-content">
<p>To obtain your own high-quality embeddings, the size/length of the
training dataset plays a crucial role. Generally <a href="https://cs.stanford.edu/~quocle/paragraph_vector.pdf" class="external-link">tens of
thousands of documents</a> are considered a reasonable amount of data
for decent results.</p>
<p>Is there however a strict minimum? Not really. Things to keep in mind
is that <code>vocabulary size</code>, <code>document length</code> and
<code>desired vector size</code> interacts with each other. The higher
the dimensional vectors (e.g. 200-300 dimensions) the more data is
required, and of high quality, i.e. that allows the learning of words in
a variety of contexts.</p>
<p>While word2vec models typically perform better with large datasets
containing millions of words, using a single page is sufficient for
demonstration and learning purposes. This smaller dataset allows us to
train the model quickly and understand how word2vec works without the
need for extensive computational resources.</p>
</div>
</div>
</div>
<div id="train-your-own-word2vec-model" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<span class="callout-header">Discussion</span>
<div id="train-your-own-word2vec-model" class="callout-inner">
<h3 class="callout-title">Train your own Word2Vec model</h3>
<div class="callout-content">
<ol style="list-style-type: decimal">
<li><p>Load the necessary libraries. See the Gensim <a href="https://radimrehurek.com/gensim/models/word2vec.html" class="external-link">documentation</a></p></li>
<li><p>Prepare the data (preprocessing pipeline)</p></li>
<li><p>Train your model using the <code>Word2Vec</code> object.</p></li>
<li><p>Save your trained model using ::: solution</p></li>
<li><p>Import the necessary libraries:</p></li>
</ol>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a><span class="im">import</span> gensim</span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a><span class="im">from</span> gensim.models <span class="im">import</span> Word2Vec</span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a></span></code></pre>
</div>
<ol start="2" style="list-style-type: decimal">
<li><p>Prepare the data</p></li>
<li><p>Train your own model then:</p></li>
</ol>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a>model <span class="op">=</span> Word2Vec([tokens_no_stopwords], vector_size<span class="op">=</span><span class="dv">300</span>, window<span class="op">=</span><span class="dv">5</span>, min_count<span class="op">=</span><span class="dv">1</span>, workers<span class="op">=</span><span class="dv">4</span>, sg<span class="op">=</span><span class="dv">0</span>)</span></code></pre>
</div>
</div>
</div>
</div>
<p>::::</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<span class="callout-header">Key Points</span>
<div class="callout-inner">
<div class="callout-content">
<ul>
<li>The first step for working with text is to run a preprocessing
pipeline to obtain clear features</li>
<li>We can represent text as vectors of numbers (which makes it
interpretable for machines)</li>
<li>One of the most efficient and useful ways is to use word
embeddings</li>
<li>We can easily compute how words are similar to each other with the
cosine similarity</li>
</ul>
</div>
</div>
</div>
</div>
</section></section><section id="aio-03-transformers"><p>Content from <a href="03-transformers.html">Transformers: BERT and Beyond</a></p>
<hr>
<p>Last updated on 2025-09-24 |

        <a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/edit/main/episodes/03-transformers.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What are some drawbacks of static word embeddings?</li>
<li>What are Transformers?</li>
<li>What is BERT and how does it work?</li>
<li>How can I use BERT to solve NLP tasks?</li>
<li>How should I evaluate my classifiers?</li>
<li>Which other Transformer variants are available?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Understand how a Transformer works and recognize their different use
cases.</li>
<li>Understand how to use pre-trained tranfromers (Use Case: BERT)</li>
<li>Use BERT to classify texts.</li>
<li>Use BERT as a Named Entity Recognizer.</li>
<li>Understand assumptions and basic evaluation for NLP outputs.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<p>Static word embeddings such as Word2Vec can be used to represent each
word as a unique vectors. Vector representations also allow us to apply
numerical operations that can be mapped to some syntactic and semantic
properties of words, such as the cases of analogies or finding synonyms.
Once we transform words into vectors, these can also be used as
<strong>features</strong> for classifiers that can be trained predict
any supervised NLP task.</p>
<p>However, a big drawback of Word2Vec is that <strong>each word is
represented in isolation</strong>, and unfortunately that is not how
language works. Words get their meanings based on the specific context
in which they are used (take for example polysemy, the cases where the
same word can have very different meanings depending on the context);
therefore, we would like to have richer vector representations of words
that also integrate context into account in order to obtain more
powerful representations.</p>
<div id="polysemy-in-language" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="polysemy-in-language" class="callout-inner">
<h3 class="callout-title">Polysemy in Language</h3>
<div class="callout-content">
<p>Think of (at least 2) different words that can have more than one
meaning depending on the context. Come up with one simple sentence per
meaning and explain what they mean in each context. Discuss: How do you
know what of the possible meanings does the word have when you use
it?</p>
<p>OPTIONAL: Why do you think Word2Vec can’t caputure different meanings
of words?</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<p>Two possible examples can be the words ‘fine’ and ‘run’</p>
<p>Sentences for ‘fine’: - She has a fine watch (fine == high-quality) -
He had to pay a fine (fine == penalty) - I am feeling fine (fine == not
bad)</p>
<p>Sentences for ‘run’: - I had to run to catch the bus (run == moving
fast) - Stop talking, before you run out of ideas (run (out) ==
exhaust)</p>
<p>Note how in the “run out” example we even have to understand that the
meaning of run is not literal but goes accompained with a preposition
that changes its meaning.</p>
</div>
</div>
</div>
</div>
<p>In 2019, the BERT language model was introduced. Using a novel
architecture called Transformer (2017), BERT can integrate context into
word representations. To understand BERT, we will first look at what a
transformer is and we will then directly use some code to make use of
BERT.</p>
<div class="section level1">
<h1 id="transformers">Transformers<a class="anchor" aria-label="anchor" href="#transformers"></a>
</h1>
<p>The Transformer is a neural network architecture proposed by Google
researchers <a href="https://arxiv.org/pdf/1706.03762" class="external-link">in 2017</a> in a
paper called <em>Attention is all you Need</em>. They tackled
specifically the NLP task of Machine Translation (MT), which is stated
as: how to generate a sentence (sequence of words) in target language B
given a sentence in source language A? We all know that translation
cannot be done word by word in isolations, therefore integrating the
context from both the source language and the target language is
necessary. In order to translate, first one neural network needs to
<em>encode</em> the whole meaning of the senetence in language A into a
single vector representation, then a second neural network needs to
<em>decode</em> that representation into tokens that are both coherent
with the meaning of language A and understandable in language B.
Therefore we say that translation is modeling language B
<em>conditioned</em> on what language A originally said.</p>
<figure><img src="fig/trans1.png" alt="Transformer Architecture" class="figure mx-auto d-block"><div class="figcaption">Transformer Architecture</div>
</figure><p>As seen in the picture, the original Transformer is an
Encoder-Decoder network that tackles translation. We first need a token
embedder which converts the string of words into a sequence of vectors
that the Transformer network can process. The first component, the
<strong>Encoder</strong>, is optimized for creating <strong>rich
representations</strong> of the source sequence (in this case an English
sentence) while the second one, the <strong>Decoder</strong> is a
<strong>generative network</strong> that is conditioned on the encoded
representation. The third component we see is the infamous attention
mechanism, a third neural network what computes the correlation between
source and target tokens (<em>Which word in Dutch should I pay attention
to decide a better next English word?</em>) to generate the most likely
token in the target sequence (in this case Dutch words).</p>
<div id="emulate-the-attention-mechanism" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<span class="callout-header">Challenge</span>
<div id="emulate-the-attention-mechanism" class="callout-inner">
<h3 class="callout-title">Emulate the Attention Mechanism</h3>
<div class="callout-content">
<p>Pair with a person who speaks a language different from English (we
will cal it language B). This time you should think of 2 simple
sentences in English and come up with their translations in the second
language. In a piece of paper write down both sentences (one on top of
the other) and try to: 1. Draw a one to one mapping of words in English
to language B. Is it always possible to do this? 2. Think of each word
in language B and draw as many lines as necessary to the relevant
English words that can “help you” predict the word in language B. If you
managed, congratulations, this is how attention works!</p>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2"> Show me the solution </h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" data-bs-parent="#accordionSolution2" aria-labelledby="headingSolution2">
<div class="accordion-body">
<p>Here an image of a bilingual “manual attention” example</p>
</div>
</div>
</div>
</div>
<p>Next, we will see how BERT exploits the idea of a <strong>Transformer
Encoder</strong> to perform the NLP Task we are interested in:
generating powerful word representations.</p>
</div>
<div class="section level1">
<h1 id="bert">BERT<a class="anchor" aria-label="anchor" href="#bert"></a>
</h1>
<p><a href="https://aclanthology.org/N19-1423.pdf" class="external-link">BERT</a> is an
acronym that stands for <strong>B</strong>idirectional
<strong>E</strong>ncoder <strong>R</strong>epresentations from
<strong>T</strong>ransformers. The name describes it all: the idea is to
use the power of the Encoder component of the Transformer architecture
to create powerful token representations that preserve the contextual
meaning of the whole input segment, instead of each word in isolation.
The BERT vector representations of each token take into account both the
left context (what comes before the word) and the right context (what
comes after the word). Another advantage of the transformer Encoder is
that it is parallelizable, which made it posible for the first time to
train these networks on millions of datapoints, dramatically improving
model generalization.</p>
<div id="pretraining-bert" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div id="pretraining-bert" class="callout-inner">
<h3 class="callout-title">Pretraining BERT</h3>
<div class="callout-content">
<p>To obtain the BERT vector representations the Encoder is pre-trained
with two different tasks: - <strong>Masked Language Model:</strong> for
each sentence, mask one token at a time and predict which token is
missing based on the context from both sides. A training input example
would be “Maria [MASK] Groningen” and the model should predict the word
“loves”. - <strong>Next Sentence Prediction:</strong> the Encoder gets a
linear binary classifier on top, which is trained to decide for each
pair of sequences A and B, if sequence A precedes sequence B in a text.
For the sentence pair: “Maria loves Groningen.” and “This is a city in
the Netherlands.” the output of the classifier is “True” and for the
pair “Maria loves Groningen.” and “It was a tasty cake.” the output
should be “false” as there is no obvious continuation between the two
sentences.</p>
<p>Already the second pre-training task gives us an idea of the power of
BERT: after it has been pretrained on hundreds of thousands of texts,
one can plug-in a classifier on top and re-use the <em>linguistic</em>
knowledge previously acquired to fine-tune it for a specific task,
without needing to learn the weights of the whole network from scratch
all over again. In the next sections we will describe the components of
BERT and show how to use it. This model and hundreds of related
transformer-based pre-trained encoders can also be found on <a href="https://huggingface.co/google-bert/bert-base-cased" class="external-link">Hugging
Face</a>.</p>
</div>
</div>
</div>
</div>
<div class="section level1">
<h1 id="bert-architecture">BERT Architecture<a class="anchor" aria-label="anchor" href="#bert-architecture"></a>
</h1>
<p>The BERT Architecture can be seen as a basic NLP pipeline on its
own:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Tokenizer:</strong> splits text into tokens that the model
recognizes</li>
<li>
<strong>Embedder:</strong> converts each token into a fixed-sized
vector that represents it. These vectors are the actual input for the
Encoder.</li>
<li>
<strong>Encoder</strong> several neural layers that model the
token-level interactions of the input sequence to enhance meaning
representation. The output of the encoder is a set of
<strong>H</strong>idden layers, the vector representation of the
ingested sequence.</li>
<li>
<strong>Output Layer:</strong> the final encoder layer (which we
depict as a sequence <strong>H</strong>’s in the figure) contains
arguably the best token-level representations that encode syntactic and
semantic properties of each token, but this time each vector is already
contextualized with the specific sequence.</li>
<li>
<em>OPTIONAL</em> <strong>Classifier Layer:</strong> an additional
classifier can be connected on top of the BERT token vectors which are
used as features for performing a downstream task. This can be used to
classify at the text level, for example sentiment analysis of a
sentence, or at the token-level, for example Named Entity
Recognition.</li>
</ol>
<figure><img src="fig/bert3.png" alt="BERT Architecture" class="figure mx-auto d-block"><div class="figcaption">BERT Architecture</div>
</figure><p>BERT uses (self-) attention, which is very useful to capture
longer-range word dependencies such as correference, where, for example,
a pronoun can be linked to the noun it refers to previously in the same
sentence. See the following example:</p>
<figure><img src="fig/trans5.png" alt="The Encoder Self-Attention Mechanism" class="figure mx-auto d-block"><div class="figcaption">The Encoder Self-Attention Mechanism</div>
</figure><div class="section level2">
<h2 id="bert-for-word-based-analysis">BERT for Word-Based Analysis<a class="anchor" aria-label="anchor" href="#bert-for-word-based-analysis"></a>
</h2>
<p>Let’s see how these components can be manipulated with code. For this
we will be using the HugingFace’s <em>transformers</em> python library.
The first two main components we need to initialize are the model and
tokenizer. The HuggingFace hub contains thousands of models based on a
Transformer architecture for dozens of tasks, data domains and also
hundreds of languages. Here we will explore the vanilla English BERT
which was how everything started. We can initialize this model with the
next lines:</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> BertTokenizer, BertModel</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a>tokenizer <span class="op">=</span> BertTokenizer.from_pretrained(<span class="st">'bert-base-cased'</span>)</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a>model <span class="op">=</span> BertModel.from_pretrained(<span class="st">"bert-base-cased"</span>)</span></code></pre>
</div>
<div class="section level3">
<h3 id="bert-tokenizer">BERT Tokenizer<a class="anchor" aria-label="anchor" href="#bert-tokenizer"></a>
</h3>
<p>We start with a string of text as written in any blog, book,
newspaper etcetera. The <code>tokenizer</code> object is responsible of
splitting the string into recognizable tokens for the model and
embedding the tokens into their vector representations</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>text <span class="op">=</span> <span class="st">"Maria loves Groningen"</span></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>encoded_input <span class="op">=</span> tokenizer(text, return_tensors<span class="op">=</span><span class="st">'pt'</span>)</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a><span class="bu">print</span>(encoded_input)</span></code></pre>
</div>
<p>The print shows the <code>encoded_input</code> object returned by the
tokenizer, with its attributes and values. The <code>input_ids</code>
are the most important output for now, as these are the token IDs
recognized by BERT</p>
<pre><code>{
    'input_ids': tensor([[  101,  3406,  7871,   144,  3484, 15016,   102]]),
    'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]),
    'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])
}
</code></pre>
<p>NOTE: the printing function shows transformers objects as
dictionaries; however, to access the attributes, you must use the python
object syntax, such as in the following example:</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="bu">print</span>(encoded_input.input_ids.shape)</span></code></pre>
</div>
<p>Output:</p>
<p><code>torch.Size([1, 7])</code></p>
<p>The output is a 2-dimensional tensor where the first dimention
contains 1 element (this dimension represents the batch size), and the
second dimension contains 7 elements which are equivalent to the 7
tokens that BERT generated with our string input.</p>
<p>In order to see what these Token IDs represent, we can
<em>translate</em> them into human readable strings. This includes
converting the tensors into numpy arrays and converting each ID into its
string representation:</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a>token_ids <span class="op">=</span> <span class="bu">list</span>(encoded_input.input_ids[<span class="dv">0</span>].detach().numpy())</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>string_tokens <span class="op">=</span> tokenizer.convert_ids_to_tokens(token_ids)</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"IDs:"</span>, token_ids)</span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"TOKENS:"</span>, string_tokens)</span></code></pre>
</div>
<p><code>IDs: [101, 3406, 7871, 144, 3484, 15016, 102]</code></p>
<p><code>TOKENS: ['[CLS]', 'Maria', 'loves', 'G', '##ron', '##ingen', '[SEP]']</code></p>
<div id="callout2" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div class="callout-inner">
<div class="callout-content">
<p>In the case of wanting to obtain a single vector for
<em>enchanting</em>, you can average the three vectors that belong to
the token pieces that ultimately form that word. For example:</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>tok_en <span class="op">=</span> output.last_hidden_state[<span class="dv">0</span>][<span class="dv">15</span>].detach().numpy()</span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a>tok_chan <span class="op">=</span> output.last_hidden_state[<span class="dv">0</span>][<span class="dv">16</span>].detach().numpy()</span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a>tok_ting <span class="op">=</span> output.last_hidden_state[<span class="dv">0</span>][<span class="dv">17</span>].detach().numpy()</span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a>tok_enchanting <span class="op">=</span> np.mean([tok_en, tok_chan, tok_ting], axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a>tok_enchanting.shape</span></code></pre>
</div>
<p>We use the functions <code>detach().numpy()</code> to bring the
values from the Pytorch execution environment (for example a GPU) into
the main python thread and treat it as a numpy vector for convenvience.
Then, since we are dealing with three numpy vectors we can average the
three of them and end op with a single <code>enchanting</code> vector of
768-dimensions representing the average of
<code>'en', '##chan', '##ting'</code>.</p>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="polysemy-in-bert">Polysemy in BERT<a class="anchor" aria-label="anchor" href="#polysemy-in-bert"></a>
</h3>
<p>We can encode two sentences containing the word <em>note</em> to see
how BERT actually handles polysemy (<em>note</em> means something very
different in each sentence) thanks to the representation of each word
now being contextualized instead of isolated as was the case with
word2vec.</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="co"># Search for the index of 'note' and obtain its vector from the sequence</span></span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>note_index_1 <span class="op">=</span> string_tokens.index(<span class="st">"note"</span>)</span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a>note_vector_1 <span class="op">=</span> output.last_hidden_state[<span class="dv">0</span>][note_index_1].detach().numpy()</span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a>note_token_id_1 <span class="op">=</span> token_ids[note_index_1]</span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a><span class="bu">print</span>(note_index_1, note_token_id_1, string_tokens)</span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a><span class="bu">print</span>(note_vector_1[:<span class="dv">5</span>])</span></code></pre>
</div>
<p>We are basically printing the tokenized sentence from the previous
example and showing the index of the token <code>note</code> in the list
of tokens. We are also printing the tokenID assigned to this token and
the list of tokens. Finally, the last print shows the first five
dimensions of the vector representing the token <code>note</code>.</p>
<pre><code>12 3805 ['[CLS]', 'Maria', "'", 's', 'passion', 'for', 'music', 'is', 'clearly', 'heard', 'in', 'every', 'note', 'and', 'every', 'en', '##chan', '##ting', 'melody', '.', '[SEP]']
[0.15780845 0.38866335 0.41498923 0.03389652 0.40278202]</code></pre>
<p>Let’s encode now another sentence, also containing the word
<code>note</code>, and confirm that the same token string, with the same
assigned tokenID holds a vector with different weights:</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="co"># Encode and then take the 'note' token from the second sentence</span></span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a>note_text_2 <span class="op">=</span> <span class="st">"I could not buy milk in the supermarket because the bank note I wanted to use was fake."</span></span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a>encoded_note_2 <span class="op">=</span> tokenizer(note_text_2, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a>token_ids <span class="op">=</span> <span class="bu">list</span>(encoded_note_2.input_ids[<span class="dv">0</span>].detach().numpy())</span>
<span id="cb9-5"><a href="#cb9-5" tabindex="-1"></a>string_tokens_2 <span class="op">=</span> tokenizer.convert_ids_to_tokens(token_ids)</span>
<span id="cb9-6"><a href="#cb9-6" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" tabindex="-1"></a>note_index_2 <span class="op">=</span> string_tokens_2.index(<span class="st">"note"</span>)</span>
<span id="cb9-8"><a href="#cb9-8" tabindex="-1"></a>note_vector_2 <span class="op">=</span> model(<span class="op">**</span>encoded_note_2).last_hidden_state[<span class="dv">0</span>][note_index_2].detach().numpy()</span>
<span id="cb9-9"><a href="#cb9-9" tabindex="-1"></a>note_token_id_2 <span class="op">=</span> token_ids[note_index_2]</span>
<span id="cb9-10"><a href="#cb9-10" tabindex="-1"></a></span>
<span id="cb9-11"><a href="#cb9-11" tabindex="-1"></a><span class="bu">print</span>(note_index_2, note_token_id_2, string_tokens_2)</span>
<span id="cb9-12"><a href="#cb9-12" tabindex="-1"></a><span class="bu">print</span>(note_vector_2[:<span class="dv">5</span>])</span></code></pre>
</div>
<pre><code>12 3805 ['[CLS]', 'I', 'could', 'not', 'buy', 'milk', 'in', 'the', 'supermarket', 'because', 'the', 'bank', 'note', 'I', 'wanted', 'to', 'use', 'was', 'fake', '.', '[SEP]']
[ 0.5003222   0.653664    0.22919582 -0.32637975  0.52929205]</code></pre>
<p>To be sure, we can compute the cosine similarity of the word
<em>note</em> in the first sentence and the word <em>note</em> in the
second sentence confirming that they are indeed two different
representations, even when in both cases they have the same token-id and
they are the 12th token of the sentence:</p>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a><span class="im">from</span> sklearn.metrics.pairwise <span class="im">import</span> cosine_similarity</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a>vector1 <span class="op">=</span> np.array(note_vector_1).reshape(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a>vector2 <span class="op">=</span> np.array(note_vector_2).reshape(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb11-5"><a href="#cb11-5" tabindex="-1"></a></span>
<span id="cb11-6"><a href="#cb11-6" tabindex="-1"></a>similarity <span class="op">=</span> cosine_similarity(vector1, vector2)</span>
<span id="cb11-7"><a href="#cb11-7" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Cosine Similarity 'note' vs 'note': </span><span class="sc">{</span>similarity[<span class="dv">0</span>][<span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code></pre>
</div>
<p>With this small experiment, we have confirmed that the Encoder
produces context-dependent word representations, as opposed to Word2Vec,
where <em>note</em> would always have the same vector no matter where it
appeared.</p>
<div id="callout3" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div class="callout-inner">
<div class="callout-content">
<p>When running examples in a BERT pre-trained model, it is advisable to
wrap your code inside a <code>torch.no_grad():</code> context. This is
linked to the fact that BERT is a Neural Network that has been trained
(and can be further finetuned) with the Backpropagation algorithm.
Essentially, this wrapper tells the model that we are not in training
mode, and we are not interested in <em>updating</em> the weights (as it
would happen when training any neural network), because the weights are
already optimal enough. By using this wrapper, we make the model more
efficient as it does not need to calculate the gradients for an eventual
backpropagation step, since we are only interested in what <em>comes
out</em> of the Encoder. So the previous code can be made more efficient
like this:</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a><span class="im">import</span> torch </span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb12-4"><a href="#cb12-4" tabindex="-1"></a>    output <span class="op">=</span> model(<span class="op">**</span>encoded_input)</span>
<span id="cb12-5"><a href="#cb12-5" tabindex="-1"></a>    <span class="bu">print</span>(output)</span>
<span id="cb12-6"><a href="#cb12-6" tabindex="-1"></a>    <span class="bu">print</span>(output.last_hidden_state.shape)</span></code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="section level1">
<h1 id="bert-as-a-language-model">BERT as a Language Model<a class="anchor" aria-label="anchor" href="#bert-as-a-language-model"></a>
</h1>
<p>As mentioned before, the main pre-training task of BERT is Language
Modelling (LM): calculating the probability of a word based on the known
neighboring words (yes, Word2Vec was also a kind of LM!). Obtaining
training data for this task is very cheap, as all we need is millions of
sentences from existing texts, without any labels. In this setting, BERT
encodes a sequence of words, and predicts from a set of English tokens,
what is the most likely token that could be inserted in the
<code>[MASK]</code> position</p>
<figure><img src="fig/bert1b.png" alt="BERT Language Modeling" class="figure mx-auto d-block"><div class="figcaption">BERT Language Modeling</div>
</figure><p>We can therefore start using BERT as a predictor for word completion.
From now own, we will learn how to use the <code>pipeline</code> object,
this is very useful when we only want to use a pre-trained model for
predictions (no need to fine-tune or do word-specific analysis). The
<code>pipeline</code> will internally initialize both model and
tokenizer for us and also merge back word pieces into complete
words.</p>
<p>In this case again we use <code>bert-base-cased</code>, which refers
to the vanilla BERT English model. Once we declared a pipeline, we can
feed it with sentences that contain one masked token at a time (beware
that BERT can only predict one word at a time, since that was its
training scheme). For example:</p>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a><span class="kw">def</span> pretty_print_outputs(sentences, model_outputs):</span>
<span id="cb13-4"><a href="#cb13-4" tabindex="-1"></a>    <span class="cf">for</span> i, model_out <span class="kw">in</span> <span class="bu">enumerate</span>(model_outputs):</span>
<span id="cb13-5"><a href="#cb13-5" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">=====</span><span class="ch">\t</span><span class="st">"</span>,sentences[i])</span>
<span id="cb13-6"><a href="#cb13-6" tabindex="-1"></a>        <span class="cf">for</span> label_scores <span class="kw">in</span> model_out:</span>
<span id="cb13-7"><a href="#cb13-7" tabindex="-1"></a>            <span class="bu">print</span>(label_scores)</span>
<span id="cb13-8"><a href="#cb13-8" tabindex="-1"></a></span>
<span id="cb13-9"><a href="#cb13-9" tabindex="-1"></a></span>
<span id="cb13-10"><a href="#cb13-10" tabindex="-1"></a>nlp <span class="op">=</span> pipeline(task<span class="op">=</span><span class="st">"fill-mask"</span>, model<span class="op">=</span><span class="st">"bert-base-cased"</span>, tokenizer<span class="op">=</span><span class="st">"bert-base-cased"</span>)</span>
<span id="cb13-11"><a href="#cb13-11" tabindex="-1"></a>sentences <span class="op">=</span> [<span class="st">"Paris is the [MASK] of France"</span>, <span class="st">"I want to eat a cold [MASK] this afternoon"</span>, <span class="st">"Maria [MASK] Groningen"</span>]</span>
<span id="cb13-12"><a href="#cb13-12" tabindex="-1"></a>model_outputs <span class="op">=</span> nlp(sentences, top_k<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb13-13"><a href="#cb13-13" tabindex="-1"></a>pretty_print_outputs(sentences, model_outputs)</span></code></pre>
</div>
<pre><code>=====	 Paris is the [MASK] of France
{'score': 0.9807965755462646, 'token': 2364, 'token_str': 'capital', 'sequence': 'Paris is the capital of France'}
{'score': 0.004513159394264221, 'token': 6299, 'token_str': 'Capital', 'sequence': 'Paris is the Capital of France'}
{'score': 0.004281804896891117, 'token': 2057, 'token_str': 'center', 'sequence': 'Paris is the center of France'}
{'score': 0.002848200500011444, 'token': 2642, 'token_str': 'centre', 'sequence': 'Paris is the centre of France'}
{'score': 0.0022805952467024326, 'token': 1331, 'token_str': 'city', 'sequence': 'Paris is the city of France'}

=====	 I want to eat a cold [MASK] this afternoon
{'score': 0.19168031215667725, 'token': 13473, 'token_str': 'pizza', 'sequence': 'I want to eat a cold pizza this afternoon'}
{'score': 0.14800849556922913, 'token': 25138, 'token_str': 'turkey', 'sequence': 'I want to eat a cold turkey this afternoon'}
{'score': 0.14620967209339142, 'token': 14327, 'token_str': 'sandwich', 'sequence': 'I want to eat a cold sandwich this afternoon'}
{'score': 0.09997560828924179, 'token': 5953, 'token_str': 'lunch', 'sequence': 'I want to eat a cold lunch this afternoon'}
{'score': 0.06001955270767212, 'token': 4014, 'token_str': 'dinner', 'sequence': 'I want to eat a cold dinner this afternoon'}

=====	 Maria [MASK] Groningen
{'score': 0.24399833381175995, 'token': 117, 'token_str': ',', 'sequence': 'Maria, Groningen'}
{'score': 0.12300779670476913, 'token': 1104, 'token_str': 'of', 'sequence': 'Maria of Groningen'}
{'score': 0.11991506069898605, 'token': 1107, 'token_str': 'in', 'sequence': 'Maria in Groningen'}
{'score': 0.07722211629152298, 'token': 1306, 'token_str': '##m', 'sequence': 'Mariam Groningen'}
{'score': 0.0632941722869873, 'token': 118, 'token_str': '-', 'sequence': 'Maria - Groningen'}
</code></pre>
<p>When we call the <code>nlp</code> pipeline, requesting to return the
<code>top_k</code> most likely suggestions to complete the provided
sentences (in this case <code>k=5</code>). The pipeline returns a list
of outputs as python dictionaries. Depending on the task, the fields of
the dictionary will differ. In this case, the <code>fill-mask</code>
task returns a score (between 0 and 1, the higher the score the more
likely the token is), a tokenId, and its corresponding string, as well
as the full “unmasked” sequence.</p>
<p>In the list of outputs we can observe: the first example shows
correctly that the missing token in the first sentence is
<em>capital</em>, the second example is a bit more ambiguous, but the
model at least uses the context to correctly predict a series of items
that can be eaten (unfortunately, none of its suggestions sound very
tasty); finally, the third example gives almost no useful context so the
model plays it safe and only suggests prepositions or punctuation. This
already shows some of the weaknesses of the approach.</p>
<p>We will next see the case of combining BERT with a classifier on
top.</p>
</div>
<div class="section level1">
<h1 id="bert-for-text-classification">BERT for Text Classification<a class="anchor" aria-label="anchor" href="#bert-for-text-classification"></a>
</h1>
<p>The task of text classification is assigning a label to a whole
sequence of tokens, for example a sentence. With the parameter
<code>task="text_classification"</code> the <code>pipeline()</code>
function will load the base model and automatically add a linear layer
with a softmax on top. This layer can be fine-tuned with our own labeled
data or we can also directly load the fully pre-trained text
classification models that are already available in HuggingFace.</p>
<figure><img src="fig/bert4.png" alt="BERT as an Emotion Classifier" class="figure mx-auto d-block"><div class="figcaption">BERT as an Emotion Classifier</div>
</figure><p>Let’s see the example of a ready pre-trained emotion classifier based
on <code>RoBERTa</code> model. This model was fine-tuned in the Go
emotions <a href="https://huggingface.co/datasets/google-research-datasets/go_emotions" class="external-link">dataset</a>,
taken from English Reddit and labeled for 28 different emotions at the
sentence level. The fine-tuned model is called <a href="https://huggingface.co/SamLowe/roberta-base-go_emotions" class="external-link">roberta-base-go_emotions</a>.
This model takes a sentence as input and ouputs a probability
distribution over the 28 possible emotions that might be conveyed in the
text. For example:</p>
<div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a></span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a>classifier <span class="op">=</span> pipeline(task<span class="op">=</span><span class="st">"text-classification"</span>, model<span class="op">=</span><span class="st">"SamLowe/roberta-base-go_emotions"</span>, top_k<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb15-3"><a href="#cb15-3" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" tabindex="-1"></a>sentences <span class="op">=</span> [<span class="st">"I am not having a great day"</span>, <span class="st">"This is a lovely and innocent sentence"</span>, <span class="st">"Maria loves Groningen"</span>]</span>
<span id="cb15-5"><a href="#cb15-5" tabindex="-1"></a>model_outputs <span class="op">=</span> classifier(sentences)</span>
<span id="cb15-6"><a href="#cb15-6" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" tabindex="-1"></a>pretty_print_outputs(sentences, model_outputs)</span></code></pre>
</div>
<pre><code>=====	 I am not having a great day
{'label': 'disappointment', 'score': 0.46669483184814453}
{'label': 'sadness', 'score': 0.39849498867988586}
{'label': 'annoyance', 'score': 0.06806594133377075}

=====	 This is a lovely and innocent sentence
{'label': 'admiration', 'score': 0.6457845568656921}
{'label': 'approval', 'score': 0.5112180113792419}
{'label': 'love', 'score': 0.09214121848344803}

=====	 Maria loves Groningen
{'label': 'love', 'score': 0.8922032117843628}
{'label': 'neutral', 'score': 0.10132959485054016}
{'label': 'approval', 'score': 0.02525361441075802}</code></pre>
<p>This code outputs again a list of dictionaries with the
<code>top-k</code> (<code>k=3</code>) emotions that each of the two
sentences convey. In this case, the first sentence evokes (in order of
likelihood) <em>dissapointment</em>, <em>sadness</em> and
<em>annoyance</em>; whereas the second sentence evokes <em>love</em>,
<em>neutral</em> and <em>approval</em>. Note however that the likelihood
of each prediction decreases dramatically below the top choice, so
perhaps this specific classifier is only useful for the top emotion.</p>
<div id="callout4" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<span class="callout-header">Callout</span>
<div class="callout-inner">
<div class="callout-content">
<p>Finetunning BERT is very cheap, because we only need to train the
<em>classifier</em> layer, a very small neural network, that can learn
to choose between the classes (labels) for your custom classification
problem, without needing a big amount of annotated data. This classifier
is just a one-layer neural layer with a softmax that assigns a score
that can be translated to the probability over a set of labels, given
the input features provided by BERT, which <em>encodes</em> the meaning
of the entire sequence in its hidden states.</p>
</div>
</div>
</div>
<figure><img src="fig/bert4b.png" alt="BERT as an Emotion Classifier" class="figure mx-auto d-block"><div class="figcaption">BERT as an Emotion Classifier</div>
</figure>
</div>
<div class="section level1">
<h1 id="bert-for-token-classification">BERT for Token Classification<a class="anchor" aria-label="anchor" href="#bert-for-token-classification"></a>
</h1>
<p>Just as we plugged in a trainable text classifier layer, we can add a
token-level classifier that assigns a class to each of the tokens
encoded by a transformer (as opposed to one label for the whole
sequence). A specific example of this task is Named Entity Recognition,
but you can basically define any task that requires to
<em>highlight</em> sub-strings of text and classify them using this
technique.</p>
<div class="section level2">
<h2 id="named-entity-recognition">Named Entity Recognition<a class="anchor" aria-label="anchor" href="#named-entity-recognition"></a>
</h2>
<p>Named Entity Recognition (NER) is the task of recognizing mentions of
real-world entities inside a text. The concept of
<strong>Entity</strong> includes proper names that unequivocally
identify a unique individual (PER), place (LOC), organization (ORG), or
other object/name (MISC). Depending on the domain, the concept can
expanded to recognize other unique (and more conceptual) entities such
as DATE, MONEY, WORK_OF_ART, DISEASE, PROTEIN_TYPE, etcetera…</p>
<p>In terms of NLP, this boils down to classifying each token into a
series of labels (<code>PER</code>, <code>LOC</code>, <code>ORG</code>,
<code>O</code>[no-entity] ). Since a single entity can be expressed with
multiple words (e.g. New York) the usual notation used for labeling the
text is IOB (<strong>I</strong>nner <strong>O</strong>ut
<strong>B</strong>eginnig of entity) notations which identifies the
limits of each entity tokens. For example:</p>
<figure><img src="fig/bert5.png" alt="BERT as an NER Classifier" class="figure mx-auto d-block"><div class="figcaption">BERT as an NER Classifier</div>
</figure><p>This is a typical sequence classification problem where an imput
sequence must be fully mapped into an output sequence of labels with
global constraints (for example, there can’t be an inner I-LOC label
before a beginning B-LOC label). Since the labels of the tokens are
context dependent, a language model with attention mechanism such as
BERT is very beneficial for a task like NER.</p>
<p>Because this is one of the core tasks in NLP, there are dozens of
pre-trained NER classifiers in HuggingFace that you can use right away.
We use once again the <code>pipeline()</code> to run the model for
predictions in your custom data, in this case with
<code>task="ner"</code>. For example:</p>
<div class="codewrapper sourceCode" id="cb17">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer, AutoModelForTokenClassification</span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb17-3"><a href="#cb17-3" tabindex="-1"></a></span>
<span id="cb17-4"><a href="#cb17-4" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(<span class="st">"dslim/bert-base-NER"</span>)</span>
<span id="cb17-5"><a href="#cb17-5" tabindex="-1"></a>model <span class="op">=</span> AutoModelForTokenClassification.from_pretrained(<span class="st">"dslim/bert-base-NER"</span>)</span>
<span id="cb17-6"><a href="#cb17-6" tabindex="-1"></a></span>
<span id="cb17-7"><a href="#cb17-7" tabindex="-1"></a>ner_classifier <span class="op">=</span> pipeline(<span class="st">"token-classification"</span>, model<span class="op">=</span>model, tokenizer<span class="op">=</span>tokenizer)</span>
<span id="cb17-8"><a href="#cb17-8" tabindex="-1"></a>example <span class="op">=</span> <span class="st">"My name is Wolfgang Schmid and I live in Berlin"</span></span>
<span id="cb17-9"><a href="#cb17-9" tabindex="-1"></a></span>
<span id="cb17-10"><a href="#cb17-10" tabindex="-1"></a>ner_results <span class="op">=</span> ner_classifier(example)</span>
<span id="cb17-11"><a href="#cb17-11" tabindex="-1"></a><span class="cf">for</span> nr <span class="kw">in</span> ner_results:</span>
<span id="cb17-12"><a href="#cb17-12" tabindex="-1"></a>    <span class="bu">print</span>(nr)</span></code></pre>
</div>
<p>The code prints the following:</p>
<pre><code>{'entity': 'B-PER', 'score': 0.9996068, 'index': 4, 'word': 'Wolfgang', 'start': 11, 'end': 19}
{'entity': 'I-PER', 'score': 0.999582, 'index': 5, 'word': 'Sc', 'start': 20, 'end': 22}
{'entity': 'I-PER', 'score': 0.9990482, 'index': 6, 'word': '##hm', 'start': 22, 'end': 24}
{'entity': 'I-PER', 'score': 0.9951691, 'index': 7, 'word': '##id', 'start': 24, 'end': 26}
{'entity': 'B-LOC', 'score': 0.99956733, 'index': 12, 'word': 'Berlin', 'start': 41, 'end': 47}</code></pre>
<p>In this case the output of the pipeline is a list of dictionaries,
each one representing only entity <code>IOB</code> labels at the BERT
token level. IMPORTANT: this list is per wordPiece and NOT per <em>human
word</em> even if the provided text is pre-tokenized. You can assume all
of the tokens that don’t appear in the output were labeled as no-entity,
that is <code>"O"</code>. To recover the full-word entities you can
initialize the pipeline with
<code>aggregation_strategy="first"</code>:</p>
<div class="codewrapper sourceCode" id="cb19">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" tabindex="-1"></a>ner_classifier <span class="op">=</span> pipeline(<span class="st">"token-classification"</span>, model<span class="op">=</span>model, tokenizer<span class="op">=</span>tokenizer, aggregation_strategy<span class="op">=</span><span class="st">"first"</span>)</span>
<span id="cb19-2"><a href="#cb19-2" tabindex="-1"></a>example <span class="op">=</span> <span class="st">"My name is Wolfgang Schmid and I live in Berlin"</span></span>
<span id="cb19-3"><a href="#cb19-3" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" tabindex="-1"></a>ner_results <span class="op">=</span> ner_classifier(example)</span>
<span id="cb19-5"><a href="#cb19-5" tabindex="-1"></a><span class="cf">for</span> nr <span class="kw">in</span> ner_results:</span>
<span id="cb19-6"><a href="#cb19-6" tabindex="-1"></a>    <span class="bu">print</span>(nr)</span></code></pre>
</div>
<p>The code now prints the following:</p>
<pre><code>{'entity_group': 'PER', 'score': 0.9995944, 'word': 'Wolfgang Schmid', 'start': 11, 'end': 26}
{'entity_group': 'LOC', 'score': 0.99956733, 'word': 'Berlin', 'start': 41, 'end': 47}</code></pre>
<p>As you can see, entities aggregated at the Span Leven (instead of the
Token Level). Word pieces are merged back into <em>human words</em> and
also multiword entities are assigned a single entity label unifying the
<code>IOB</code> labels into one. Depending on your use case you can
request the pipeline to give different
<code>aggregation_strateg[ies]</code>. More info about the pipeline can
be found <a href="https://huggingface.co/docs/transformers/main_classes/pipelines" class="external-link">here</a>.</p>
<p>The next step is crucial: evaluate how does the pre-trained model
actually performs in <strong>your dataset</strong>. This is important
since the fine-tuned model could be overfitted to other custom
benchmarks that do not share the characteristics of your dataset.</p>
<p>To observe this, we can first see the performance on the test portion
of the dataset in which this classifier was trained, and then evaluate
the same pre-trained classifier on a NER dataset form a different
domain.</p>
</div>
<div class="section level2">
<h2 id="model-evaluation">Model Evaluation<a class="anchor" aria-label="anchor" href="#model-evaluation"></a>
</h2>
<p>To perform evaluation in your data you can use again the
<code>seqeval</code> package:</p>
<div class="codewrapper sourceCode" id="cb21">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" tabindex="-1"></a></span>
<span id="cb21-2"><a href="#cb21-2" tabindex="-1"></a><span class="im">from</span> seqeval.metrics <span class="im">import</span> classification_report</span>
<span id="cb21-3"><a href="#cb21-3" tabindex="-1"></a><span class="bu">print</span>(classification_report(gold_labels, model_predictions))</span></code></pre>
</div>
<p>Since we took a classifier that was not trained for the book domain,
the performance is quite poor. But this example shows us that
classifiers performing very well on their own domain most of the times
transfer poorly to other apparently similar datasets.</p>
<p>The solution in this case is to use another of the great
characteristics of BERT: fine-tuning for domain adaptation. It is
possible to train your own classifier with relatively small data (given
that a lot of linguistic knowledge was already provided during the
language modeling pre-training). In the following section we will see
how to train your own NER model and use it for predictions.</p>
</div>
</div></section><section id="aio-04-LargeLanguageModels"><p>Content from <a href="04-LargeLanguageModels.html">Episode 3: Using large language models</a></p>
<hr>
<p>Last updated on 2025-09-24 |

        <a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/edit/main/episodes/04-LargeLanguageModels.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<section><h2 class="section-heading" id="background">Background<a class="anchor" aria-label="anchor" href="#background"></a>
</h2>
<hr class="half-width">
<p>Chat assistants like <a href="https://chatgpt.com/" class="external-link">ChatGPT</a> and
<a href="http://claude.ai" class="external-link">Claude</a>, which are based on Large Language
Models (LLMs) are widely used today for tasks such as content
generation, question answering, research and software development. The
rapid rise of such models has had quite a disruptive and strong impact.
But what are these models exactly? How do they work ‘under the hood’?
And how can one use them programmatically, in a responsible and
effective way?</p>
<p>This episode is a gentle introduction to LLMs which aims to equip you
with knowledge of the underpinnings of LLMs based on transformers
architecture, as well as practical skills to programmatically work with
LLMs in your own projects.</p>
<p><img src="fig/llm-logos/anthropic.png" alt="Company A" width="80" class="figure"><img src="fig/llm-logos/alibaba.png" alt="Company B" width="80" class="figure"><img src="fig/llm-logos/xai.jpg" alt="Company C" width="80" class="figure"><img src="fig/llm-logos/zhipu.png" alt="Company C" width="150" class="figure"><img src="fig/llm-logos/google.png" alt="Company C" width="80" class="figure"><img src="fig/llm-logos/openai.jpg" alt="Company C" width="150" class="figure"></p>
<p><img src="fig/llm-logos/nvidia.png" alt="Company D" width="80" class="figure"><img src="fig/llm-logos/deepseek.png" alt="Company E" width="80" class="figure"><img src="fig/llm-logos/huggingface.png" alt="Company F" width="80" class="figure"><img src="fig/llm-logos/meta.png" alt="Company C" width="150" class="figure"></p>
</section><section><h2 class="section-heading" id="what-are-large-language-models-llms">1. What are Large Language Models (LLMs)?<a class="anchor" aria-label="anchor" href="#what-are-large-language-models-llms"></a>
</h2>
<hr class="half-width">
<p>Large language models (LLMs) are transformer-based language models
that are specialised to interpret and generate text, and to converse in
a conversational-like manner with humans. The text that they generate
are mostly natural language but can, in theory, constitute any character
or symbol sequence such as software code. They represent a significant
advancement in AI and NLP. and are trained on vast amounts of textual
data mostly obtained from the internet.</p>
<div class="section level3">
<h3 id="examples-of-llms">1.1 Examples of LLMs<a class="anchor" aria-label="anchor" href="#examples-of-llms"></a>
</h3>
<p>Many different LLMs have been, and continue to be, developed. There
are both proprietary and open-source varieties. Open-source varieties
often make the data that their LLMs are trained on free, open and
accessible online. Some even make the code they use to train these
models open-source as well. Below is a summary of some current LLMs
together with their creators, chat assistant interfaces, and proprietary
status:</p>
<figure><img src="fig/llm_table4.png" alt="LLMs table" width="1000" class="figure mx-auto d-block"></figure><hr>
</div>
<div class="section level3">
<h3 id="applications-of-llms">1.2 Applications of LLMs<a class="anchor" aria-label="anchor" href="#applications-of-llms"></a>
</h3>
<p>LLMs can be used for many different helpful tasks. Some common tasks
include:</p>
<ul>
<li>Question Answering</li>
<li>Text Generation</li>
<li>Text Summarisation</li>
<li>Sentiment Analysis</li>
<li>Machine Translation</li>
<li>Code Generation</li>
</ul>
<div class="section level4">
<h4 id="exercise-1-your-first-programmatic-llm-interaction-30-minutes">Exercise 1: Your first programmatic LLM interaction (30
minutes)<a class="anchor" aria-label="anchor" href="#exercise-1-your-first-programmatic-llm-interaction-30-minutes"></a>
</h4>
<p>Before exploring how we can invoke LLMs programmatically to solve the
kinds of tasks abve, let us setup and load our first LLM.</p>
<div class="section level5">
<h5 id="step-1--setup-code">Step 1. Setup code<a class="anchor" aria-label="anchor" href="#step-1--setup-code"></a>
</h5>
<p>Install required packages <code>transformers</code> and
<code>torch</code> and import required libraries.</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModelForCausalLM, AutoTokenizer, pipeline</span></code></pre>
</div>
</div>
<div class="section level5">
<h5 id="step-2-load-and-setup-an-llm">Step 2: Load and setup an LLM<a class="anchor" aria-label="anchor" href="#step-2-load-and-setup-an-llm"></a>
</h5>
<p>Let’s load a lightweight LLM.</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="co"># We'll use SmolLM-135M - an open, small, fast model</span></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a><span class="co"># model_name = "HuggingFaceTB/SmolLM2-135M" # base model</span></span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>model_name <span class="op">=</span> <span class="st">"HuggingFaceTB/SmolLM2-135M-Instruct"</span> <span class="co"># fine-tuned assistant model</span></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a><span class="co"># model_name = "HuggingFaceTB/SmolLM3-3B-Base" # base model</span></span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a><span class="co"># model_name = "HuggingFaceTB/SmolLM3-3B" # fine-tuned assistant model</span></span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a><span class="co"># Load tokenizer and model</span></span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(model_name)</span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a>model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(model_name)</span>
<span id="cb2-10"><a href="#cb2-10" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" tabindex="-1"></a><span class="co"># Check if model is loaded correctly</span></span>
<span id="cb2-12"><a href="#cb2-12" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Model loaded! It has </span><span class="sc">{</span>model<span class="sc">.</span>num_parameters()<span class="sc">:,}</span><span class="ss"> parameters"</span>)</span></code></pre>
</div>
</div>
<div class="section level5">
<h5 id="step-3-basic-text-generation">Step 3: Basic Text Generation<a class="anchor" aria-label="anchor" href="#step-3-basic-text-generation"></a>
</h5>
<p>Let’s perform inference with the LLM to generate some text.</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="co"># Set pad_token_id to eos_token_id to avoid warnings</span></span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a><span class="co"># if tokenizer.pad_token_id is None:</span></span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a><span class="co">#     tokenizer.pad_token = tokenizer.eos_token</span></span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a><span class="co">#     model.config.pad_token_id = tokenizer.eos_token_id</span></span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a>    </span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a><span class="co"># Build pipeline</span></span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a>llm <span class="op">=</span> pipeline(<span class="st">"text-generation"</span>, model<span class="op">=</span>model, tokenizer<span class="op">=</span>tokenizer)</span>
<span id="cb3-8"><a href="#cb3-8" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"</span><span class="sc">{}</span><span class="st">"</span></span>
<span id="cb3-9"><a href="#cb3-9" tabindex="-1"></a>response <span class="op">=</span> llm(prompt, max_new_tokens<span class="op">=</span><span class="dv">100</span>, do_sample<span class="op">=</span><span class="va">True</span>, top_k<span class="op">=</span><span class="dv">50</span>, temperature<span class="op">=</span><span class="fl">0.7</span>)[<span class="dv">0</span>][<span class="st">"generated_text"</span>]</span>
<span id="cb3-10"><a href="#cb3-10" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Prompt: </span><span class="sc">{</span>prompt<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-11"><a href="#cb3-11" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Response: </span><span class="sc">{</span>response<span class="sc">}</span><span class="ss">"</span>)</span></code></pre>
</div>
<p><strong><em>max_new_tokens</em>:</strong> sets maximum number of
tokens (roughly words/word pieces) that the model will generate in
total. It’s a hard limit - generation stops when this limit is reached,
even mid-sentence. Useful for controlling cost / time. The more tokens
you need to generate for an answer the more time it takes. LLMs called
through paid APIs often charge per a set number of tokens (e.g. $0.008
per 1000 tokens).</p>
<p><strong><em>temperature</em>:</strong> positive float value that
controls the randomness/creativity of the model’s token selection during
generation. The model predicts probabilities for each possible next
token, temperature modifies these probabilities before making the final
choice.</p>
<p>0.0: Completely deterministic - always picks the most likely token
1.0+: More random, and “creative”, but potentially less coherent</p>
<p><strong><em>do_sample</em>:</strong> when do_sample=True, the model
generates text by sampling from the probability distribution of possible
next tokens. If do_sample=False, the model uses <a href="https://huggingface.co/docs/transformers/generation_strategies" class="external-link">greedy
decoding</a> (always picking the most likely next token), which makes
the output more deterministic but often repetitive.</p>
<p><strong><em>top_k</em>:</strong> This is a sampling strategy called
<a href="https://arxiv.org/pdf/1805.04833" class="external-link">Top-K sampling</a>. Instead
of considering all possible next tokens, the model looks at the k most
likely tokens (based on their probabilities) and samples only from that
reduced set. If top_k=50, the model restricts its choices to the top 50
most probable words at each step.</p>
</div>
<div class="section level5">
<h5 id="step-5--sentiment-analysis">Step 5. Sentiment analysis<a class="anchor" aria-label="anchor" href="#step-5--sentiment-analysis"></a>
</h5>
<p>Let us try a sentiment analysis task to see how well different models
(with different number of parameters perform). Consider the following
set of lines from product reviews:</p>
<p><strong>Product reviews:</strong></p>
<ol style="list-style-type: decimal">
<li>I love this movie! It was absolutely fantastic and made my day.
[<strong>positive</strong>]</li>
<li>This product is terrible. I hate everything about it.
[<strong>negative</strong>]</li>
<li>Nothing says quality like a phone that dies after 20 minutes.
[<strong>negative</strong>]</li>
<li>The movie was dark and depressing — exactly what I was hoping for.
[<strong>positive</strong>]</li>
<li>The food was delicious, but the service was painfully slow.
[<strong>mixed</strong>]</li>
</ol>
<p>Set the prompt for this as (substitute the above sentences for
<a href="https://r-text.org/" class="external-link">text</a> each time):</p>
<p><code>Classify the sentiment of the following text as either POSITIVE or NEGATIVE. Text: "{text}"</code></p>
<p>Examine the results afterwards to see which models correctly
classified them and which didn’t.</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a>sentiment_llm <span class="op">=</span> pipeline(<span class="st">"text-generation"</span>, model<span class="op">=</span>model, tokenizer<span class="op">=</span>tokenizer)</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>sentiment_texts <span class="op">=</span> [</span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>    <span class="st">"I love this movie! It was absolutely fantastic and made my day."</span>,</span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a>    <span class="st">"This product is terrible. I hate everything about it."</span>,</span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a>    <span class="st">"Nothing says quality like a phone that dies after 20 minutes."</span>,</span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a>    <span class="st">"The movie was dark and depressing — exactly what I was hoping for."</span>,</span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a>    <span class="st">"The food was delicious, but the service was painfully slow."</span></span>
<span id="cb4-8"><a href="#cb4-8" tabindex="-1"></a>]</span>
<span id="cb4-9"><a href="#cb4-9" tabindex="-1"></a>text <span class="op">=</span> sentiment_texts[<span class="dv">0</span>]</span>
<span id="cb4-10"><a href="#cb4-10" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"Classify the sentiment of the following text as either POSITIVE or NEGATIVE. Text: "</span>{text}<span class="st">""</span></span>
<span id="cb4-11"><a href="#cb4-11" tabindex="-1"></a>response <span class="op">=</span> sentiment_llm(prompt, max_new_tokens<span class="op">=</span><span class="dv">100</span>, do_sample<span class="op">=</span><span class="va">True</span>, top_k<span class="op">=</span><span class="dv">50</span>, temperature<span class="op">=</span><span class="fl">0.7</span>)[<span class="dv">0</span>][<span class="st">"generated_text"</span>]</span>
<span id="cb4-12"><a href="#cb4-12" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Prompt: </span><span class="sc">{</span>prompt<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb4-13"><a href="#cb4-13" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Response: </span><span class="sc">{</span>response<span class="sc">}</span><span class="ss">"</span>)</span></code></pre>
</div>
</div>
<div class="section level5">
<h5 id="discussion-post-exercise-questions">Discussion: Post-exercise questions<a class="anchor" aria-label="anchor" href="#discussion-post-exercise-questions"></a>
</h5>
<ol style="list-style-type: decimal">
<li>
<strong>What did you notice about the models’ responses?</strong>
<ul>
<li>Were they always accurate? Always coherent?</li>
<li>How did different prompts affect the quality?</li>
</ul>
</li>
<li>
<strong>Temperature Effects:</strong>
<ul>
<li>What happened when temperature was low (e.g. 0.0 or 0.1) vs. high
(e.g. 1.2)?</li>
<li>Under which circumstances would you want more random / creative
responses vs. consistent responses?</li>
</ul>
</li>
<li>
<strong>Model Size:</strong>
<ul>
<li>What were the differences across different models?</li>
<li>What trade-offs do you think exist between model size and
performance?</li>
</ul>
</li>
<li>
<strong>Max Length Effects:</strong>
<ul>
<li>Did you notice a difference in speed of responses when adjusting the
max_length parameter?</li>
</ul>
</li>
</ol>
</div>
</div>
<div class="section level4">
<h4 id="exercise-2-other-nlp-tasks">Exercise 2: Other NLP tasks<a class="anchor" aria-label="anchor" href="#exercise-2-other-nlp-tasks"></a>
</h4>
<p>Write and execute the following prompts in Python code within Jupyter
notebook.</p>
<ol style="list-style-type: decimal">
<li>
<p><strong>Question answering:</strong></p>
<ul>
<li>answering general knowledge questions <br>
</li>
</ul>
<pre><code>Human: What is the longest river in the world?
LLM: The Nile River in Africa is traditionally considered the longest river in the world, stretching about 6,650 km (4,130 miles) through 11 countries before emptying into the Mediterranean Sea.</code></pre>
<p><br></p>
<ul>
<li>expert advice in a particular domain or scientific field <br>
</li>
</ul>
<pre><code>Human: What are good strategies for film-making on a budget in Night conditions?
LLM: Night filmmaking can be both creatively rewarding and technically challenging, especially on a budget. Here are some strategies that indie filmmakers often use to maximize image quality and mood without expensive gear:...</code></pre>
<p><br></p>
</li>
<li>
<p><strong>Text generation:</strong></p>
<ul>
<li>Writing essays, business plans and other documents <br>
</li>
</ul>
<pre><code>Human: Write a template cover letter for a mid-level software engineering position at a large AI startup.
LLM: Dear [Hiring Manager’s Name], I am excited to apply for the Software Engineer position at [Company Name]...</code></pre>
<p><br></p>
<ul>
<li>Creative writing (slogans, poems, fiction, jokes) <br>
</li>
</ul>
<pre><code>Human: Write a two sentence comedy sketch.
LLM: [Scene: A man sits in a job interview.] Interviewer: "Your resume says you’re fluent in Python?" Candidate: pulls out a snake from his briefcase "She does most of the talking."</code></pre>
<p><br></p>
</li>
<li>
<p><strong>Text summarisation</strong> <br></p>
<pre><code>Human: Shorten: "Amsterdam, the capital of the Netherlands, is a city celebrated for its rich history, cultural diversity, and iconic canals. Known as the “Venice of the North,” it is home to a vast network of waterways lined with elegant 17th-century houses, giving the city its distinct charm. Amsterdam seamlessly blends old-world character with modern vibrancy, attracting millions of visitors each year."
LLM: Amsterdam, the capital of the Netherlands, is known for its history, canals, and 17th-century houses, combining old charm with modern vibrancy that attracts millions each year.</code></pre>
<p><br></p>
</li>
<li>
<p><strong>Sentiment or text classification</strong> <br></p>
<pre><code>Sentiment analysis
Human: Is this a positive, neutral or negative statement about the movie: "Incredible and not in a good way."
LLM: That’s a negative statement about the movie. The phrase “Incredible” usually suggests something impressive, but the qualifier “not in a good way” makes it clear the speaker means the film was bad or shockingly poor.</code></pre>
<p><br></p>
<pre><code>Text classification
Human: Categorise this statement into either sports, business, or science: "Lionel Messi scored twice to lead Argentina to victory in the World Cup qualifier."
LLM: This statement falls under Sports. It talks about a soccer player and a match result, which clearly relates to athletic competition.</code></pre>
<p><br></p>
</li>
<li><p><strong>Language translation</strong> <br><code>Human: Translate "I have recently naturalised as a Dutch citizen" into Dutch.     LLM: Ik ben onlangs genaturaliseerd als Nederlands staatsburger.</code>
<br></p></li>
<li>
<p><strong>Generating software code</strong> <br></p>
<pre><code>Human: Write a short Python function to demonstrate bubble sort
LLM: ...</code></pre>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a><span class="kw">def</span> bubble_sort(arr):</span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a>    n <span class="op">=</span> <span class="bu">len</span>(arr)</span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb13-4"><a href="#cb13-4" tabindex="-1"></a>        <span class="co"># Last i elements are already in place</span></span>
<span id="cb13-5"><a href="#cb13-5" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, n <span class="op">-</span> i <span class="op">-</span> <span class="dv">1</span>):</span>
<span id="cb13-6"><a href="#cb13-6" tabindex="-1"></a>            <span class="cf">if</span> arr[j] <span class="op">&gt;</span> arr[j <span class="op">+</span> <span class="dv">1</span>]:</span>
<span id="cb13-7"><a href="#cb13-7" tabindex="-1"></a>                <span class="co"># Swap if the element is greater than the next</span></span>
<span id="cb13-8"><a href="#cb13-8" tabindex="-1"></a>                arr[j], arr[j <span class="op">+</span> <span class="dv">1</span>] <span class="op">=</span> arr[j <span class="op">+</span> <span class="dv">1</span>], arr[j]</span>
<span id="cb13-9"><a href="#cb13-9" tabindex="-1"></a>    <span class="cf">return</span> arr</span>
<span id="cb13-10"><a href="#cb13-10" tabindex="-1"></a></span>
<span id="cb13-11"><a href="#cb13-11" tabindex="-1"></a><span class="co"># Example usage</span></span>
<span id="cb13-12"><a href="#cb13-12" tabindex="-1"></a>numbers <span class="op">=</span> [<span class="dv">64</span>, <span class="dv">34</span>, <span class="dv">25</span>, <span class="dv">12</span>, <span class="dv">22</span>, <span class="dv">11</span>, <span class="dv">90</span>]</span>
<span id="cb13-13"><a href="#cb13-13" tabindex="-1"></a>sorted_numbers <span class="op">=</span> bubble_sort(numbers)</span>
<span id="cb13-14"><a href="#cb13-14" tabindex="-1"></a><span class="bu">print</span>(sorted_numbers)</span></code></pre>
</div>
</li>
</ol>
</div>
<div class="section level4">
<h4 id="key-takeaways">Key Takeaways<a class="anchor" aria-label="anchor" href="#key-takeaways"></a>
</h4>
<ul>
<li>
<strong>LLMs are generative models</strong> - they predict the next
most likely tokens</li>
<li>
<strong>Prompts matter</strong> - the way you ask affects what you
get</li>
<li>
<strong>Parameters control behavior</strong> - temperature,
max_length, etc. tune the output</li>
<li>
<strong>Models have limitations</strong> - they can be wrong,
inconsistent, or biased</li>
<li>
<strong>Size vs Speed trade-off</strong> - smaller models are faster
but less capable</li>
</ul>
<hr>
</div>
</div>
<div class="section level3">
<h3 id="llm-selection-criteria">1.3 LLM selection criteria<a class="anchor" aria-label="anchor" href="#llm-selection-criteria"></a>
</h3>
<p>Choosing the right LLM for your specific use case requires
consideration of multiple factors. This section will guide you through
some decision points that will help you select an appropriate model for
your needs.</p>
<div class="section level4">
<h4 id="openness-and-licensing-considerations">1.3.1 Openness and Licensing Considerations<a class="anchor" aria-label="anchor" href="#openness-and-licensing-considerations"></a>
</h4>
<p>The spectrum of model availability ranges from fully open to
completely proprietary:</p>
<p><strong>Open-weights</strong> release the trained model parameters
while keeping training code or data proprietary. This allows you to run
and fine-tune the model locally but if you don’t have the code used to
train the model or information about the architecture used, it limits
your ability to fully understand or replicate the training process.</p>
<p><strong>Open training data</strong> they release the text data used
for pretraining.</p>
<p><strong>Open architecture</strong> they publish a paper about the
neural network architecture and specific configuration they used for
training. Or they release the actual source code they used for
pretraining.</p>
<p>Ideally, if you want to use a model for empirical academic research
you might decide for models that are completely open in all three of the
above facets. Although, open training data is quite rare for available
state-of-the-art models.</p>
<p><strong>Commercial/proprietary models</strong> like GPT-4, Claude, or
Gemini are accessed only through APIs. While often offering superior
performance, they provide no access to internal architecture and may
have usage restrictions or costs that scale with volume.</p>
<p>Consider your requirements for: - Code modification and customization
- Data privacy and control - Commercial usage rights - Research
reproducibility - Long-term availability guarantees</p>
<p>If you wish to build an application that makes use of LLM text
generation, and you need accurate results, commercial APIs may be more
suitable.</p>
</div>
<div class="section level4">
<h4 id="hardware-and-compute-requirements">1.3.2 Hardware and Compute Requirements<a class="anchor" aria-label="anchor" href="#hardware-and-compute-requirements"></a>
</h4>
<p>Your available computational resources significantly constrain your
model options:</p>
<p><strong>Modern GPU access</strong> (RTX 4090, A100, H100, etc.)
enables you to run larger models locally. Consider: - VRAM requirements:
7B parameter models typically need 14+ GB, 13B models need 26+ GB, 70B
models require 140+ GB or multi-GPU setups - Inference speed
requirements for your application - Whether you need real-time responses
or can accept slower processing</p>
<p><strong>CPU-only environments</strong> limit you to smaller models
(such as SmolLM2 and SmolLM3) or <a href="https://ojs.aaai.org/index.php/AAAI/article/view/29908" class="external-link">quantized</a>
versions.</p>
<p><strong>Cloud/API access</strong> removes hardware constraints but
introduces ongoing costs and potential latency issues.</p>
</div>
<div class="section level4">
<h4 id="performance-evaluation">1.3.3 Performance Evaluation<a class="anchor" aria-label="anchor" href="#performance-evaluation"></a>
</h4>
<p>Different models excel at different tasks. Some evaluation criteria
include:</p>
<p><strong>General capability benchmarks</strong> like those found on
the <a href="https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard" class="external-link">Open
LLM Leaderboard</a> provide standardized comparisons across models for
reasoning, knowledge, and language understanding tasks.</p>
<p><strong>Multilingual performance</strong> varies significantly
between models. The <a href="https://mmluprox.github.io/" class="external-link">MMLU-Pro
benchmark</a> offers insights into cross-lingual capabilities if you
need support for non-English languages.</p>
<p><strong>Task-specific performance</strong> should be evaluated based
on your particular needs: - Code generation - Mathematical reasoning -
Reading comprehension and summarization - Creative writing and dialogue
quality - Scientific and technical domain knowledge</p>
<p>Always validate benchmark performance with your own test cases, as
real-world performance may differ from standardized evaluations.</p>
</div>
<div class="section level4">
<h4 id="purpose-or-use-case">1.3.4 Purpose or Use Case<a class="anchor" aria-label="anchor" href="#purpose-or-use-case"></a>
</h4>
<p><strong>Scientific and research applications</strong> often
prioritize reproducibility, transparency, and the ability to modify
model behavior. Open-source models with detailed documentation are
typically preferred (e.g. SmolLM, LLama, Olmo)</p>
<p><strong>Applications (mobile or web apps)</strong> may require: -
Reliable API uptime and support - Clear licensing for commercial use -
Scalability to handle many concurrent users - Content filtering and
safety features</p>
<p><strong>Personal or educational use</strong> might emphasize: -
Cost-effectiveness - Ease of setup and use</p>
</div>
<div class="section level4">
<h4 id="integration-and-deployment-considerations">1.3.5 Integration and Deployment Considerations<a class="anchor" aria-label="anchor" href="#integration-and-deployment-considerations"></a>
</h4>
<p><strong>Software integration</strong> requirements affect model
choice: - API-based models offer simpler integration but require
internet connectivity - Local models provide more control but require
more complex deployment - Consider latency requirements, offline
capabilities, and data privacy needs</p>
<p><strong>Hosting and serving capabilities</strong> determine whether
you can run models locally: - Do you have the infrastructure to serve
models at scale? - Are you self-hosting the model?</p>
</div>
<div class="section level4">
<h4 id="domain-specific-models">1.3.6 Domain-Specific Models<a class="anchor" aria-label="anchor" href="#domain-specific-models"></a>
</h4>
<p>Many models have been fine-tuned for specific domains or tasks. For
example:</p>
<ul>
<li>Medical and healthcare applications (e.g., <a href="https://huggingface.co/microsoft/biogpt" class="external-link">BioGPT</a>)</li>
<li>Legal document processing (e.g., <a href="https://huggingface.co/Equall/Saul-7B-Instruct-v1" class="external-link">SaulLM</a>)</li>
</ul>
<p>Remember that the LLM landscape evolves rapidly. New models are
released frequently, and performance benchmarks should be regularly
reassessed. Consider building your system with model-agnostic interfaces
to facilitate future transitions between different LLMs as your needs
evolve or better options become available.</p>
<hr>
</div>
</div>
<div class="section level3">
<h3 id="transformers-and-llms">1.4 Transformers and LLMs<a class="anchor" aria-label="anchor" href="#transformers-and-llms"></a>
</h3>
<p>LLMs are also trained using the transformer neural network
architecture, making use of the self-attention mechanism discussion in
Lesson 02. This means that an LLM is also a transformer-based language
model. However, they are distinct from <em>general</em>
transformer-based language models in three main characteristics:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Scale:</strong> there are two dimensions in which current
LLMs exceed general transformer language models in terms of scale. The
most important one is the number of <em>training parameters</em>
(weights) that are used for training models. In current models there are
hundreds of billions of parameters up to trillions. The second factor is
the <em>amount of training data</em> (raw text sequences) used for
training. Current LLMs use snapshots of the internet (upwards of
hundreds of terabytes in size) as a base for training and possibly
augment this with additional manually curated data. The sheer scale
characteristic of LLMs mean that such models require extremely
resource-intensive computation to train. State-of-the-art LLMs require
multiple dedicated Graphical Processing Units (GPUs) with tens or
hundreds of gigabytes of memory to load and train in reasonable time.
GPUs offer high parallelisability in their architecture for data
processing which makes them more efficient for training these
models.</p></li>
<li><p><strong>Post-training:</strong> After training a base language
model on textual data, there is an additional step of fine-tuning for
enabling conversation in a prompt style of interaction with users, which
current LLMs are known for. After the pre-training and neural network
training stages we end up with what is called a <em>base</em> model. The
base model is a language model which is essentially a token sequence
generator. This model by itself is not suitable for the interaction
style we see with current LLMs, which can do things like answer
questions, interpret instructions from the user, and incorporate
feedback to improve responses in conversations.</p></li>
<li><p><strong>Generalization:</strong> LLMs can be applied across a
wide range of NLP tasks such as summarization, translation, question
answering, etc., without necessarily the need for fine-tuning or
training separate models for different NLP tasks.</p></li>
</ol>
<figure><img src="fig/llm_analogy3.png" alt="llm engine analogy" width="1000" class="figure mx-auto d-block"></figure><p>What about the relation between BERT, which we learned about in
Lesson 02, and LLMs? Apart from the differences described above, BERT
only makes use of the encoder layer of the transformers architecture
because the goal is on creating token representations preserving
contextual meaning. There is no generative component to do something
with those representations. <br></p>
<figure><img src="fig/llms_vs_bert2.png" alt="llms vs bert" width="800" class="figure mx-auto d-block"></figure><hr>
</div>
<div class="section level3">
<h3 id="how-are-llms-trained">2. How are LLMs trained?<a class="anchor" aria-label="anchor" href="#how-are-llms-trained"></a>
</h3>
<p>Training LLMs involves a series of steps. There are two main phases:
pretraining and post training. Pretraining generally involves the
following substeps:</p>
<div class="section level4">
<h4 id="obtaining-and-pre-processing-textual-data-for-training">2.1 Obtaining and pre-processing textual data for training<a class="anchor" aria-label="anchor" href="#obtaining-and-pre-processing-textual-data-for-training"></a>
</h4>
<ul>
<li>
<em>Downloading and pre-processing text:</em> State-of-the-art LLMs
include entire snapshots of the internet as the core textual data for
training. This data can be sourced from efforts such as <a href="https://commoncrawl.org/" class="external-link">CommonCrawl</a>. Proprietary LLMs may
augment or supplement this training data with additional licensed or
proprietary textual data (e.g., books) from other sources or companies.
The raw web pages are not usable by themselves, we need to extract the
raw text from those HTML pages. This requires a preprocessing or data
cleaning step.</li>
</ul>
<figure><img src="fig/html_to_text.png" alt="html to text processing" width="800" class="figure mx-auto d-block"></figure><ul>
<li>
<em>Tokenization:</em> As we saw in Lesson 01, the raw text itself
cannot be used in the training step, we need a way to tokenize and
encode the text for processing by the neural network. As an example of
what these encodings look like for OpenAI models like GPT, you can visit
<a href="https://tiktokenizer.f2api.com/" class="external-link">TikTokenizer</a>.</li>
</ul>
<figure><img src="fig/text_to_tokenids.png" alt="tokenization" width="800" class="figure mx-auto d-block"></figure>
</div>
<div class="section level4">
<h4 id="neural-network-training">2.2 Neural network training<a class="anchor" aria-label="anchor" href="#neural-network-training"></a>
</h4>
<p>With LLMs the training goal is to predict the next token in a
one-dimensional sequence of tokens. This is different from BERT where
the goal is to predict masked tokens in the input sequence. BERT is
therefore not natively developed for generating text, whereas LLMs are.
In the internals of the transformer architecture, this is illustrated by
the fact that BERT only makes use of the Encoder component to create its
contextualised word embeddings. It does not use the Decoder component to
generate new tokens for the input sequence.</p>
<figure><img src="fig/llm_training_goal.png" alt="training goal llms" width="800" class="figure mx-auto d-block"></figure><p>After training we obtain a <em>base</em> LLM which is predicts or
generates token sequences that resemble its training data. However, a
post training step is required in order to fine-tune the model to accept
instructions, answer questions in a conversational style and to have
behavior that is more suitable for interaction with humans.</p>
</div>
<div class="section level4">
<h4 id="post-training">2.3 Post training<a class="anchor" aria-label="anchor" href="#post-training"></a>
</h4>
<p>What does post training for LLMs look like? Why is this step
necessary? What would happen if you skip this step and just use the base
model trained in Step 2.2 for inference? The answer is that the base
model is just a token sequence predictor. It just predicts the most
likely next token for an input sequence of tokens. It does not
understand how to deal with conversations or to interpret instructions
(the intentions and tone behind written communication).</p>
<p>Therefore, you may encounter unexpected conversations like this if
interacting with a base LLM:</p>
<p><strong>Not Following Instructions</strong></p>
<pre><code>Human: Summarize this paragraph in one sentence: The Nile is the longest river in Africa and flows through 11 countries before emptying into the Mediterranean Sea.
Raw LLM: The Nile is the longest river in Africa and flows through 11 countries before emptying into the Mediterranean Sea.</code></pre>
<p>In this example interaction, the model was trained to predict text,
not to follow instructions. So it might not give expected or correct
responses although, statistically, these response tokens are indeed the
next most likely tokens.</p>
<p><strong>Regurgitation</strong></p>
<pre><code>Human: Donald John Trump (born June 14, 1946) is an American politician,
Raw LLM: media personality, and businessman who is the 47th president of the United States. A member of the Republican Party, he served as the 45th president from 2017 to 2021...</code></pre>
<figure><img src="fig/trump.png" alt="tokenization" width="800" class="figure mx-auto d-block"></figure><p>In this interaction, the model just “regurgitates” verbatim a
response from Wikipedia (part of it’s training data), instead of
creatively formulating a response in its own words. These are just a few
examples of the kind of undesirable behavior one might encounter when
doing inference with the base model without post training.</p>
<p>Therefore, to avoid issues like this, and to be useful as
conversational assistants, LLMs generally have to be fine-tuned on
additional unseen conversation datasets (containing on the order of
hundreds of thousands of conversations). These datasets are manually
created and validated by humans. For proprietary models, these humans
are hired professionally to contribute to this dataset from platforms
such as Upwork and ScaleAI. There are also open-source efforts to create
such datasets such as <a href="https://huggingface.co/datasets/OpenAssistant/oasst2" class="external-link">OpenAssistant</a>.</p>
<p>Once such a dataset is obtained, the conversations need to be
represented as a one-dimensional token sequence. Conversations therefore
have to be encoded into such representations before being used as input
for training. The encoding method for conversations currently differ
among LLMs. The figure below shows how GPT-4 tokenizes and encodes input
conversations as one dimensional token sequences (<a href="https://tiktokenizer.f2api.com/" class="external-link">TikTokenizer</a> is a page for
visualizing and experimenting with tokenization approaches used by
OpenAI LLMs).</p>
<figure><img src="fig/tokenize_conversations.png" alt="tokenization for conversation data" width="800" class="figure mx-auto d-block"></figure><p>After encoding or tokenization, the conversational data is supplied
as input for fine-tuning the base model (replacing the original training
data). I.e., the base model’s weights or parameters are updated through
this process.</p>
<p>Below are some examples of conversational data that one might use to
fine-tune an LLM. In general, you want multi-turn conversations where
each participant in the conversation gets the opportunity to respond
multiple times. Because this is the likely mode of interaction that will
take place between humans and LLM-based chat assistants:</p>
<pre><code>**Conversation 1 (Travel Planning):**
Human: Can you suggest a weekend getaway near Amsterdam?
LLM: Sure! Giethoorn – Known as the “Venice of the North,” this village has canals, thatched-roof houses, and a peaceful atmosphere. Utrecht, a charming city with medieval streets, beautiful canals, and cozy cafés.
Human: Which one would be better for a quiet, relaxing trip?
LLM: Giethoorn would be the quieter option. Unlike Utrecht, which is a bustling city, Giethoorn is a small village with a calm, serene atmosphere, ideal for a peaceful getaway.

**Conversation 2 (Math Help):**
Human: What's the derivative of 3𝑥^2 + 5𝑥 − 7?
LLM: The derivative is 6𝑥 + 5.
Human: Great, and what about the second derivative?
LLM: The second derivative is 6.

**Conversation 3 (Creative Writing):**
Human: Give me a one-line horror story.
LLM: The reflection in the mirror smiled back before I did.
Human: Can you make it even creepier?
LLM: The reflection winked, though I was alone in the room.</code></pre>
<hr>
</div>
</div>
<div class="section level3">
<h3 id="llm-prompting-strategies">3. LLM Prompting strategies<a class="anchor" aria-label="anchor" href="#llm-prompting-strategies"></a>
</h3>
<p>See <a href="">notebooks/prompting.ipynb</a>…</p>
<hr>
</div>
<div class="section level3">
<h3 id="solving-nlp-tasks-with-llms">4. Solving NLP tasks with LLMs<a class="anchor" aria-label="anchor" href="#solving-nlp-tasks-with-llms"></a>
</h3>
<p>See <a href="">notebooks/classifandeval.ipynb</a>…</p>
<hr>
</div>
<div class="section level3">
<h3 id="creating-your-own-chat-assistant">5. Creating your own chat assistant<a class="anchor" aria-label="anchor" href="#creating-your-own-chat-assistant"></a>
</h3>
<p>See <a href="">notebooks/chatbot.ipynb</a>…</p>
<hr>
</div>
<div class="section level3">
<h3 id="drawbacks-and-biases-with-llms">6. Drawbacks and Biases with LLMs<a class="anchor" aria-label="anchor" href="#drawbacks-and-biases-with-llms"></a>
</h3>
<p>See <a href="">notebooks/responsible.ipynb</a>…</p>
</div>
</section><div class="section level1">
<h1 id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h1>
<p>Comprehensive overview of LLMs: <a href="https://dl.acm.org/doi/abs/10.1145/3744746" class="external-link uri">https://dl.acm.org/doi/abs/10.1145/3744746</a></p>
<p>Andrej Karpathy deep dive into LLMs: <a href="https://www.youtube.com/watch?v=7xTGNNLPyMI" class="external-link uri">https://www.youtube.com/watch?v=7xTGNNLPyMI</a></p>
<p>Multilingual benchmark for LLMs: <a href="https://mmluprox.github.io/" class="external-link uri">https://mmluprox.github.io/</a></p>
</div></section>
</div>
    </main>
</div>
<!-- END  : inst/pkgdown/templates/content-extra.html -->

      </div>
<!--/div.row-->
      		<footer class="row footer mx-md-3"><hr>
<div class="col-md-6">
        <p>This lesson is subject to the <a href="CODE_OF_CONDUCT.html">Code of Conduct</a></p>
        <p>

        <a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/edit/main/README.md" class="external-link">Edit on GitHub</a>

	
        | <a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/blob/main/CONTRIBUTING.md" class="external-link">Contributing</a>
        | <a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/" class="external-link">Source</a></p>
				<p><a href="https://github.com/esciencecenter-digital-skills/Natural-language-processing/blob/main/CITATION.cff" class="external-link">Cite</a> | <a href="mailto:team@carpentries.org">Contact</a> | <a href="https://carpentries.org/about/" class="external-link">About</a></p>
			</div>
			<div class="col-md-6">

        <p>Materials licensed under <a href="LICENSE.html">CC-BY 4.0</a> by the authors</p>

        <p>Template licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/" class="external-link">CC-BY 4.0</a> by <a href="https://carpentries.org/" class="external-link">The Carpentries</a></p>
        <p>Built with <a href="https://github.com/carpentries/sandpaper/tree/0.17.1" class="external-link">sandpaper (0.17.1)</a>, <a href="https://github.com/carpentries/pegboard/tree/0.7.9" class="external-link">pegboard (0.7.9)</a>, and <a href="https://github.com/carpentries/varnish/tree/1.0.7" class="external-link">varnish (1.0.7)</a></p>
			</div>
		</footer>
</div> <!-- / div.container -->
	<div id="to-top">
		<a href="#top">
      <i class="search-icon" data-feather="arrow-up" role="img" aria-label="Back To Top"></i><br><!-- <span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top --><span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top
		</a>
	</div>
  <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "LearningResource",
  "@id": "https://esciencecenter-digital-skills.github.io/Natural-language-processing/aio.html",
  "inLanguage": "en",
  "dct:conformsTo": "https://bioschemas.org/profiles/LearningResource/1.0-RELEASE",
  "description": "A Carpentries Lesson teaching foundational data and coding skills to researchers worldwide",
  "keywords": "software, data, lesson, The Carpentries, NLP, English, social sciences, pre-alpha",
  "name": "All in One View",
  "creativeWorkStatus": "active",
  "url": "https://esciencecenter-digital-skills.github.io/Natural-language-processing/aio.html",
  "identifier": "https://esciencecenter-digital-skills.github.io/Natural-language-processing/aio.html",
  "dateCreated": "2024-04-24",
  "dateModified": "2025-10-21",
  "datePublished": "2025-10-21"
}

  </script><script>
		feather.replace();
	</script>
</body>
</html><!-- END:   inst/pkgdown/templates/layout.html-->

